I"å<h1 id="lecture-14-æƒå€¼åˆå§‹åŒ–">Lecture 14 æƒå€¼åˆå§‹åŒ–</h1>

<p>åœ¨å‰å‡ èŠ‚è¯¾ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•æ­å»ºç½‘ç»œæ¨¡å‹ã€‚åœ¨ç½‘ç»œæ¨¡å‹æ­å»ºå¥½ä¹‹åï¼Œæœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„æ­¥éª¤ï¼Œå°±æ˜¯å¯¹æ¨¡å‹ä¸­çš„æƒå€¼è¿›è¡Œåˆå§‹åŒ–ï¼šæ­£ç¡®çš„æƒå€¼åˆå§‹åŒ–å¯ä»¥åŠ å¿«æ¨¡å‹çš„æ”¶æ•›ï¼Œè€Œä¸é€‚å½“çš„æƒå€¼åˆå§‹åŒ–å¯ä»¥ä¼šå¼•å‘æ¢¯åº¦æ¶ˆå¤±æˆ–è€…çˆ†ç‚¸ï¼Œæœ€ç»ˆå¯¼è‡´æ¨¡å‹æ— æ³•è®­ç»ƒã€‚æœ¬èŠ‚è¯¾ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•è¿›è¡Œæƒå€¼åˆå§‹åŒ–ã€‚</p>

<h2 id="1-æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸">1. æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸</h2>

<p>è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥ä¸ŠèŠ‚è¯¾ä¸­æåˆ°çš„ä¸€ä¸ªä¸‰å±‚çš„å…¨è¿æ¥ç½‘ç»œä¸ºä¾‹ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ç¬¬äºŒä¸ªéšè—å±‚ä¸­çš„æƒå€¼ $W_2$ çš„æ¢¯åº¦æ˜¯æ€ä¹ˆæ±‚å–çš„ã€‚</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-18-WX20201218-170939%402x.png" width="80%" /></p>

<p>ä»å…¬å¼è§’åº¦æ¥çœ‹ï¼Œä¸ºäº†é˜²æ­¢å‘ç”Ÿæ¢¯åº¦æ¶ˆå¤±æˆ–è€…çˆ†ç‚¸ï¼Œæˆ‘ä»¬å¿…é¡»ä¸¥æ ¼æ§åˆ¶ç½‘ç»œå±‚è¾“å‡ºå€¼çš„å°ºåº¦èŒƒå›´ï¼Œå³æ¯ä¸ªç½‘ç»œå±‚çš„è¾“å‡ºå€¼ä¸èƒ½å¤ªå¤§æˆ–è€…å¤ªå°ã€‚</p>

<p><strong>ä»£ç ç¤ºä¾‹</strong>ï¼š</p>

<p><strong>100 ä¸ªçº¿æ€§å±‚çš„ç®€å•å åŠ </strong>ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">tools.common_tools</span> <span class="kn">import</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># è®¾ç½®éšæœºç§å­
</span>

<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neural_num</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">neural_num</span><span class="p">,</span> <span class="n">neural_num</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span><span class="p">)])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">neural_num</span> <span class="o">=</span> <span class="n">neural_num</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">linear</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linears</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># normal: mean=0, std=1
</span>

<span class="n">layer_nums</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># ç½‘ç»œå±‚æ•°
</span><span class="n">neural_nums</span> <span class="o">=</span> <span class="mi">256</span>  <span class="c1"># æ¯å±‚çš„ç¥ç»å…ƒä¸ªæ•°
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># è¾“å…¥æ•°æ®çš„ batch size
</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">neural_nums</span><span class="p">,</span> <span class="n">layer_nums</span><span class="p">)</span>
<span class="n">net</span><span class="p">.</span><span class="n">initialize</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">neural_nums</span><span class="p">))</span>  <span class="c1"># normal: mean=0, std=1
</span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>è¾“å‡ºç»“æœï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre>tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;MmBackward&gt;) 
</pre></td></tr></tbody></table></code></pre></div></div>

<p>æˆ‘ä»¬å‘ç° <code class="language-plaintext highlighter-rouge">output</code> ä¸­çš„æ¯ä¸€ä¸ªå€¼éƒ½æ˜¯ <code class="language-plaintext highlighter-rouge">nan</code>ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ•°æ®å€¼å¯èƒ½éå¸¸å¤§æˆ–è€…éå¸¸å°ï¼Œå·²ç»è¶…å‡ºäº†å½“å‰ç²¾åº¦èƒ½å¤Ÿè¡¨ç¤ºçš„èŒƒå›´ã€‚ç°åœ¨æˆ‘ä»¬è¿›å…¥ <code class="language-plaintext highlighter-rouge">forward</code> å‡½æ•°ä¸­æ¥çœ‹ä¸€ä¸‹æ•°æ®</p>

<p>ä¸‹èŠ‚å†…å®¹ï¼šæƒå€¼åˆå§‹åŒ–</p>
:ET