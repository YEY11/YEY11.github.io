I"6<h1 id="lecture-10-分布语义学">Lecture 10 分布语义学</h1>

<p>这节课我继续学习语义学相关内容，这次我们不再关注单词层面的语义学，而是从语料库中直接学习单词含义，这个领域也被称为分布语义学。</p>

<h2 id="1-分布语义学">1. 分布语义学</h2>
<h3 id="11-词汇数据库的问题">1.1 词汇数据库的问题</h3>
<ul>
  <li>需要手工构建
    <ul>
      <li>成本高</li>
      <li>人类的注解可能存在偏见和噪音</li>
    </ul>
  </li>
  <li>语言是动态的
    <ul>
      <li>新的单词：俚语、专业术语等等</li>
      <li>新的词义（senses）</li>
    </ul>
  </li>
  <li>互联网为我们提供了大量的文本，我们可以利用它们获得单词含义吗？</li>
</ul>

<h3 id="12-分布假设">1.2 分布假设</h3>
<ul>
  <li>“<em>You shall know a word by the company it keeps（你可以通过其周围的上下文单词来了解一个目标单词）</em>” —— (Firth, 1957)</li>
  <li>共现文档通常指示了主题（<strong>文档（document）</strong> 作为上下文）。
    <ul>
      <li>例如：$\textit{voting}$（投票）和 $\textit{politics}$（政治）<br />
如果我们观察文档，会发现这两个单词经常出现在同一文档中。因此，不同单词的共现文档在一定程度上反映了这些单词在某种主题方面的关联。</li>
    </ul>
  </li>
  <li>局部上下文反映了一个单词的语义类别（<strong>单词窗口（word window）</strong> 作为上下文）。
    <ul>
      <li>例如：$\textit{eat a pizza, eat a burger}$<br />
可以看到，$“\textit{pizza}”$ 和 $“\textit{burger}”$ 这两个单词都具有共同的局部上下文 $“\textit{eat a}”$，由此我们可以知道这两个单词都具有和 $“\textit{eat a}”$ 相关的某种含义。</li>
    </ul>
  </li>
</ul>

<h3 id="13-根据上下文猜测单词含义">1.3 根据上下文猜测单词含义</h3>

<ul>
  <li>
    <p>根据其用法来学习一个未知单词。</p>
  </li>
  <li>
    <p>例如：现在我们有一个单词 $\textit{tezgüino}$，我们并不知道其含义，我们试图通过从该单词的一些用法中学习到其含义。<br />
下面是该单词出现过的一些例句：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-132004%402x.png" width="50%" /></p>

    <p>作为人类，通过结合常识，我们可以大概猜测到 $\textit{tezgüino}$ 可能是某种含酒精饮品。</p>
  </li>
  <li>
    <p>我们再查看一下在相同（或者类似）上下文中的其他单词的情况。</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-132156%402x.png" width="50%" /></p>

    <p>可以看到，单词 $\textit{wine}$ 出现过的类似场景最多。因此，尽管我们并不知道 $\textit{tezgüino}$ 的具体含义，我们还是可以认为 $\textit{tezgüino}$ 和 $\textit{wine}$ 在单词含义方面非常相近。</p>
  </li>
</ul>

<h3 id="14-词向量">1.4 词向量</h3>

<p>在前面的例子中，我们可以将这些由 $0$ 和 $1$ 组成的行视为词向量，因为它们能够很好地代表这些用例。例如：给定 100 个非常好的例句，我们可以基于这些单词是否出现在这些例句中，将其转换为 100 维的向量。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-132156%402x.png" width="50%" /></p>

<ul>
  <li>每一行都可以视为一个 <strong>词向量（word vector）</strong>。</li>
  <li>它描述了单词的分布特性（目标单词附近的上下文单词信息）。</li>
  <li>捕获各种语义关系，例如：同义（synonymy）、类比（analogy）等。</li>
</ul>

<h3 id="15-词嵌入">1.5 词嵌入</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-133517%402x.png" width="40%" /></p>

<ul>
  <li>在之前的神经网络的章节中，我们已经见过另一种词向量：<strong>词嵌入（word embeddings）</strong>。
    <ul>
      <li>例如：在使用前馈神经网络进行文本分类时，第一层相当于是词嵌入层，该层的权重矩阵即词嵌入矩阵。</li>
    </ul>
  </li>
  <li>这里，我们将学习通过其他方法产生词向量：
    <ul>
      <li>基于计数的方法</li>
      <li>专为学习词向量而设计的更高效的神经网络方法</li>
    </ul>
  </li>
</ul>

<h2 id="2-基于计数的方法">2. 基于计数的方法</h2>
<p>首先，我们将学习如何通过基于计数的方法学习词向量。</p>
<h3 id="21-向量空间模型">2.1 向量空间模型</h3>
<p>这里，我们学习的第一个模型是 <strong>向量空间模型（Vector Space Model，VSM）</strong>。</p>
<ul>
  <li>基本思想：将单词含义表示为一个向量。</li>
  <li>通常，我们将 <strong>文档（documents）</strong>视为上下文。</li>
  <li>一个矩阵，两种视角：
    <ul>
      <li>一个文档由其所包含的单词表示</li>
      <li>一个单词由其出现过的文档表示</li>
    </ul>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-143123%402x.png" width="60%" /></p>

    <p>这里，每一行都表示语料库中的一个文档，每一列表示语料库的词汇表中的一个单词，单元格中的数字表示该单词在对应文档中出现的频率。例如：单词 $\textit{state}$ 没有在文档 $425$ 中出现过，所以对应的值为 $0$，但是它在文档 $426$ 中出现过 $3$ 次，所以对应值为 $3$。</p>

    <p>当我们构建完成这样一个矩阵后，我们可以从两种视角来看待它：如果我们观察每一行，我们可以将其视为每个文档的词袋模型表示；如果我们观察每一列，我们可以将其视为每个单词的词向量表示。</p>
  </li>
</ul>

<h3 id="22-操作-vsm">2.2 操作 VSM</h3>
<p>一旦我们构建了一个向量空间模型，我们可以对其进行一些操作：</p>
<ul>
  <li>给值进行加权（不仅是频率）<br />
我们可以对矩阵中的值进行一些除了词频之外的更好的加权方式。</li>
  <li>创建低维的密集向量<br />
假如我们有非常多的文档，例如 100 万个，那么我们的词向量的维度也将会是 100 万维，因为每个维度都代表一个文档。但是，这样的话词向量的维度过高了，并且非常稀疏，其中大部分维度的值都是 $0$。</li>
  <li>一旦我们完成了这些，我们就可以比较不同的词向量，并计算彼此之间的相似度等等。</li>
</ul>

<h3 id="23-tf-idf">2.3 TF-IDF</h3>

<p>首先，我们学习一种比单纯的词频更好的加权方法：<strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>。它是 <strong>信息检索（information retrieval）</strong>领域的一种标准加权方案。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-145252%402x.png" width="40%" /></p>

<p><strong><center><span style="font-size:10pt">TF 矩阵</span></center></strong></p>

<p>我们首先可以得到一个 <strong>TF（term-frequency）矩阵</strong>，和之前一样，单元格中的数字表示该单词在对应文档中出现的频率。</p>

<p>然后，我们将计算该单词对应的 <strong>IDF（inverse document frequency）</strong>值。</p>

<script type="math/tex; mode=display">\textit{idf}_w=\log \dfrac{|D|}{\textit{df}_w}</script>

<p>其中，$|D|$ 表示文档总数。$\textit{df}_w$ 表示单词 $w$ 的文档频率，即该单词在所有文档（即语料库）中出现的总次数（TF 矩阵中最后一行）。这里，$\log$ 的底数为 $2$。</p>

<p>例如：假设一共有 $500$ 个文档，单词 “$\textit{the}$” 的 $\textit{df}$ 值为 $500$，那么这里单词 “$\textit{the}$” 的 IDF 值为：</p>

<script type="math/tex; mode=display">\textit{idf}_\textit{the}=\log_2 \dfrac{500}{500}=\log_2 1=0</script>

<p>分别计算每个单词的 IDF 值，并将其和对应单元格的 TF 值相乘，我们可以得到下面的 TF-IDF 矩阵</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-145305%402x.png" width="40%" /></p>

<p><strong><center><span style="font-size:10pt">TF-IDF 矩阵</span></center></strong></p>

<p>可以看到，单词 “$\textit{the}$” 对应的列的值都为 $0$，这是因为其 IDF 值为 $0$，所以无论对应单元格的 TF 值为多少，相乘后得到的结果都是 $0$。</p>

<p>TF-IDF 的核心思想在于：对于在大部分文档中都频繁出现的单词（例如：“$\textit{the}$”），我们给予更低的权重，因为它们包含的信息量很少。</p>

<h3 id="24-降维">2.4 降维</h3>

<ul>
  <li>Term-document 矩阵过于 <strong>稀疏（sparse）</strong></li>
  <li><strong>降维（Dimensionality reduction）</strong>：创建更短、更密集的向量。</li>
  <li>更实用（特征更少）</li>
  <li>消除噪声（更好地避免过拟合）</li>
</ul>

<h3 id="25-奇异值分解svd">2.5 奇异值分解（SVD）</h3>

<p><strong>奇异值分解（Singular Value Decomposition，SVD）</strong>是一种流行的降维方法。</p>

<p>SVD 的核心思想很简单：给定一个矩阵 $A$，我们可以将其分解为 3 个相乘的矩阵：$U$、$\Sigma$ 和 $V^{\mathrm T}$。</p>

<script type="math/tex; mode=display">A=U\Sigma V^{\mathrm T}</script>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-154720%402x.png" width="80%" /></p>

<p>可以看到：</p>
<ul>
  <li>原始矩阵 $A$ 是 term-document 矩阵（出现为 $1$，没有出现为$0$），其行数为词汇表大小 $|V|$，列数为文档总数 $|D|$。</li>
  <li>$U$ 是新的 term 矩阵，行数为词汇表大小 $|V|$，列数为 $m$。其中，$m$ 为矩阵 $A$ 的秩，即 $m=Rank(A)$。</li>
  <li>$\Sigma$ 是大小为 $m\times m$ 的奇异值矩阵，它是一个对角矩阵。</li>
  <li>$V^{\mathrm T}$ 是新的 document 矩阵，行数为 $m$，列数为文档总数 $|D|$。</li>
</ul>

<p>显然，经过 SVD，我们得到的新的 term 矩阵要比之前的 $A$ 矩阵维度更小，并且更密集。</p>

<h3 id="26-截断潜在语义分析lsa">2.6 截断：潜在语义分析（LSA）</h3>

<p>我们还可以在 SVD 的基础上更进一步，采用 <strong>截断（truncating）</strong>方法，也被称为 <strong>潜在语义分析 (Latent Semantic Analysis, LSA)</strong>。</p>

<ul>
  <li>将 $U$、$\Sigma$ 和 $V^{\mathrm T}$ 截断为 $k$ 个维度，从而生成原始矩阵的最佳 $k$ 阶近似。</li>
  <li>因此，截断后的 $U_k$（或者 $V_k^{\mathrm T}$）是对应单词的一个新的低维表示。</li>
  <li>通常，$k$ 的取值为 100-5000。</li>
</ul>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-162657%402x.png" width="80%" /></p>

<p>可以看到，矩阵 $U_k$ 在矩阵 $U$ 的基础上进一步压缩了，通常 $k$ 取 100 或者 200 时已经足够很好地表示单词含义了。现在，矩阵 $U_k$ 中的每一行都可以视为一个词向量。</p>

<h3 id="27-单词作为上下文">2.7 单词作为上下文</h3>

<p>我们已经学习了将文档作为上下文，我们还可以将单词作为上下文。</p>
<ul>
  <li>构建一个矩阵，关于目标单词随着其他单词一起出现的频率。
    <ul>
      <li>
        <p>在某些预定义的上下文中（通常是一个窗口）。<br />
相比将文档作为上下文，我们可以选择目标单词附近固定范围内的某些单词作为上下文。</p>

        <p>例如：我们可以选择窗口大小为 $5$，即目标单词前后长度为 $5$ 的范围内的单词作为上下文单词。</p>

        <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-163651%402x.png" width="60%" /></p>

        <p>可以看到，在上面的矩阵中，每一行都表示一个单词，每一列也表示一个单词。单元格中的数字表示目标单词和上下文单词在整个语料库中所有大小为 $5$ 的窗口内（即从语料库中提取所有的 five-grams）共同出现的频率。</p>
      </li>
    </ul>
  </li>
  <li>但是，行频率存在一个明显的问题：整个矩阵被常见单词所主导。
    <ul>
      <li>例如：我们可以看到，由于单词 “$\textit{the}$” 出现频率很高，使得对应单元格内的值非常大。之前，我们采用 TF-IDF 加权来给予常见单词的值一个折扣。但是这里，我们无法采用 TF-IDF，因为这里我们没有涉及到文档。相应地，这里我们可以采用点互信息（PMI）的方法来处理这个问题。</li>
    </ul>
  </li>
</ul>

<h3 id="28-点互信息pmi">2.8 点互信息（PMI）</h3>

<p><strong>点互信息（Pointwise Mutual Information，PMI）</strong>的思想非常简单，对于两个事件 $x$ 和 $y$（即两个单词），PMI 计算二者的差异：</p>
<ul>
  <li>它们的联合概率分布</li>
  <li>它们各自的概率分布（假设彼此之间互相独立）</li>
</ul>

<script type="math/tex; mode=display">\textit{PMI}(x,y)=\log_2 \dfrac{p(x,y)}{p(x)p(y)}</script>

<h3 id="29-例子计算-pmi">2.9 例子：计算 PMI</h3>
<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-171638%402x.png" width="65%" /></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(x,y) &= \text{count}(x,y)/\Sigma \\
p(x) &= \Sigma_x/\Sigma \\
p(y) &= \Sigma_y/\Sigma \\
\end{align} %]]></script>

<p>假设 $x=\textit{state},\,y=\textit{country}$：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(x,y) &= 10/15871304=6.3 \times 10^ -7 \\
p(x) &= \Sigma_x/\Sigma \\
p(y) &= \Sigma_y/\Sigma \\
\end{align} %]]></script>

<p>下节内容：</p>

:ET