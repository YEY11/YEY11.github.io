I"ĝ<h1 id="lecture-13-nn-网络层池化层全连接层和激活函数层">Lecture 13 nn 网络层：池化层、全连接层和激活函数层</h1>

<p>上节课中，我们学习了网络层中的卷积层。本节课中，我们将继续学习其他几种网络层：池化层、线性层和激活函数层。</p>

<h2 id="1-池化层">1. 池化层</h2>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-mp.gif" width="60%" /></p>

<p><strong>池化运算 (Pooling)</strong>：对信号进行 <strong>“收集”</strong> 并 <strong>“总结”</strong>，类似水池收集水资源，因而得名池化层。</p>

<ul>
  <li><strong>“收集”</strong>：多变少。</li>
  <li><strong>“总结”</strong>：最大值/平均值。</li>
</ul>

<p><strong>最大池化 vs. 平均池化</strong>：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max.png" width="60%" /></p>

<h4 id="nnmaxpool2d"><code class="language-plaintext highlighter-rouge">nn.MaxPool2d</code></h4>

<p><strong>功能</strong>：对二维信号（图像）进行最大值池化。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">return_indices</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">kernel_size</code>：池化核尺寸。</li>
  <li><code class="language-plaintext highlighter-rouge">stride</code>：步长。</li>
  <li><code class="language-plaintext highlighter-rouge">padding</code>：填充个数。</li>
  <li><code class="language-plaintext highlighter-rouge">dilation</code>：池化核间隔大小。</li>
  <li><code class="language-plaintext highlighter-rouge">ceil_mode</code>：尺寸是否向上取整。用于计算输出特征图尺寸，默认设置为向下取整。</li>
  <li><code class="language-plaintext highlighter-rouge">return_indices</code>：记录池化像素索引。通常在最大值反池化上采样时使用。</li>
</ul>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">tools.common_tools</span> <span class="kn">import</span> <span class="n">transform_invert</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 设置随机种子
</span>
<span class="c1"># ================================= load img ==================================
</span><span class="n">path_img</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">__file__</span><span class="p">)),</span> <span class="s">"lena.png"</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">path_img</span><span class="p">).</span><span class="n">convert</span><span class="p">(</span><span class="s">'RGB'</span><span class="p">)</span>  <span class="c1"># 0~255
</span>
<span class="c1"># convert to tensor
</span><span class="n">img_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()])</span>
<span class="n">img_tensor</span> <span class="o">=</span> <span class="n">img_transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">img_tensor</span><span class="p">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>    <span class="c1"># C*H*W to B*C*H*W
</span>
<span class="c1"># ========================== create maxpool layer =============================
</span><span class="n">maxpool_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>   <span class="c1"># input:(i, o, size) weights:(o, i , h, w)
</span><span class="n">img_pool</span> <span class="o">=</span> <span class="n">maxpool_layer</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>

<span class="c1"># ================================= visualization =============================
</span><span class="k">print</span><span class="p">(</span><span class="s">"池化前尺寸:{}</span><span class="se">\n</span><span class="s">池化后尺寸:{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">img_pool</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">img_pool</span> <span class="o">=</span> <span class="n">transform_invert</span><span class="p">(</span><span class="n">img_pool</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="p">...],</span> <span class="n">img_transform</span><span class="p">)</span>
<span class="n">img_raw</span> <span class="o">=</span> <span class="n">transform_invert</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">img_transform</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">).</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_pool</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">).</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_raw</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>池化前尺寸:torch.Size([1, 3, 512, 512])
池化后尺寸:torch.Size([1, 3, 256, 256])
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-WX20201216-193358%402x.png" width="60%" /></p>

<p>可以看到，经过最大池化后的图像尺寸减小了一半，而图像质量并没有明显降低。因此，池化操作可以剔除图像中的冗余信息，以及减小后续的计算量。</p>

<h4 id="nnavgpool2d"><code class="language-plaintext highlighter-rouge">nn.AvgPool2d</code></h4>

<p><strong>功能</strong>：对二维信号（图像）进行平均值池化。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">AvgPool2d</span><span class="p">(</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">count_include_pad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">divisor_override</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">kernel_size</code>：池化核尺寸。</li>
  <li><code class="language-plaintext highlighter-rouge">stride</code>：步长。</li>
  <li><code class="language-plaintext highlighter-rouge">padding</code>：填充个数。</li>
  <li><code class="language-plaintext highlighter-rouge">ceil_mode</code>：尺寸向上取整。</li>
  <li><code class="language-plaintext highlighter-rouge">count_include_pad</code>：是否将填充值用于平均值的计算。</li>
  <li><code class="language-plaintext highlighter-rouge">divisor_override</code>：除法因子。计算平均值时代替像素个数作为分母。</li>
</ul>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">tools.common_tools</span> <span class="kn">import</span> <span class="n">transform_invert</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 设置随机种子
</span>
<span class="c1"># ================================= load img ==================================
</span><span class="n">path_img</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">__file__</span><span class="p">)),</span> <span class="s">"lena.png"</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">path_img</span><span class="p">).</span><span class="n">convert</span><span class="p">(</span><span class="s">'RGB'</span><span class="p">)</span>  <span class="c1"># 0~255
</span>
<span class="c1"># convert to tensor
</span><span class="n">img_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()])</span>
<span class="n">img_tensor</span> <span class="o">=</span> <span class="n">img_transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">img_tensor</span><span class="p">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>    <span class="c1"># C*H*W to B*C*H*W
</span>
<span class="c1"># ========================== create avgpool layer =============================
</span><span class="n">avgpoollayer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>   <span class="c1"># input:(i, o, size) weights:(o, i , h, w)
</span><span class="n">img_pool</span> <span class="o">=</span> <span class="n">avgpoollayer</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>

<span class="c1"># =============================== visualization ===============================
</span><span class="k">print</span><span class="p">(</span><span class="s">"池化前尺寸:{}</span><span class="se">\n</span><span class="s">池化后尺寸:{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">img_pool</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">img_pool</span> <span class="o">=</span> <span class="n">transform_invert</span><span class="p">(</span><span class="n">img_pool</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="p">...],</span> <span class="n">img_transform</span><span class="p">)</span>
<span class="n">img_raw</span> <span class="o">=</span> <span class="n">transform_invert</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">img_transform</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">).</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_pool</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">).</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_raw</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>池化前尺寸:torch.Size([1, 3, 512, 512])
池化后尺寸:torch.Size([1, 3, 256, 256])
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-WX20201216-195332%402x.png" width="60%" /></p>

<p>同样，图像尺寸减小了一半，而质量并没有明显降低。另外，如果我们仔细对比最大池化与平均池化的结果，可以发现最大池化后的图像会偏亮一些，而平均池化后的图像会偏暗一些，这是由于两种池化操作采用不同的计算方式造成的 (像素值越大，图像亮度越高)。</p>

<h5 id="divisor_override-的使用"><code class="language-plaintext highlighter-rouge">divisor_override</code> 的使用</h5>

<p>现在，我们来看一下除法因子的使用。这里，我们初始化一个 $4\times 4$ 的图像，并且采用一个 $2\times 2$ 的窗口，步长设置为 $2$。</p>

<p><strong>正常的平均池化</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">img_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">avgpool_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">img_pool</span> <span class="o">=</span> <span class="n">avgpool_layer</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"raw_img:</span><span class="se">\n</span><span class="s">{}</span><span class="se">\n</span><span class="s">pooling_img:</span><span class="se">\n</span><span class="s">{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">,</span> <span class="n">img_pool</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>raw_img:
tensor([[[[1., 1., 1., 1.],
          [1., 1., 1., 1.],
          [1., 1., 1., 1.],
          [1., 1., 1., 1.]]]])
pooling_img:
tensor([[[[1., 1.],
          [1., 1.]]]])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>计算池化后的像素值：</p>

\[\dfrac{1+1+1+1}{4} = 1\]

<p><strong><code class="language-plaintext highlighter-rouge">divisor_override=3</code> 的平均池化</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">img_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">avgpool_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">divisor_override</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">img_pool</span> <span class="o">=</span> <span class="n">avgpool_layer</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"raw_img:</span><span class="se">\n</span><span class="s">{}</span><span class="se">\n</span><span class="s">pooling_img:</span><span class="se">\n</span><span class="s">{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">,</span> <span class="n">img_pool</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>raw_img:
tensor([[[[1., 1., 1., 1.],
          [1., 1., 1., 1.],
          [1., 1., 1., 1.],
          [1., 1., 1., 1.]]]])
pooling_img:
tensor([[[[1.3333, 1.3333],
          [1.3333, 1.3333]]]])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>计算池化后的像素值：</p>

\[\dfrac{1+1+1+1}{3} = 1.3333\]

<p>目前为止，我们学习了最大池化和平均池化，它们都是对图像实现下采样的过程，即输入尺寸较大的图像，输出尺寸较小的图像。下面我们将学习反池化，即将小尺寸图像变为大尺寸图像。</p>

<h4 id="nnmaxunpool2d"><code class="language-plaintext highlighter-rouge">nn.MaxUnpool2d</code></h4>

<p><strong>功能</strong>：对二维信号（图像）进行最大值反池化上采样。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>
<span class="n">nn</span><span class="p">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>

<span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">kernel_size</code>：池化核尺寸。</li>
  <li><code class="language-plaintext highlighter-rouge">stride</code>：步长。</li>
  <li><code class="language-plaintext highlighter-rouge">padding</code>：填充个数。</li>
</ul>

<p><strong>最大值反池化：</strong></p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-null-16.png" width="80%" /></p>

<p>早期的自编码器和图像分割任务中都会涉及一个上采样的操作，当时普遍采用的方法是最大值反池化上采样。上图左半部分是最大池化过程，原始 $4\times 4$ 的图像经过最大池化后得到一个 $2\times 2$ 的下采样图像，然后经过一系列的网络层之后，进入上图右半部分的上采样解码器，即将一个尺寸较小的图像经过上采样得到一个尺寸较大的图像。此时，涉及到的一个问题是：我们应该将像素值放到什么位置。例如：右边 $2\times 2$ 图像中的左上角的 $3$ 应当放入最终 $4\times 4$ 图像中的左上部分的 $4$ 个像素中的哪一个？这时，我们就可以利用之前最大池化过程中记录的池化像素索引，将 $3$ 放入之前原始 $4\times 4$ 图像中左上角的 $4$ 个像素中最大值对应的位置。</p>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="c1"># pooling
</span><span class="n">img_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">maxpool_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">return_indices</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">img_pool</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">maxpool_layer</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>

<span class="c1"># unpooling
</span><span class="n">img_reconstruct</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">img_pool</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">maxunpool_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxUnpool2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">img_unpool</span> <span class="o">=</span> <span class="n">maxunpool_layer</span><span class="p">(</span><span class="n">img_reconstruct</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"raw_img:</span><span class="se">\n</span><span class="s">{}</span><span class="se">\n</span><span class="s">img_pool:</span><span class="se">\n</span><span class="s">{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">,</span> <span class="n">img_pool</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"img_reconstruct:</span><span class="se">\n</span><span class="s">{}</span><span class="se">\n</span><span class="s">img_unpool:</span><span class="se">\n</span><span class="s">{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">img_reconstruct</span><span class="p">,</span> <span class="n">img_unpool</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre>raw_img:
tensor([[[[0., 4., 4., 3.],
          [3., 3., 1., 1.],
          [4., 2., 3., 4.],
          [1., 3., 3., 0.]]]])
img_pool:
tensor([[[[4., 4.],
          [4., 4.]]]])
img_reconstruct:
tensor([[[[-1.0276, -0.5631],
          [-0.8923, -0.0583]]]])
img_unpool:
tensor([[[[ 0.0000, -1.0276, -0.5631,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000],
          [-0.8923,  0.0000,  0.0000, -0.0583],
          [ 0.0000,  0.0000,  0.0000,  0.0000]]]])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>这里，我们初始化一个 $4\times 4$ 的图像，并且采用一个 $2\times 2$ 的窗口，步长设置为 $2$。首先，我们对其进行最大值池化，并记录其中的最大值像素的索引。然后，我们进行反池化，这里反池化的输入和之前最大池化后得到的图像尺寸是一样的，并且反池化层的窗口和步长与之前最大池化层是一致的。最后，我们将输入和索引传入反池化层，得到与原始图像尺寸相同的图像。</p>

<h2 id="2-线性层">2. 线性层</h2>

<p><strong>线性层 (Linear Layer)</strong> 又称 <strong>全连接层 (Full-connected Layer)</strong>，其每个神经元与上一层所有神经元相连，实现对前一层的 <strong>线性组合/线性变换</strong>。</p>

<p>在卷积神经网络进行分类的时候，在输出之前，我们通常会采用一个全连接层对特征进行处理，在 PyTorch 中，全连接层又称为线性层，因为如果不考虑激活函数的非线性性质，那么全连接层就是对输入数据进行一个线性组合。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-WX20201216-205835%402x.png" width="80%" /></p>

<p>每个神经元都和前一层中的所有神经元相连，每个神经元的计算方式是对上一层的加权求和的过程。因此，线性层可以采用矩阵乘法来实现。注意，上图中我们暂时忽略了偏置项。</p>

<h4 id="nnlinear"><code class="language-plaintext highlighter-rouge">nn.Linear</code></h4>

<p><strong>功能</strong>：对一维信号（向量）进行线性组合。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">in_features</code>：输入结点数。</li>
  <li><code class="language-plaintext highlighter-rouge">out_features</code>：输出结点数。</li>
  <li><code class="language-plaintext highlighter-rouge">bias</code>：是否需要偏置。</li>
</ul>

<p><strong>计算公式</strong>：</p>

\[y = x W^{\mathrm T} + \text{bias}\]

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">linear_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
                                         <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
                                         <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span>
                                         <span class="p">[</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]])</span>
<span class="n">linear_layer</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">linear_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">linear_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>tensor([[1., 2., 3.]]) torch.Size([1, 3])
tensor([[1., 1., 1.],
        [2., 2., 2.],
        [3., 3., 3.],
        [4., 4., 4.]]) torch.Size([4, 3])
tensor([[ 6.5000, 12.5000, 18.5000, 24.5000]], grad_fn=&lt;AddmmBackward&gt;) torch.Size([1, 4])
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="3-激活函数层">3. 激活函数层</h2>

<p><strong>激活函数 (Activation Function)</strong> 是对特征进行非线性变换，赋予多层神经网络具有 <strong>深度</strong> 的意义。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-WX20201216-221810%402x.png" width="80%" /></p>

<p>在上面最后一步中，由于矩阵乘法的结合性，我们可以把右边三个权重矩阵先结合相乘，可以得到一个大的权重矩阵 $W$。这样我们可以看到，我们的 $\textit{Output}$ 实际上就是输入 $X$ 乘以一个大的权重矩阵 $W$。因此，这里的三层线性全连接层实际上等价于一个一层的全连接层，这是由于线性运算当中矩阵乘法的结合性导致的，并且这里我们没有引入非线性激活函数。如果加上 <strong>非线性激活函数</strong>，这一结论将不再成立，因此我们说，激活函数赋予了多层神经网络具有 <strong>深度</strong> 的意义。</p>

<h4 id="nnsigmoid"><code class="language-plaintext highlighter-rouge">nn.Sigmoid</code></h4>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-sigmoid.png" width="60%" /></p>

<p><strong>计算公式</strong>：</p>

\[y = \dfrac{1}{1+e^{-x}}\]

<p><strong>梯度公式</strong>：</p>

\[y' = y*(1-y)\]

<p><strong>特性</strong>：</p>

<ul>
  <li>输出值在 $(0,1)$，符合概率性质。</li>
  <li>导数范围是 $[0, 0.25]$，容易导致梯度消失。</li>
  <li>输出为非 $0$ 均值，会破坏数据分布。</li>
</ul>

<h4 id="nntanh"><code class="language-plaintext highlighter-rouge">nn.tanh</code></h4>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-tanh.png" width="60%" /></p>

<p><strong>计算公式</strong>：</p>

\[y= \dfrac{\sin x}{\cos x}=\dfrac{e^x - e^{-x}}{e^x + e^{-x}} = \dfrac{2}{1+e^{-2x}}+1\]

<p><strong>梯度公式</strong>：</p>

\[y' = 1- y^2\]

<p><strong>特性</strong>：</p>

<ul>
  <li>输出值在 $(- 1,1)$，数据符合 $0$ 均值。</li>
  <li>导数范围是 $(0, 1)$，容易导致梯度消失。</li>
</ul>

<h4 id="nnrelu"><code class="language-plaintext highlighter-rouge">nn.ReLU</code></h4>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-relu.png" width="60%" /></p>

<p><strong>计算公式</strong>：</p>

\[y= \max(0,x)\]

<p><strong>梯度公式</strong>：</p>

\[y'=\begin{cases}1 &amp; \end{cases}\]

<p>下节内容：nn 网络层：池化层、全连接层和激活函数层</p>
:ET