I"<h1 id="lecture-10-分布语义学">Lecture 10 分布语义学</h1>

<p>这节课我继续学习语义学相关内容，这次我们不再关注单词层面的语义学，而是从语料库中直接学习单词含义，这个领域也被称为分布语义学。</p>

<h2 id="1-分布语义学">1. 分布语义学</h2>
<h3 id="11-词汇数据库的问题">1.1 词汇数据库的问题</h3>
<ul>
  <li>需要手工构建
    <ul>
      <li>成本高</li>
      <li>人类的注解可能存在偏见和噪音</li>
    </ul>
  </li>
  <li>语言是动态的
    <ul>
      <li>新的单词：俚语、专业术语等等</li>
      <li>新的词义（senses）</li>
    </ul>
  </li>
  <li>互联网为我们提供了大量的文本，我们可以利用它们获得单词含义吗？</li>
</ul>

<h3 id="12-分布假设">1.2 分布假设</h3>
<ul>
  <li>“<em>You shall know a word by the company it keeps（你可以通过其周围的上下文单词来了解一个目标单词）</em>” —— (Firth, 1957)</li>
  <li>共现文档通常指示了主题（<strong>文档（document）</strong> 作为上下文）。
    <ul>
      <li>例如：$\textit{voting}$（投票）和 $\textit{politics}$（政治）<br />
如果我们观察文档，会发现这两个单词经常出现在同一文档中。因此，不同单词的共现文档在一定程度上反映了这些单词在某种主题方面的关联。</li>
    </ul>
  </li>
  <li>局部上下文反映了一个单词的语义类别（<strong>单词窗口（word window）</strong> 作为上下文）。
    <ul>
      <li>例如：$\textit{eat a pizza, eat a burger}$<br />
可以看到，$“\textit{pizza}”$ 和 $“\textit{burger}”$ 这两个单词都具有共同的局部上下文 $“\textit{eat a}”$，由此我们可以知道这两个单词都具有和 $“\textit{eat a}”$ 相关的某种含义。</li>
    </ul>
  </li>
</ul>

<h3 id="13-根据上下文猜测单词含义">1.3 根据上下文猜测单词含义</h3>

<ul>
  <li>
    <p>根据其用法来学习一个未知单词。</p>
  </li>
  <li>
    <p>例如：现在我们有一个单词 $\textit{tezgüino}$，我们并不知道其含义，我们试图通过从该单词的一些用法中学习到其含义。<br />
下面是该单词出现过的一些例句：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-132004%402x.png" width="50%" /></p>

    <p>作为人类，通过结合常识，我们可以大概猜测到 $\textit{tezgüino}$ 可能是某种含酒精饮品。</p>
  </li>
  <li>
    <p>我们再查看一下在相同（或者类似）上下文中的其他单词的情况。</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-132156%402x.png" width="50%" /></p>

    <p>可以看到，单词 $\textit{wine}$ 出现过的类似场景最多。因此，尽管我们并不知道 $\textit{tezgüino}$ 的具体含义，我们还是可以认为 $\textit{tezgüino}$ 和 $\textit{wine}$ 在单词含义方面非常相近。</p>
  </li>
</ul>

<h3 id="14-词向量">1.4 词向量</h3>

<p>在前面的例子中，我们可以将这些由 $0$ 和 $1$ 组成的行视为词向量，因为它们能够很好地代表这些用例。例如：给定 100 个非常好的例句，我们可以基于这些单词是否出现在这些例句中，将其转换为 100 维的向量。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-132156%402x.png" width="50%" /></p>

<ul>
  <li>每一行都可以视为一个 <strong>词向量（word vector）</strong>。</li>
  <li>它描述了单词的分布特性（目标单词附近的上下文单词信息）。</li>
  <li>捕获各种语义关系，例如：同义（synonymy）、类比（analogy）等。</li>
</ul>

<h3 id="15-词嵌入">1.5 词嵌入</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-133517%402x.png" width="40%" /></p>

<ul>
  <li>在之前的神经网络的章节中，我们已经见过另一种词向量：<strong>词嵌入（word embeddings）</strong>。
    <ul>
      <li>例如：在使用前馈神经网络进行文本分类时，第一层相当于是词嵌入层，该层的权重矩阵即词嵌入矩阵。</li>
    </ul>
  </li>
  <li>这里，我们将学习通过其他方法产生词向量：
    <ul>
      <li>基于计数的方法</li>
      <li>专为学习词向量而设计的更高效的神经网络方法</li>
    </ul>
  </li>
</ul>

<h2 id="2-基于计数的方法">2. 基于计数的方法</h2>
<p>首先，我们将学习如何通过基于计数的方法学习词向量。</p>
<h3 id="21-向量空间模型">2.1 向量空间模型</h3>
<p>这里，我们学习的第一个模型是 <strong>向量空间模型（Vector Space Model，VSM）</strong>。</p>
<ul>
  <li>基本思想：将单词含义表示为一个向量。</li>
  <li>通常，我们将 <strong>文档（documents）</strong>视为上下文。</li>
  <li>一个矩阵，两种视角：
    <ul>
      <li>一个文档由其所包含的单词表示</li>
      <li>一个单词由其出现过的文档表示</li>
    </ul>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-13-WX20200613-143123%402x.png" width="60%" /></p>

    <p>这里，每一行都表示语料库中的一个文档，每一列表示语料库的词汇表中的一个单词，单元格中的数字表示该单词在对应文档中出现的频率。例如：单词 $\textit{state}$ 没有在文档 $425$ 中出现过，所以对应的值为 $0$，但是它在文档 $426$ 中出现过 $3$ 次，所以对应值为 $3$。</p>

    <p>当我们构建完成这样一个矩阵后，我们可以从两种视角来看待它：</p>
  </li>
</ul>

<p>下节内容：</p>

:ET