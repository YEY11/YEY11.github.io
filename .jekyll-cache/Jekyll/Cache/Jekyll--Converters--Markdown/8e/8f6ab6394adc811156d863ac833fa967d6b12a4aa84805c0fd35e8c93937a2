I"|<h1 id="lecture-06-线性模型选择与正则化">Lecture 06 线性模型选择与正则化</h1>

<p><strong>参考教材</strong>：</p>

<ul>
  <li><em>Gareth, J., Daniela, W., Trevor, H., &amp; Robert, T. (2013). An intruduction to statistical learning: with applications in R. Spinger.</em></li>
  <li><em>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Spinger Science &amp; Business Media.</em></li>
</ul>

<h2 id="1-引言">1. 引言</h2>

<p>回忆一下标准的线性回归模型：</p>

<script type="math/tex; mode=display">Y=\beta_0 + \beta_1 X_1 +\cdots + \beta_p X_p + \epsilon</script>

<p>它通常用于描述响应变量 $Y$ 和一系列预测变量 $X_1,X_2,\dots,X_p$ 之间的线性关系。并且我们在之前课程中介绍了一种用于拟合此模型的经典方法：最小二乘法。</p>

<p>在接下来的课程中，我们将考虑一些扩展线性模型框架的方法。在第 7 章的课程中，我们对线性模型进行了推广，以使其适用于关系为 <strong>非线性 (non-linear)</strong>，但形式仍然 <strong>可加 (additive)</strong> 的情况。在第 8 章的课程中，我们将考虑一些更通用的 <strong>非线性模型</strong>。</p>

<p>尽管线性模型非常简单，但其在可解释性方面具有明显优势，并且通常显示出良好的预测性能。因此，在正式介绍非线性模型之前，我们将先介绍几种可替代普通最小二乘拟合的一些其他拟合方法，这些方法是对简单线性模型的改进。</p>

<p>那么，为什么要采用其他拟合方法替代最小二乘法呢?</p>

<p>因为与最小二乘法相比，其他拟合方法具有更高的 <strong>预测准确率 (prediction accuracy)</strong> 和更好的 <strong>模型可解释性 (model interpretability)</strong>。</p>

<ul>
  <li>
    <p><strong>预测准确率</strong>：若响应变量和预测变量的真实关系近似线性，则最小二乘估计的偏差较低。若 $n\gg p$，即观测个数 $n$ 远大于变量个数 $p$，则到最小二乘估计的方差通常较低，从而在测试样本集上有较好表现。然而，在不满足 $n$ 远大于 $p$ 的情况下，最小二乘拟合可能会发生较大变化，发生过拟合，从而使模型在测试样本集上表现较差。此外，若 $p &gt; n$，最小二乘法得到的系数估计结果不唯一：此时方差为 <strong>无穷大 (infinite)</strong>，这种情况下最小二乘法会失效。通过 <strong>限制 (constraining)</strong> 或 <strong>收缩 (shrinking)</strong> 待估计系数，以牺牲偏差为代价，显著减小估计量方差。这种方法可以显著提高模型在测试样本集上的预测准确率。</p>
  </li>
  <li>
    <p><strong>模型可解释性</strong>：通过删除不相关的特征，即通过将相应的系数估计设置为零，我们可以获得更易于解释的模型。 我们将介绍一些自动执行特征选择的方法</p>
  </li>
</ul>

<p>在多元回归模型中，常常存在一个或多个预测变量与响应变量不存在线性关系的情况，包括一些增加了模型的复杂性、却与模型 <strong>无关 (irrelevant)</strong> 的变量。通过移除这些无关变量，即将其系数设置为 $0$，可以使模型的可解释性更强，但运用最小二乘法很难将系数缩减至 $0$。本节课中，我们将介绍几种自动进行 <strong>特征选择 (feature selection)</strong> 或 <strong>变量选择 (variable selection)</strong> 的方法，以便将无关变量从多元回归模型中剔除。</p>

<p>除了最小二乘法，还有多种方法可用于拟合线性模型，其中既有经典方法又有现代方法。本节课中我们主要讨论以下三类重要的方法：</p>

<ul>
  <li>
    <p><strong>子集选择 (Subset selection)</strong>：该方法从 $p$ 个预测变量中挑选出与响应变量相关的变量，构建一个变量子集，再对这个缩减后的变量集合应用最小二乘法拟合模型。</p>
  </li>
  <li>
    <p><strong>收缩 (Shrinkage)</strong>：该方法基于全部 $p$ 个变量进行模型拟合。但是，与最小二乘估计相比，该方法的估计系数可以缩减为 $0$。这种收缩，也称 <strong>正则化 (regularization)</strong>，具有减少方差的效果。根据收缩的类型，某些系数估计可以正好为 $0$。因此，收缩法也可以用于变量选择。</p>
  </li>
  <li>
    <p><strong>降维 (Dimension reduction)</strong>：此方法将 $p$ 个原始预测变量投影到一个 $M$-维子空间中，其中 $M&lt; p$。这是通过计算原始变量的 $M$ 个不同的 <strong>线性组合 (linear combination)</strong> 或者 <strong>投影 (projections)</strong> 实现的。然后，我们将这 $M$ 个投影作为新的预测变量，用最小二乘法拟合线性回归模型。</p>
  </li>
</ul>

<p>本节课中，我们将详细介绍上述方法及其优缺点。虽然这里我们讨论的是之前线性回归模型的扩展，但其中涉及的概念同样适用于其他方法，例如之前介绍的分类模型。</p>

<h2 id="2-子集选择">2. 子集选择</h2>

<p>这里我们将介绍几种选择预测变量子集的方法，包括最优子集选择和逐步模型选择。</p>

<h3 id="21-最优子集选择">2.1 最优子集选择</h3>

<p><strong>最优子集选择 (best subset selection)</strong>：对 $p$ 个预测变量的所有可能组合分别使用最小二乘回归进行拟合。即拟合包含一个变量所有 $p$ 个的模型；拟合包含两个变量的所有 ${p \choose 2} = p(p-1)/2$ 个模型，以此类推。最后在所有可能模型中选取一个最优模型。</p>

<p><strong>最优子集选择算法</strong>：</p>

<ol>
  <li>令 $\mathcal M_0$ 表示 <strong>零模型 (null model)</strong>，即不包含任何预测变量的模型。该模型只是简单预测各观测的样本均值。</li>
  <li>对于 $k=1,2,\dots,p$：<br />
  (a) 拟合正好包含 $k$ 个预测变量的所有 ${p \choose k}$ 个模型。<br />
  (b) 从这 ${p \choose k}$ 个模型中选择具有最小 $\mathrm{RSS}$ 或者最大 $R^2$ 的模型作为最优模型，记为 $\mathcal M_k$。</li>
  <li>根据 $\mathrm{CV}$ 预测误差、$C_p\, (\mathrm{AIC})$、$\mathrm{BIC}$ 或者调整 $R^2$，从 $\mathcal M_0,\dots, \mathcal M_p$ 中选出一个最优模型。</li>
</ol>

<h4 id="例子信用卡数据集">例子：信用卡数据集</h4>

<p>图 1 是应用最优子集选择的一个例子。图中的每个点都对应一个用 <code class="language-plaintext highlighter-rouge">Credit</code> 数据集的 $11$ 个预测变量的不同子集建立的最小二乘回归模型。其中，<code class="language-plaintext highlighter-rouge">ethnicity</code> 是一个包含三个水平的分类变量，因此将其编码为两个虚拟变量。图中横坐标表示模型中包含的变量个数，在散点圈中绘制每个模型对应的 $\mathrm{RSS}$ 和 $R^2$ 值，用红色折线将不同模型复杂度下的 $\mathrm{RSS}$ 和 $R^2$ 对应的点联结起来。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-08-WX20201109-000907%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 1</span>：图中展示了 <code class="language-plaintext highlighter-rouge">Credit</code> 数据集中 $10$ 个预测变量所有可能子集构成的模型的 $\mathrm{RSS}$ 和 $R^2$ 值。红色折线表示给定预测变量个数的情况下，最优模型的 $\mathrm{RSS}$ 和 $R^2$ 值轨迹。数据集包含 $10$ 个预测变量，而 $x$ 轴的范围在 $1$ 到 $11$ 之间，这是由于其中有一个包含三种水平的分类变量，将其编码后引入了两个虚拟变量。</span></p>

<p>可以看到，随着变量个数增加，统计量得到改善。然而，在模型引入第三个变量之后，继续增加变量对于模型 $\mathrm{RSS}$ 和 $R^2$ 值的改善幅度将变得非常小。</p>

<p>尽管我们在这里为最小二乘回归提供最优子集选择，但是相同的思想也适用于其他类型的模型，例如逻辑回归。在逻辑回归中应用最优子集选择时，算法的步骤 2 中的 $\mathrm{RSS}$ 应当用 <strong>偏差 (deviance)</strong> 代替，两者的作用相同，但偏差的适用范围更广。偏差定义为负 $2$ 倍的最大似然函数，偏差越小，拟合优度越高。</p>

<h3 id="22-逐步选择">2.2 逐步选择</h3>

<p><strong>当 $p$ 很大时</strong>，最优子集选择算法存在以下两方面的问题：</p>

<ul>
  <li>
    <p>由于 <strong>运算效率</strong> 的限制，最优子集选择方法不再适用。</p>
  </li>
  <li>
    <p>另外，还存在一些统计学上的问题：随着搜索空间的增大，通过最优子集法找到的模型虽然在训练数据上有较好表现，但对新数据并不具备良好的预测能力。从一个巨大搜索空间中得到的模型通常会存在 <strong>过拟合</strong> 和 <strong>系数估计方差高</strong> 的问题。</p>
  </li>
</ul>

<p>基于上述的两大原因，与最优子集选择相比，逐步选择的优点是限制了搜索空间，从而提高了运算效率。</p>

<h4 id="前向逐步选择">前向逐步选择</h4>

<p><strong>前向逐步选择 (Forward stepwise selection)</strong> 从不包含任何预测变量的模型开始，然后每次添加一个预测变量到模型中，直到所有预测变量都被添加到模型中。特别地，在每一步中，算法将能够对拟合带来 <strong>最大额外提升</strong> 的变量添加到模型中。</p>

<p><strong>前向逐步选择算法</strong>：</p>

<ol>
  <li>令 $\mathcal M_0$ 表示 <strong>零模型 (null model)</strong>，即不包含任何预测变量的模型。</li>
  <li>对于 $k=0,1,2,\dots,p-1$：<br />
  (a) 考虑所有 $p-k$ 个模型，其中每个模型都在模型 $\mathcal M_k$ 的基础上增加一个变量。<br />
  (b) 从这 $p-k$ 个模型中选择具有最小 $\mathrm{RSS}$ 或者最大 $R^2$ 的模型作为最优模型，记为 $\mathcal M_{k+1}$。</li>
  <li>根据 $\mathrm{CV}$ 预测误差、$C_p\, (\mathrm{AIC})$、$\mathrm{BIC}$ 或者调整 $R^2$，从 $\mathcal M_0,\dots, \mathcal M_p$ 中选出一个最优模型。</li>
</ol>

<p>与最优子集选择相比，前向逐步选择在运算效率上具有明显优势。</p>

<p>虽然前向逐步选择在实际中有很好的应用，但它无法保证找到的模型是所有 $2^p$ 个模型中最优的。</p>

<p>例如，在给定的包含三个变量的数据集中，最优的单变量模型只包含变量 $X_1$，最优的双变量模型包含 $X_2$ 和 $X_3$。这种情况下，通过前向逐步选择方法无法得到包含双变量的最优模型，因为 $\mathcal M_1$ 包含变量 $X_1$，从而 $\mathcal M_2$ 只能包含 $X_1$ 及另一变量，$X_2$ 或 $X_3$。</p>

<p><strong>例子：信用卡数据集</strong></p>

<p>表 1 给出了在 <code class="language-plaintext highlighter-rouge">Credit</code> 数据集上使用最优子集选择和前向逐步选择的前四个模型的结果。</p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">表 1</span>：对 <code class="language-plaintext highlighter-rouge">Credit</code> 数据使用最优子集选择和前向逐步选择的前四个模型的结果。前三个模型选择结果相同，第四个模型选择结果不同。</span></p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-09-WX20201109-115551%402x.png" width="80%" /></p>

<p>可以看到，最优子集选择和前向逐步选择在最优单变量模型中都引入了 <code class="language-plaintext highlighter-rouge">rating</code> 变量，紧接着在双变量和三变量模型中依次引入了 <code class="language-plaintext highlighter-rouge">income</code> 和 <code class="language-plaintext highlighter-rouge">student</code> 变量。但是，最优子集选择在四变量模型中将 <code class="language-plaintext highlighter-rouge">rating</code> 变量替换为 <code class="language-plaintext highlighter-rouge">cards</code> 变量，而在前向逐步选择中必须要在四变量模型中依然保留 <code class="language-plaintext highlighter-rouge">rating</code> 变量。从之前的图 1 来看，三变量和四变量模型之间在 $\mathrm{RSS}$ 值上并没有太大差别，因此，这两种方法选出的四变量模型都是可用的。</p>

<p>在高维数据中，甚至 $n &lt; p$ 的情况下，依然可以使用前向逐步选择方法；在这种情况下，可以建立子模型 $\mathcal M_0,\dots,\mathcal M_{n-1}$，因为每个子模型都使用最小二乘法拟合，若 $p\gg n$，结果将是不唯一的。</p>

<h4 id="后向逐步选择">后向逐步选择</h4>

<p>与前向逐步选择一样，<strong>后向逐步选择 (backward stepwise selection)</strong> 为最优子集选择提供了一种有效的替代方法。</p>

<p>但是，与前向逐步选择不同，它从包含所有 $p$ 个预测变量的最小二乘模型开始，然后每次删除一个作用最小的预测变量。</p>

<p><strong>后向逐步选择算法</strong>：</p>

<ol>
  <li>令 $\mathcal M_p$ 表示 <strong>全模型 (full model)</strong>，即包含全部 $p$ 个预测变量的模型。</li>
  <li>对于 $k=p,p-1,\dots,1$：<br />
  (a) 考虑所有 $k$ 个模型，其中每个模型都在模型 $\mathcal M_k$ 的基础上减少一个变量，即每个模型都包含 $k-1$ 个变量。<br />
  (b) 从这 $k$ 个模型中选择具有最小 $\mathrm{RSS}$ 或者最大 $R^2$ 的模型作为最优模型，记为 $\mathcal M_{k-1}$。</li>
  <li>根据 $\mathrm{CV}$ 预测误差、$C_p\, (\mathrm{AIC})$、$\mathrm{BIC}$ 或者调整 $R^2$，从 $\mathcal M_0,\dots, \mathcal M_p$ 中选出一个最优模型。</li>
</ol>

<p>与前向逐步选择类似，后向逐步选择方法只需要对 $1+p(p+1)/2$ 个模型进行搜索。因此，当 $p$ 太大导致最优子集选择不适用时，可以采用该方法。</p>

<p>同样，与前向逐步选择类似，后向逐步选择方法无法保证得到的模型在 $p$ 个预测变量的所有 $2^p$ 个可能子集中是最优的。</p>

<p>后向逐步选择要求 <strong>样本数 $n$ 要大于变量数 $p$</strong> (保证全模型可以被拟合)。相反，前向逐步选择在 $n &lt; p$ 的情况下依然适用，因此当 $p$ 非常大的时候，前向逐步选择是唯一可行的子集选择方法。</p>

<h3 id="23-选择最优模型">2.3 选择最优模型</h3>

<p>最优子集选择、前向逐步选择和后向逐步选择等方法产生了一系列由 $p$ 个预测变量的子集所构建的模型。为了实现这些方法，需要找到一种确定最优模型的方法。</p>

<p>如前所述，包含所有预测变量的模型总是具有最小的 $\mathrm{RSS}$ 和最大的 $R^2$，因为这些统计量与 <strong>训练误差</strong> 有关。然而，我们希望找到具有 <strong>最小测试误差</strong> 的模型。回想一下，训练误差通常不能很好地评估测试误差。</p>

<p>因此，$\mathrm{RSS}$ 和 $R^2$ 并不适用于从一系列具有不同预测变量数量的模型中选择出最优模型。</p>

<p>为了达到基于测试误差选择最优模型的目的，我们需要估计测试误差。通常有两种方法：</p>

<ol>
  <li>根据过拟合导致的偏差对训练误差进行 <strong>调整</strong>，从而间接地估计测试误差。</li>
  <li>通过验证集方法或交叉验证方法，<strong>直接</strong> 估计测试误差。</li>
</ol>

<p>接下来，我们将对以上两种方法进行说明。</p>

<h4 id="c_pmathrmaicmathrmbic-和调整-r2">$C_p$、$\mathrm{AIC}$、$\mathrm{BIC}$ 和调整 $R^2$</h4>

<p>这些方法可以根据模型大小对训练误差进行调整，并且可以用于选择包含不同变量数量的模型。</p>

<p>这里，我们对最优子集选择在 <code class="language-plaintext highlighter-rouge">Credit</code> 数据集上产生的不同大小的模型，应用三种方法从中选出最优模型：$C_p$、$\mathrm{BIC}$ 和调整 $R^2$。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-09-WX20201109-133003%402x.png" width="90%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 2</span>：图中显示了在 <code class="language-plaintext highlighter-rouge">Credit</code> 数据集上应用 $C_p$、$\mathrm{BIC}$ 和调整 $R^2$ 对不同大小的模型进行最优子集选择的结果。$C_p$ 和 $\mathrm{BIC}$ 是测试均方误差的估计。中图显示，$\mathrm{BIC}$ 作为测试误差的估计在模型引入四个变量后开始增加。左图和右图中，$C_p$ 和调整 $R^2$ 在模型引入四个变量后开始变得相对平缓。</span></p>

<h5 id="mallows-c_p">Mallow’s $C_p$</h5>

<p>用最小二乘法拟合模型，$C_p$ 值的计算公式如下：</p>

<script type="math/tex; mode=display">C_p= \dfrac{1}{n}(\mathrm{RSS} + 2d \hat \sigma^2)</script>

<p>其中，$d$ 是模型包含的预测变量的数量，$\hat \sigma^2$ 是与各响应变量测量相关的误差项 $\epsilon$ 的方差估计。</p>

<h5 id="mathrmaic-准则">$\mathrm{AIC}$ 准则</h5>

<p>$\mathrm{AIC}$ 准则适用于许多使用最大似然法进行拟合的模型：</p>

<script type="math/tex; mode=display">\mathrm{AIC} = -2 \log L + 2\cdot d</script>

<p>其中，$L$ 是估计模型的似然函数的最大值。</p>

<p>在具有高斯误差的线性模型的情况下，最大似然估计和最小二乘估计是等价的，并且 $C_p$ 和 $\mathrm{AIC}$ 相等。我们可以证明这一点。</p>

<h5 id="mathrmbic-准则">$\mathrm{BIC}$ 准则</h5>

<p>$\mathrm{BIC}$ 是从贝叶斯观点中衍生出来的，然而最终也和 $C_p$ (以及 $\mathrm{AIC}$) 非常相似。对于包含 $d$ 个变量的最小二乘模型，$\mathrm{BIC}$ 通常由下式给出：</p>

<script type="math/tex; mode=display">\mathrm{BIC} = \dfrac{1}{n}(\mathrm{RSS} + \log (n)d \hat \sigma^2)</script>

<p>类似 $C_p$，测试误差较低的模型的 $\mathrm{BIC}$ 值也较低，因此通常我们选择具有最低 $\mathrm{BIC}$ 值的模型作为最优模型。</p>

<p>请注意，$\mathrm{BIC}$ 用 $\log (n)d \hat \sigma^2$ 代替了 $C_p$ 中的 $2d\hat \sigma^2$，其中 $n$ 是观测数。</p>

<p>由于对于任意的 $n&gt; 7$，都有 $\log n &gt; 2$，所以 $\mathrm{BIC}$ 统计量通常会对包含多个变量的模型施加较大的惩罚。因此，与 $C_p$ 相比，$\mathrm{BIC}$ 选择的模型通常要小一些。</p>

<h5 id="调整-r2">调整 $R^2$</h5>

<p>对于包含 $d$ 个变量的最小二乘模型，调整 $R^2$ 统计量由下式计算得到：</p>

<script type="math/tex; mode=display">\text{Adjusted } R^2=1- \dfrac{\mathrm{RSS}/(n-d-1)}{\mathrm{TSS}/(n-1)}</script>

<p>其中，$\mathrm{TSS}=\sum_{i=1}^{n}(y_i - \overline y)^2$ 是响应变量的总平方和。</p>

<p>与 $C_p$、$\mathrm{AIC}$、$\mathrm{BIC}$ 越小表示模型测试误差越低不同，调整 $R^2$ 越大，模型测试误差越低。</p>

<p>最大化调整 $R^2$ 等价于最小化 $\frac{\mathrm{RSS}}{n-d-1}$。虽然 $\mathrm{RSS}$ 总是随模型中的变量数量的增加而减小，但是由于分母项中包含 $d$，所以 $\frac{\mathrm{RSS}}{n-d-1}$ 有可能增大也有可能减小。</p>

<p>不同于 $R^2$ 统计量，调整 $R^2$ 统计量对加入非必要变量的模型 <strong>引入了惩罚</strong>。</p>

<h4 id="验证集法与交叉验证法">验证集法与交叉验证法</h4>

<p>这两种方法都会返回一系列由模型大小 $k=0,1,2,\dots$ 索引的模型 $\mathcal M_k$。我们的目标是选择一个 $\hat k$，然后返回模型 $\mathcal M_{\hat k}$。</p>

<p>我们将为所考虑的每个模型 $\mathcal M_k$ 计算验证集误差或者交叉验证误差，然后选择使估计测试误差最小的 $k$。</p>

<p>与 $C_p$、$\mathrm{AIC}$、$\mathrm{BIC}$ 和调整 $R^2$ 相比，这种方法有一个好处，它提供了测试误差的一个直接估计，而 <strong>无需估计误差项的方差 $\sigma^2$</strong>。</p>

<p>这种方法可以用于更广泛的模型选择任务中，即使对于难以精确确定模型自由度 (例如，模型中的预测变量个数) 或者难以估计误差项方差 $\sigma^2$ 的情况，该方法同样适用。</p>

<p>图 3 显示了 <code class="language-plaintext highlighter-rouge">Credit</code> 数据集包含 $d$ 个变量的最优模型，曲线表示 $\mathrm{BIC}$、验证集误差和交叉验证误差作为 $d$ 的函数的取值。验证集误差通过随机选择四分之三的观测作为训练集，其余观测作为验证集计算得到。交叉验证误差按 $k= 10$ 折进行计算。在这种情况下，验证集方法和交叉验证方法得到的模型都包含 $6$ 个变量。然而，所有这三种方法表明，分别包含 $4$、$5$、$6$ 个变量的模型的测试误差都大致相等。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-09-WX20201109-143814%402x.png" width="90%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 3</span>：图中显示了 <code class="language-plaintext highlighter-rouge">Credit</code> 数据集包含 $d$ 个变量的最优模型的三种统计量的值，$d$ 的范围在 $1$ 到 $11$ 之间。基于各统计量得到的全局最优模型用 “$\times$” 表示。<strong>左图</strong>：$\mathrm{BIC}$ 的平方根。<strong>中图</strong>：验证集误差。<strong>右图</strong>：交叉验证误差。</span></p>

<p>事实上，图 3 的中图与右图的测试误差估计曲线均比较平坦。与两变量模型相比，三变量模型具有明显更低的测试误差估计，而当模型中变量个数在 $3$ 到 $11$ 之间变化时，测试误差估计值的差别不大。</p>

<p>此外，如果对不同的训练集和验证集划分重复使用验证集方法，或者对于不同的折数重复使用交叉验证方法，会得到不同的具有最低测试误差的精确模型。</p>

<p>针对这种情况，可以使用 <strong>一个标准误原则 (one-standard-error rule)</strong> 进行模型选择：首先计算不同大小的模型的测试均方误差估计值的标准误，然后选择测试误差估计值在曲线最低点附近一个标准误范围内且预测变量数最少的模型。这种做法的原因是在一系列效果近似相同的模型中，总是倾向于选择最简单的模型。</p>

<p>上面的例子中，对验证集方法和交叉验证方法使用一个标准误原则选择出的模型包含三个变量。</p>

<h2 id="3-收缩方法">3. 收缩方法</h2>

<p>前面介绍的子集选择方法使用最小二乘法对包含预测变量子集的线性模型进行拟合。除此之外，我们还可以使用对系数进行 <strong>约束 (constrains)</strong> 或 <strong>正则化 (regularizes)</strong> 的技术来对包含所有 $p$ 个预测变量的模型进行拟合，也就是说，将系数估计值向 $0$ 的方向压缩。至于为什么这种约束条件可以改善拟合效果，其原理并非显而易见。但事实证明，收缩系数可以显著减小其方差。</p>

<p>接下来，我们将介绍两种最常见的收缩方法：<strong>岭回归 (Ridge regression)</strong> 和 <strong>Lasso</strong>。</p>

<h3 id="31-岭回归">3.1 岭回归</h3>

<p>回忆一下，最小二乘拟合通过最小化下式来估计 $\beta_0,\beta_1,\dots,\beta_p$：</p>

<script type="math/tex; mode=display">\mathrm{RSS}=\sum_{i=1}^{n}\left(y_i - \beta_0 - \sum_{j=1}^{p}\beta_j x_{ij}\right)^2</script>

<p>相比之下，岭回归的估计系数 $\hat \beta^R$ 通过最小化下式得到：</p>

<script type="math/tex; mode=display">\sum_{i=1}^{n}\left(y_i - \beta_0 - \sum_{j=1}^{p}\beta_j x_{ij}\right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = \mathrm{RSS} + \lambda \sum_{j=1}^{p}\beta_j^2</script>

<p>其中，$\lambda \ge 0$ 是一个 <strong>调节参数 (tuning parameter)</strong>，需要单独确定。</p>

<p>与最小二乘法一样，岭回归通过使 $\mathrm{RSS}$ 很小，来寻找能够较好拟合数据的系数估计。</p>

<p>然而，岭回归还增加了一个被称为 <strong>收缩惩罚 (shrinkage penalty)</strong> 的项 $\lambda \sum_{j=1}^{p}\beta_j^2$。当 $\beta_1,\dots,\beta_p$ 接近 $0$ 时，该项会变得很小。因此，它具有将回归系数 $\beta_j$ 的估计向 $0$ 的方向 <strong>压缩</strong> 的效应。</p>

<p>调节参数 $\lambda$ 的作用是控制这两项对回归系数估计的相对影响程度。当 $\lambda=0$ 时，收缩惩罚项不会产生作用，岭回归与最小二乘估计结果相同。随着 $\lambda \to \infty$，收缩惩罚项的影响力增加，岭回归系数估计值越来越接近 $0$。</p>

<p>与最小二乘法得到一个唯一的估计结果不同，岭回归得到的系数估计 $\hat \beta_{\lambda}^R$ 会随 $\lambda$ 的变化而变化。选择一个合适的 $\lambda$ 值非常重要，我们可以使用交叉验证法进行选择。</p>

<h4 id="例子信用卡数据集-1">例子：信用卡数据集</h4>

<p>图 4 显示了 <code class="language-plaintext highlighter-rouge">Credit</code> 数据集的岭回归系数估计值的变化情况：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-09-WX20201109-153911%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 4</span>：图中显示了 <code class="language-plaintext highlighter-rouge">Credit</code> 数据集上的标准化岭回归系数随着 $\lambda$ 和 $\|\hat \beta_{\lambda}^R\|_2 / \|\hat \beta\|_2$ 的变化情况。</span></p>

<p>在图 4 的左图中，每条曲线都代表 (全部 $10$ 个变量中的) 一个变量的岭回归系数估计，并且被绘制为一个关于 $\lambda$ 的函数。这里，不同颜色和类型的曲线代表不同变量的岭回归估计系数随 $\lambda$ 变化而变化的情况。当 $\lambda=0$ 时 (即左图中的最左边)，此时各变量的岭回归系数估计和最小二乘回归估计值相同。但是，随着 $\lambda$ 的增加，岭回归系数估计朝着 $0$ 的方向收缩。当 $\lambda \to \infty$ 时，所有变量的岭回归系数估计都接近 $0$，此时得到的模型相当于不包含任何变量的零模型。</p>

<p>图 4 中的右图与左图一样，都显示了岭回归估计系数的变化。不过，这里的 $x$ 轴由之前的 $\lambda$ 变为了 <script type="math/tex">\|\hat \beta_{\lambda}^R\|_2 / \|\hat \beta\|_2</script>，其中，$\hat \beta$ 表示最小二乘估计系数的向量。<script type="math/tex">\|\beta\|_2</script> 表示某个向量 $\beta$ 的 <script type="math/tex">\ell_2</script> 范数，其定义为 <script type="math/tex">\|\beta\|_2 = \sqrt{\sum_{j=1}^{p}\beta_j^2}</script>，它衡量了向量空间中 $\beta$ 到原点的距离。随着 $\lambda$ 增加，<script type="math/tex">\hat \beta_{\lambda}^R</script> 的 <script type="math/tex">\ell_2</script> 范数降低，从而 <script type="math/tex">\|\hat \beta_{\lambda}^R\|_2 / \|\hat \beta\|_2</script> 也降低。后者的取值范围在 $1$ (当 $\lambda=0$ 时，岭回归系数估计与最小二乘估计结果相同，二者的 <script type="math/tex">\ell_2</script> 范数也相同) 到 $0$ (当 $\lambda =\infty$ 时，岭回归系数估计是一个零向量，其 <script type="math/tex">\ell_2</script> 范数值也为 $0$) 之间。因此，可以图 4 中右图的 $x$ 轴看作是岭回归系数估计值向 $0$ 方向收缩的程度，数值较小意味着系数估计值已经压缩得非常接近于 $0$。</p>

<h4 id="变量缩放">变量缩放</h4>

<p>我们之前讨论过，最小二乘系数估计是 <strong>尺度等变的 (scale equivariant)</strong>：将 $X_j$ 乘以一个常数 $c$，最小二乘系数估计的结果将乘以一个因子 $1/c$。换而言之，无论第 $j$ 个预测变量如何按比例缩放，$X_j\hat \beta_j$ 始终保持不变。</p>

<p>相比之下，如果将预测变量乘以一个常数，岭回归系数估计值可能会发生 <strong>显著改变</strong>。这是由岭回归目标函数中惩罚项部分的系数平方和导致的。也就是说，$X_j \hat \beta_{j,\lambda}^R$ 的值不只取决于 $\lambda$，还取决于第 $j$ 个变量的尺度，甚至还可能受到其他预测变量的尺度的影响。</p>

<p>因此，在使用岭回归之前，最好先将 <strong>预测变量标准化 (standardizing the predictors)</strong>：</p>

<script type="math/tex; mode=display">\tilde x_{ij} = \dfrac{x_{ij}}{\sqrt{\dfrac{1}{n}\sum_{i=1}^{n}(x_{ij}- \overline x_j)^2}}</script>

<h4 id="为什么岭回归要比最小二乘回归效果更好">为什么岭回归要比最小二乘回归效果更好？</h4>

<p>与最小二乘相比，岭回归的优势在于 <strong>偏差-方差权衡 (bias-variance trade-off)</strong>。随着 $\lambda$ 的增加，岭回归拟合的灵活度降低，这将导致方差降低，偏差增加。</p>

<p>图 5 使用了一个包含 $p=45$ 个预测变量和 $n=50$ 个观测的模拟数据集。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-09-WX20201109-164754%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 5</span>：图中且示了岭回归在模拟数据集上预测结果的偏差平方 (黑色)、方差 (绿色) 和测试均方误差 (紫色)，随 $\lambda$ 和 $\|\hat \beta_{\lambda}^R\|_2 / \|\hat \beta\|_2$ 的变化情况。水平虚线表示 $\mathrm{MSE}$ 可能的最低水平。紫色曲线与水平虚线的交点代表均方误差最低时对应的岭回归模型。</span></p>

<p>通常来说，当响应变量和预测变量关系近似线性时，最小二乘估计将具有较低的偏差和较高的方差。这意味着训练集数据一个微小改变可能会导致最小二乘系数的较大变化。特别地，当变量个数 $p$ 和观测个数 $n$ 差不多大的时候 (如图 5 中的情况)，最小二乘估计的方差会很大。如果 $p &gt; n$，则最小二乘估计没有唯一解，此时岭回归仍然能够通过小幅度增加偏差换取方差的大幅下降，并通过这种权衡获得比较好的模型效果。因此，当最小二乘估计的方差很大时，岭回归的效果更好。</p>

<p>另外，与需要搜索 $2^p$ 个模型的最优子集选择方法相比，岭回归在运算上也具有显著优势。如前所述，即使对于变量数 $p$ 不大的情况，全局搜索方式在运算上也不可行。相比之下，对于任意给定的 $\lambda$，岭回归仅拟合单个模型，并且模型拟合过程可以非常快速地执行。实际上，可以证明，对于所有 $\lambda$ 值，同时求解岭回归目标函数所需的计算量与使用最小二乘法拟合模型的计算量几乎相同。</p>

<h3 id="32-lasso">3.2 Lasso</h3>

<p>然而，岭回归有一个明显的缺点：与子集选择不同，子集选择通常将选择仅涉及变量子集的模型，而岭回归将包括最终模型中的所有p个预测变量</p>

<p>•套索是相对较新的山脊回归方法，克服了这一缺点。 套索系数βλL ˆ最小化</p>

<p>下节内容：线性模型选择与正则化</p>
:ET