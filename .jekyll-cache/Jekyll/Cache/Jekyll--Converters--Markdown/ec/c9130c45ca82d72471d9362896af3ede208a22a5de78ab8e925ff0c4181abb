I"#<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-09-无模型强化学习q-学习-和-sarsa">Lecture 09 无模型强化学习：Q-学习 和 SARSA</h1>

<p><strong>主要内容：</strong></p>
<ol>
  <li>动机</li>
  <li>强化学习</li>
  <li>Q-学习</li>
  <li>SARSA</li>
  <li>总结</li>
</ol>

<h2 id="1-动机">1. 动机</h2>
<h3 id="11-学习成果">1.1 学习成果</h3>
<ol>
  <li>识别在哪些情况下，无模型强化学习适用于求解 MDP 问题。</li>
  <li>解释无模型规划与基于模型规划之间的差异。</li>
  <li>应用 Q-学习 和 SARSA 手动解决小规模 MDP 问题，并编写 Q-学习 和 SARSA 算法代码自动求解中等规模 MDP 问题。</li>
  <li>比较和对比非策略强化学习与策略强化学习。</li>
</ol>

<h3 id="12-规划与学习">1.2 规划与学习</h3>

<p>到目前为止，我们已经学习了盲目/启发式搜索和价值/策略迭代。</p>

<ul>
  <li>
    <p>搜索和价值/策略迭代都属于 <strong>基于模型</strong> 技术。这意味着我们需要知道模型；具体来说，我们知道 $P_a(s’\mid s)$ 和 $r(s,a,s’)$。</p>
  </li>
  <li>
    <p>Q-学习 和 SARSA 则属于 <strong>无模型</strong> 技术。这意味着我们不知道 $P_a(s’\mid s)$ 和 $r(s,a,s’)$。</p>
  </li>
  <li>
    <p><strong>如果我们不知道转移和回报，我们该如何计算策略呢？</strong>我们通过尝试行动并观察结果，<strong>从经验中学习</strong>，从而使它成为一个机器学习问题。</p>
  </li>
  <li>
    <p>重要的是，在无模型强化学习中，我们不会尝试学习 $P_a(s’\mid s)$ 或 $r(s,a,s’)$ —— 我们将直接学习策略。</p>
  </li>
  <li>
    <p>另外，有些技术则介于基于模型和无模型之间：基于模拟的技术。在这种情况下，我们将模型视为一个 <strong>模拟器</strong>，因此我们可以利用无模型技术来 <strong>模拟</strong> $P_a(s’\mid s)$ 和 $r(s,a,s’)$ 并学习策略。</p>
  </li>
</ul>

<h2 id="2-强化学习">2. 强化学习</h2>
<h3 id="21-例子神秘游戏">2.1 例子：神秘游戏</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-05-23-WX20200523-193132%402x.png" width="40%" /></p>

<p><a href="https://programmingheroes.blogspot.com/2016/02/udacity-reinforcement-learning-mystery-game.html">游戏链接</a>：该游戏的目的是通过实验了解计算机是如何学习的。游戏通过按键盘上的数字 $1$ 到 $6$ 键进行操作。你需要了解行动产生的结果以及如何赢得比赛。</p>

<p>当你做得很好或很差时，会出现一些回报值。当你完成游戏时，会出现一行短语 “ You Win :)” 。祝你好运！</p>

<ul>
  <li>你采取了什么程序？</li>
  <li>你学到了什么？</li>
  <li>你使用了什么假设？</li>
</ul>

<p>$\to$ 想象一下，对于没有任何假设或直觉的计算机来说有多难！</p>

<center><video width="300" controls="">
  <source src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-05-23-output.mp4" type="video/mp4" />
</video></center>

<p>现在，我们来玩这个游戏。我们可以看到游戏界面是一个 $6\times 6$ 的网格组成，其中有一些不同颜色和形状的色块。刚开始，我们对于游戏规则、获胜条件等一无所知，我们只知道一共有 $6$ 种行动，分别对应键盘上的数字 $1$ 到 $6$，当我们按下这些数字键时，会执行一些行动。</p>

<p>这个游戏背后的思想是：我们并不知道应该做什么，我们要做的就是开始探索。所以，一开始我们可能只是随机地按下键盘上的数字 $1$ 到 $6$ 键，然后进行观察，我们发现，通过这些数字键，我们可以控制较小的绿色方块的行为：</p>

<ul>
  <li>$1$：向上移动</li>
  <li>$2$：向左移动</li>
  <li>$3$：向右移动</li>
  <li>$4$：放下圆形色块</li>
  <li>$5$：向下移动</li>
  <li>$6$：拾取圆形色块</li>
</ul>

<p>并且，我们发现，黑色色块和地图边缘起到了墙的作用（会阻挡方块移动），当我们成功拾取圆形色块时会得到 $+1$ 的回报，当我们在非指定区域放下圆形色块时会得到 $-1$ 的回报，而当我们将圆形色块运送到指定区域（四个角落上对应颜色区域，即右上角的绿色区）时，我们将获得胜利（“ You Win :)”）。</p>

<p>此外，当刷新页面重新开始游戏时，我们会发现可移动小方块和圆形色块的颜色发生了变化（例如：红色），作为人类，我们可以很容易推测此时指定区域也发生了相应的变化（即左上角的红色区），但是对于计算机而言，做到这点并不容易。</p>

<p>我们为什么玩这个游戏？这是在无模型环境下应用强化学习的一个例子。即，我们并不知道环境相关信息，我们只知道我们能够采取的行动，这有点类似于人类的学习过程：我们体验一些事物，尝试一些事物，有时结果比较理想，有时则不如预期，我们学习如何在这个世界中行事。</p>

<h3 id="22-ai-规划和学习的方法">2.2 AI 规划和学习的方法</h3>
<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-05-23-WX20200523-215305%402x.png" width="70%" /></p>

<h3 id="23-强化学习基础">2.3 强化学习：基础</h3>
<p>强化学习有许多不同的模型，它们都具有相同的基础：</p>

<ul>
  <li>我们对要解决的问题执行了许多不同的 <strong>episodes</strong>（注：agent 根据某个策略执行一系列行动到结束就是一个 episode），从中我们学习到一个 <strong>策略</strong>。</li>
  <li>在学习过程中，我们尝试学习在特定状态下应用特定行为的价值。</li>
  <li>在每个 episode 中，我们需要执行一些行动。每次行动完成后，我们都会得到一个回报（可能为 $0$），并且我们可以看到新的状态。</li>
  <li>由此，我们 <strong>强化（reinforce）</strong>了在先前状态下应用先前行动的估计。</li>
  <li>我们在遇到以下情况时终止：（1）我们的训练时间用完了；（2）我们认为我们的策略已经收敛到了最优策略（对于每个新的 episode，我们不再看到任何改进）；或者（3）我们的策略已经 “足够好”（对于每个新的 episode，我们只能看到非常小的改进）。</li>
</ul>

<h3 id="23-基于模型-vs-无模型">2.3 基于模型 vs. 无模型</h3>

<p>强化学习中一个非常重要的概念是 <strong>基于模型（model-based）</strong>和 <strong>无模型（model-free）</strong>的区别。</p>

<p>为了快速理解这点，我们回顾一下之前的马尔可夫决策过程（MDP），在 MDP 中，我们至少具备以下 4 点（当然，我们也可以再加一项初始状态 $s_0$）：</p>

<ul>
  <li>状态集合 $S$</li>
  <li>转移概率 $P_a(s’\mid s)$</li>
  <li>回报函数 $r(s,a,s’)$</li>
  <li>折扣因子 $\gamma$</li>
</ul>

<p>当我们在价值/策略迭代或者 MCTS 中谈论它们时，我们是确切知道转移概率 $P_a(s’\mid s)$ 和回报函数 $r(s,a,s’)$ 的，我们可以利用它们来计算期望回报。但是，在无模型环境下，这两项是未知的。</p>

<p>例如：在前面的神秘游戏的例子中，我们一开始并不知道我们能采取哪些行动，我们只知道总共可以采取 $6$ 种行动，我们并不知道什么时候可以采取哪种行动，以及相应的回报是什么。作为人类，我们已经具备了大量的先验知识来帮助我们发现其中的规则，我们实际上是在此基础上建立了一个该问题的模型：$1$ 表示向上移动、$6$ 表示拾取圆形色块等等。而在无模型强化学习中，我们实际上不会建立模型，我们所做的是从 <strong>经验（experience）</strong>直接映射到 <strong>策略（policy）</strong>，而并非从动态的环境中学习任何事情。我们只知道采取某些行动后，我们将观察到一些特定的状态以及一些特定的回报，</p>

<p>此外，还有一种介于基于模型和无模型之间：<strong>基于模拟（simulation-based）</strong>。在这种情况下，我们并没有转移概率的显式模型，但是我们可以通过代码模拟估计转移概率。我们可以将无模型强化学习应用在模拟器上，从而学习到一个策略，如果我们模拟得很好，该策略应当表现不错。</p>

<h3 id="24-时序差分学习">2.4 时序差分学习</h3>
<p>关于无模型强化学习，这里，我们将主要关注 <strong>时序差分学习（Temporal Difference Learning）</strong>。</p>

<h2 id="3-q-学习">3. Q-学习</h2>

<p>下节内容：蒙特卡洛树搜索：利用和探索的权衡</p>

:ET