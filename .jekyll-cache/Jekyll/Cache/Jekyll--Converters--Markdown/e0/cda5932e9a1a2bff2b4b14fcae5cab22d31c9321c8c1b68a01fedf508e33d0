I"<h1 id="lecture-15-损失函数-一">Lecture 15 损失函数 (一)</h1>

<p>在前几节课中，我们学习了模型模块中的一些知识，包括如何构建模型以及怎样进行模型初始化。本节课我们将开始学习损失函数模块。</p>

<h2 id="1-损失函数的概念">1. 损失函数的概念</h2>

<p><strong>损失函数 (Loss Function)</strong>：衡量模型输出与真实标签之间的差异。</p>

<p>下面是一个一元线性回归的拟合过程示意图：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-20-WX20201220-200705%402x.png" width="60%" /></p>

<p>图中的绿色方块代表训练样本点 $(x_i, y_i)$，蓝色直线代表训练得到的模型 $\hat y = w_0 + w_1 x$，其中，$w_0$ 代表截距，$w_1 = \Delta y / \Delta x$ 代表斜率。可以看到，模型并没有完美地拟合每一个数据点，所以数据点和模型之间存在一个 <strong>损失 (Loss)</strong>，这里我们采用垂直方向上模型输出与真实数据点之差的绝对值 $|\hat y -y|$ 作为损失函数的度量。</p>

<p>另外，当我们谈到损失函数时，经常会涉及到以下三个概念：</p>

<ul>
  <li>
    <p><strong>损失函数 (Loss Function)</strong>：计算单个样本的差异。</p>

\[\mathrm{Loss} = f(\hat y, y)\]
  </li>
  <li>
    <p><strong>代价函数 (Cost Function)</strong>：计算整个训练集 $\mathrm{Loss}$ 的平均值。</p>

\[\mathrm{Cost} = \dfrac{1}{n}\sum_{i=1}^{n} f(\hat y_i, y_i)\]
  </li>
  <li>
    <p><strong>目标函数 (Objective Function)</strong>：最终需要优化的目标，通常包含代价函数和正则项。</p>

\[\mathrm{Obj} = \mathrm{Cost} + \mathrm{Regularization}\]
  </li>
</ul>

<p>注意，代价函数并不是越小越好，因为存在过拟合的风险。所以我们需要加上一些约束 (即正则项) 来防止模型变得过于复杂而导致过拟合，常用的有 L1 和 L2 正则项。因此，代价函数和正则项最终构成了我们的目标函数。</p>

<p>下面我们来看一下 PyTorch 中的 <code class="language-plaintext highlighter-rouge">_Loss</code> 类：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">_Loss</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">or</span> <span class="nb">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="p">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="nb">reduce</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，<code class="language-plaintext highlighter-rouge">_Loss</code> 是继承于 <code class="language-plaintext highlighter-rouge">Module</code> 类的，所以从某种程度上我们可以将 <code class="language-plaintext highlighter-rouge">_Loss</code> 也视为一个网络层。它的初始化函数中主要有 3 个参数，其中 <code class="language-plaintext highlighter-rouge">size_average</code> 和 <code class="language-plaintext highlighter-rouge">reduce</code> 这两个参数即将在后续版本中被舍弃，因为 <code class="language-plaintext highlighter-rouge">reduction</code> 参数已经可以实现前两者的功能。</p>

<h2 id="2-交叉熵损失函数">2. 交叉熵损失函数</h2>

<h4 id="nncrossentropyloss"><code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss</code></h4>

<p><strong>功能</strong>：<code class="language-plaintext highlighter-rouge">nn.LogSoftmax()</code> 与 <code class="language-plaintext highlighter-rouge">nn.NLLLoss()</code> 结合，进行交叉熵计算。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span>
    <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>注意：这里的计算过程和交叉熵公式存在一些差异。主要差别在于这里使用了一个 Softmax 函数对数据进行了归一化处理，使其落在一个正常的概率值范围内。这是因为交叉熵函数经常用于分类任务，而在分类任务中我们常常需要计算不同类别的概率值。所以交叉熵可以用来衡量两个概率分布之间的差异，交叉熵值越低说明两个概率分布越接近。</p>

<p>那么为什么交叉熵值越低，两个概率分布越接近呢？这需要从它与信息熵和相对熵之间的关系说起：</p>

<p><span><center>交叉熵 $=$ 信息熵 $+$ 相对熵</center></span></p>

<p>我们先来看最基本的 <strong>熵 (Entropy)</strong> 的概念：熵准确来说应该叫做 <strong>信息熵 (Information Entropy)</strong>，它是由信息论之父香农从热力学中借鉴过来的一个概念，用于描述某个事件的不确定性：某个事件不确定性越高，它的熵就越大。例如：“明天下雨” 这一事件要比 “明天太阳会升起” 这一事件的熵大得多，因为前者的不确定性较高。这里我们需要引入 <strong>自信息</strong> 的概念。</p>

<ul>
  <li>
    <p><strong>自信息 (Self-information)</strong>：用于衡量单个事件的不确定性。</p>

\[I(X) = -\log [p(X)]\]

    <p>其中，$p(X)$ 为随机事件</p>
  </li>
  <li>
    <p><strong>熵 (Entropy)</strong>：自信息的期望。</p>

\[H(P) = \mathrm{E}_{x\sim P}\]
  </li>
</ul>

<p>主要参数：</p>

<p>• weight ：各类别的 loss 设置权值</p>

<p>• ignore_index ：忽略某个类别</p>

<p>• reduction ：计算模式， 可为 none/sum/mean none - 逐个元素计算 sum - 所有元素求和， 返回标量 mean - 加权平均， 返回标量</p>

<p>下节内容：损失函数 (一)</p>
:ET