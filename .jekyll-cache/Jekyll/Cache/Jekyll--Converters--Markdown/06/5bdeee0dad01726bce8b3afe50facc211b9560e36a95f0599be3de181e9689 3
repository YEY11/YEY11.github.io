I"ʮ<h1 id="lecture-05-autograd-与逻辑回归">Lecture 05 autograd 与逻辑回归</h1>

<p>本节课主要分为两部分：PyTorch 中的自动求导系统以及逻辑回归模型。我们知道，深度模型的训练就是不断地更新权值，而权值的更新需要求解梯度，因此，梯度在我们的模型训练过程中是至关重要的。然而，求解梯度通常十分繁琐，因此，PyTorch 中引入了自动求导系统帮助我们完成这一过程。在 PyTorch 中，我们无需手动计算梯度，只需要搭建好前向传播的计算图，然后根据 PyTorch 中的 <code class="language-plaintext highlighter-rouge">autograd</code> 方法就可以得到所有张量的梯度。</p>

<h2 id="1-autograd自动求导系统">1. autograd：自动求导系统</h2>

<h4 id="torchautogradbackward"><code class="language-plaintext highlighter-rouge">torch.autograd.backward()</code></h4>

<p><strong>功能</strong>：自动求取计算图中各结点的梯度。</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">tensors</code>：用于求导的张量，如 <code class="language-plaintext highlighter-rouge">loss</code>。</li>
  <li><code class="language-plaintext highlighter-rouge">retain_graph</code>：保存计算图，PyTorch 默认在反向传播完成后丢弃计算图，如需保存则将该项设为 <code class="language-plaintext highlighter-rouge">True</code>。</li>
  <li><code class="language-plaintext highlighter-rouge">create_graph</code>：创建导数计算图，用于高阶求导。</li>
  <li><code class="language-plaintext highlighter-rouge">grad_tensors</code>：多梯度权重，当我们有多个 <code class="language-plaintext highlighter-rouge">loss</code> 需要计算梯度的时候，就需要设置各个 <code class="language-plaintext highlighter-rouge">loss</code> 的权重比例。</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span>
    <span class="n">tensors</span><span class="p">,</span>
    <span class="n">grad_tensors</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">retain_graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">create_graph</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>回顾一下如何通过计算图求解梯度：</p>

\[y=(x+w) * (w+1)\]

<ul>
  <li>$a = x+w$</li>
  <li>$b=w+1$</li>
  <li>$y=a*b$</li>
</ul>

\[\begin{align}
\dfrac{\partial y}{\partial w} &amp;= \dfrac{\partial y}{\partial a} \dfrac{\partial a}{\partial w} + \dfrac{\partial y}{\partial b} \dfrac{\partial b}{\partial w} \\[2ex]
&amp;= b * 1+ a * 1 \\[2ex]
&amp;= (w+1) + (x + w) \\[2ex]
&amp;= 2*w + x + 1 \\[2ex]
&amp;= 2*1 + 2 + 1 = 5
\end{align}\]

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-10-WX20201210-153230%402x.png" width="40%" /></p>

<p><strong>Python 代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># 如果希望后面再次执行该计算图，可以将 retain_graph 参数设为 True
# y.backward(retain_graph=True) 
</span>
<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>tensor([5.])
</pre></td></tr></tbody></table></code></pre></div></div>
<p>当有多个 <code class="language-plaintext highlighter-rouge">loss</code> 需要计算梯度时，通过 <code class="language-plaintext highlighter-rouge">grad_tensors</code> 设置各 <code class="language-plaintext highlighter-rouge">loss</code> 权重比例：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># y0 = (x+w) * (w+1)    dy0/dw = 2*w + x + 1 = 5
</span><span class="n">y0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># y1 = (x+w) + (w+1)    dy1/dw = 2
</span><span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  

<span class="c1"># 这种情况下，loss 是一个向量 [y0, y1]
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 梯度的权重：dy0/dw 权重为 1，dy1/dw 权重为 2
</span><span class="n">grad_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>

<span class="c1"># gradient 传入 torch.autograd.backward() 中的 grad_tensors
</span><span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradient</span><span class="o">=</span><span class="n">grad_tensors</span><span class="p">)</span>  

<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># 5*1 + 2*2 = 9
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>tensor([9.])
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="torchautogradgrad"><code class="language-plaintext highlighter-rouge">torch.autograd.grad()</code></h4>

<p><strong>功能</strong>：求取梯度。</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">outputs</code>：用于求导的张量，如 <code class="language-plaintext highlighter-rouge">loss</code>。</li>
  <li><code class="language-plaintext highlighter-rouge">inputs</code>：需要梯度的张量。</li>
  <li><code class="language-plaintext highlighter-rouge">create_graph</code>：创建导数计算图，用于高阶求导。</li>
  <li><code class="language-plaintext highlighter-rouge">retain_graph</code>：保存计算图。</li>
  <li><code class="language-plaintext highlighter-rouge">grad_outputs</code>：多梯度权重。</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span>
    <span class="n">outputs</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">grad_outputs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">retain_graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">create_graph</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>求取二阶梯度：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># y = x**2
</span>
<span class="c1"># grad_1 = dy/dx = 2x = 2 * 3 = 6
</span><span class="n">grad_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
<span class="k">print</span><span class="p">(</span><span class="n">grad_1</span><span class="p">)</span>

<span class="c1"># grad_2 = d(dy/dx)/dx = d(2x)/dx = 2
</span><span class="n">grad_2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">grad_1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>  
<span class="k">print</span><span class="p">(</span><span class="n">grad_2</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>(tensor([6.], grad_fn=&lt;MulBackward0&gt;),)
(tensor([2.]),)
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>注意事项</strong>：</p>

<ol>
  <li>梯度不自动清零。</li>
  <li>依赖于叶子结点的结点，<code class="language-plaintext highlighter-rouge">requires_grad</code> 默认为 <code class="language-plaintext highlighter-rouge">True</code>。</li>
  <li>叶子结点不可执行原位操作 (in-place)。</li>
</ol>

<p><strong>Python 示例 1</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="rouge-code"><pre><span class="c1"># 1. 梯度不会自动清零，重复求取会叠加，可以使用 .grad.zero_() 方法手动清零
</span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

<span class="c1"># 梯度清零，下划线表示原位操作 (in-place)
</span><span class="n">w</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">w</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>tensor([5.])
tensor([10.])
tensor([15.])
tensor([5.])
tensor([5.])
tensor([5.])
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>Python 示例 2</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># 2. 依赖于叶子结点的结点， requires_grad 默认为 True
</span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">b</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>True True True
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>Python 示例 3</strong>：</p>

<pre><code class="language-```python"># 3. 叶子结点不可执行 in-place (原位操作)。因为 PyTorch 计算图中引用叶子结点的值是
#    直接引用其前向传播时的地址，为了防止计算出错，叶子结点不可执行 in-place 操作。

#    in-place (原位操作): 从原始内存地址中直接改变数据。
#    非 in-place 操作: 开辟一块新的内存地址存储改变后的数据。

a = torch.ones((1, ))
print(id(a), a)

# 非 in-place 操作
a = a + torch.ones((1, ))
print(id(a), a)

# in-place 操作
a += torch.ones((1, ))
print(id(a), a)
</code></pre>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre>4875211904 tensor([1.])
4875212336 tensor([2.])
4875212336 tensor([3.])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>对叶子结点执行 in-place 操作将导致报错：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="rouge-code"><pre><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># 对非叶子结点 a 执行非 in-place 操作
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># 对非叶子结点 a 执行 in-place 操作
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># 对叶子结点 w 执行非 in-place 操作
</span><span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># 对叶子结点 w 执行 in-place 操作，会报错
</span><span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre>tensor([4.], grad_fn=&lt;AddBackward0&gt;)
tensor([4.], grad_fn=&lt;AddBackward0&gt;)
tensor([2.], grad_fn=&lt;AddBackward0&gt;)
Traceback (most recent call last):
  File "&lt;input&gt;", line 1, in &lt;module&gt;
  File "/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_umd.py", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File "/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/Users/andy/PycharmProjects/hello_pytorch/lesson/lesson-05/lesson-05-autograd.py", line 145, in &lt;module&gt;
    print(w.add_(1))
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="2-逻辑回归">2. 逻辑回归</h2>

<p><strong>逻辑回归 (Logistic Regression)</strong> 是 <strong>线性</strong> 的 <strong>二分类</strong> 模型。</p>

<p>模型表达式：</p>

\[y=f(WX+b)\]

\[f(x)=\dfrac{1}{1+e^{-x}}\]

<p>即：</p>

\[y=\dfrac{1}{1+e^{-(WX+b)}}\]

<p>这里，我们将 $f(x)$ 称为 Sigmoid 函数，又称 Logistic 函数：</p>

\[\text{class}=\begin{cases}0, &amp; y&lt; 0.5 \\[2ex] 1, &amp;  y \ge 0.5 \end{cases}\]

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-10-WX20201210-180559%402x.png" width="40%" /></p>

<p>线性回归是分析 <strong>自变量 $x$</strong> 与 <strong>因变量 $y$ (标量)</strong> 之间关系的方法；而逻辑回归是分析 <strong>自变量 $x$</strong> 与 <strong>因变量 $y$ (概率)</strong> 之间关系的方法。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-10-WX20201210-184101%402x.png" width="90%" /></p>

<p><strong>机器学习训练的 5 个步骤</strong>：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-11-WX20201211-145639%402x.png" width="90%" /></p>

<ol>
  <li><strong>数据</strong>：数据收集、清洗、划分、预处理。</li>
  <li><strong>模型</strong>：根据任务的难易程度，选择简单的线性模型或者复杂的神经网络模型等等。</li>
  <li><strong>损失函数</strong>：根据不同任务选择不同的损失函数并计算其梯度。例如：在线性回归中，我们可以选择均方误差损失函数；在分类任务中，我们可以选择交叉熵损失函数。</li>
  <li><strong>优化器</strong>：得到梯度之后，我们选择某种优化器来更新权值。</li>
  <li><strong>迭代训练</strong>：有了数据、模型、损失函数和优化器之后，我们就可以进行迭代训练了。</li>
</ol>

<p><strong>Python 代码示例</strong>：</p>

<center><video width="80%" controls="">
  <source src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-10-untitiled-1.mp4" type="video/mp4" />
</video></center>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># ============================== Step 1/5: 生成数据 ===================================
</span><span class="n">sample_nums</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">mean_value</span> <span class="o">=</span> <span class="mf">1.7</span>
<span class="n">bias</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sample_nums</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean_value</span> <span class="o">*</span> <span class="n">n_data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>    <span class="c1"># 类别0 数据 shape=(100, 2)
</span><span class="n">y0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">sample_nums</span><span class="p">)</span>                       <span class="c1"># 类别0 标签 shape=(100, 1)
</span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="n">mean_value</span> <span class="o">*</span> <span class="n">n_data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>   <span class="c1"># 类别1 数据 shape=(100, 2)
</span><span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sample_nums</span><span class="p">)</span>                        <span class="c1"># 类别1 标签 shape=(100, 1)
</span><span class="n">train_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>


<span class="c1"># ============================== Step 2/5: 选择模型 ===================================
</span><span class="k">class</span> <span class="nc">LR</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LR</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">features</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="n">lr_net</span> <span class="o">=</span> <span class="n">LR</span><span class="p">()</span>   <span class="c1"># 实例化逻辑回归模型
</span>
<span class="c1"># ============================== Step 3/5: 选择损失函数 ================================
</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCELoss</span><span class="p">()</span>  <span class="c1"># 二分类交叉熵损失 Binary Cross Entropy Loss
</span>
<span class="c1"># ============================== Step 4/5: 选择优化器 ==================================
</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>   <span class="c1"># 学习率
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr_net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>    <span class="c1"># 随机梯度下降
</span>
<span class="c1"># ============================== Step 5/5: 模型训练 ====================================
</span><span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>

    <span class="c1"># 前向传播
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr_net</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>

    <span class="c1"># 计算 loss
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">train_y</span><span class="p">)</span>

    <span class="c1"># 反向传播
</span>    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 更新参数
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># 绘图
</span>    <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">.</span><span class="n">ge</span><span class="p">(</span><span class="mf">0.5</span><span class="p">).</span><span class="nb">float</span><span class="p">().</span><span class="n">squeeze</span><span class="p">()</span>  <span class="c1"># 以 0.5 为阈值进行分类
</span>        <span class="n">correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="n">train_y</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>   <span class="c1"># 计算正确预测的样本个数
</span>        <span class="n">acc</span> <span class="o">=</span> <span class="n">correct</span><span class="p">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">train_y</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>   <span class="c1"># 计算分类准确率
</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x0</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x0</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'class 0'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x1</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'class 1'</span><span class="p">)</span>

        <span class="n">w0</span><span class="p">,</span> <span class="n">w1</span> <span class="o">=</span> <span class="n">lr_net</span><span class="p">.</span><span class="n">features</span><span class="p">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">w0</span><span class="p">,</span> <span class="n">w1</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">w0</span><span class="p">.</span><span class="n">item</span><span class="p">()),</span> <span class="nb">float</span><span class="p">(</span><span class="n">w1</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">plot_b</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">lr_net</span><span class="p">.</span><span class="n">features</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">item</span><span class="p">())</span>
        <span class="n">plot_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
        <span class="n">plot_y</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">w0</span> <span class="o">*</span> <span class="n">plot_x</span> <span class="o">-</span> <span class="n">plot_b</span><span class="p">)</span> <span class="o">/</span> <span class="n">w1</span>

        <span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plot_x</span><span class="p">,</span> <span class="n">plot_y</span><span class="p">)</span>

        <span class="n">plt</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s">'Loss=%.4f'</span> <span class="o">%</span> <span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span> <span class="s">'color'</span><span class="p">:</span> <span class="s">'red'</span><span class="p">})</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Iteration: {}</span><span class="se">\n</span><span class="s">w0:{:.2f} w1:{:.2f} b:{:.2f} accuracy:{:.2%}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">plot_b</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">acc</span> <span class="o">&gt;</span> <span class="mf">0.99</span><span class="p">:</span>
            <span class="k">break</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="3-总结">3. 总结</h2>

<p>本节课介绍了 PyTorch 自动求导系统中的 <code class="language-plaintext highlighter-rouge">torch.autograd.backward</code> 和 <code class="language-plaintext highlighter-rouge">torch.autograd.grad</code> 这两个常用方法，并演示了一阶、二阶导数的求导过程；理解了自动求导系统，以及数据载体 —— 张量，前向传播构建计算图，计算图求取梯度过程。有了这些知识之后，我们就可以开始正式训练机器学习模型。这里通过演示逻辑回归模型的训练，学习了机器学习回归模型的五大模块：数据、模型、损失函数、优化器和迭代训练过程。这五大模块将是后面学习的主线。</p>

<p>下节内容：数据读取机制：Dataloader 与 Dataset</p>
:ET