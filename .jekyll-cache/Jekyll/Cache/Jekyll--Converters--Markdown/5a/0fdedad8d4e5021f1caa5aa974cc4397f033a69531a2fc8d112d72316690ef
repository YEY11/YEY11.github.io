I"	<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-09-无模型强化学习q-learning-和-sarsa">Lecture 09 无模型强化学习：Q-Learning 和 SARSA</h1>

<p><strong>主要内容：</strong></p>
<ol>
  <li>动机</li>
  <li>强化学习</li>
  <li>Q-Learning</li>
  <li>SARSA</li>
  <li>总结</li>
</ol>

<h2 id="1-动机">1. 动机</h2>
<h3 id="11-学习成果">1.1 学习成果</h3>
<ol>
  <li>识别在哪些情况下，无模型强化学习适用于求解 MDP 问题。</li>
  <li>解释无模型规划与基于模型规划之间的差异。</li>
  <li>应用 Q-Learning 和 SARSA 手动解决小规模 MDP 问题，并编写 Q-Learning 和 SARSA 算法代码自动求解中等规模 MDP 问题。</li>
  <li>比较和对比非策略强化学习与策略强化学习。</li>
</ol>

<h3 id="12-规划与学习">1.2 规划与学习</h3>

<p>到目前为止，我们已经学习了盲目/启发式搜索和价值/策略迭代。</p>

<ul>
  <li>
    <p>搜索和价值/策略迭代都属于 <strong>基于模型</strong> 技术。这意味着我们需要知道模型；具体来说，我们知道 $P_a(s’\mid s)$ 和 $r(s,a,s’)$。</p>
  </li>
  <li>
    <p>Q-Learning 和 SARSA 则属于 <strong>无模型</strong> 技术。这意味着我们不知道 $P_a(s’\mid s)$ 和 $r(s,a,s’)$。</p>
  </li>
  <li>
    <p><strong>如果我们不知道转移和回报，我们该如何计算策略呢？</strong>我们通过尝试行动并观察结果，<strong>从经验中学习</strong>，从而使它成为一个机器学习问题。</p>
  </li>
  <li>
    <p>重要的是，在无模型强化学习中，我们不会尝试学习 $P_a(s’\mid s)$ 或 $r(s,a,s’)$ —— 我们将直接学习一个策略。</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>另外，有些技术则介于基于模型和无模型之间：基于模拟的技术。在这种情况下，我们将模型视为一个模拟器，因此我们可以模拟P a（s’</td>
          <td>s）和r（s，a，s’）并使用无模型技术学习策略。</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>下节内容：蒙特卡洛树搜索：利用和探索的权衡</p>

:ET