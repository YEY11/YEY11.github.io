I"<h1 id="lecture-17-优化器-一">Lecture 17 优化器 (一)</h1>

<p>前两节课中，我们学习了损失函数的概念以及 PyTorch 中的一系列损失函数方法，我们知道了损失函数的作用是衡量模型输出与真实标签之间的差异。在得到了 loss 函数之后，我们应该如何去更新模型参数，使得 loss 逐步降低呢？这正是优化器的工作。本节课我们开始学习优化器模块。</p>

<h2 id="1-什么是优化器">1. 什么是优化器</h2>

<p>在学习优化器模块之前，我们先回顾一下机器学习模型训练的 5 个步骤：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-11-WX20201211-145639%402x.png" width="90%" /></p>

<p>我们看到，优化器是第 4 个模块，那么它的作用是什么呢？我们知道，在前一步的损失函数模块中，我们会得到一个 loss 值，即模型输出与真实标签之间的差异。有了 loss 值之后，我们一般会采用 PyTorch 中的 AutoGrid 自动梯度求导模块对模型中参数的梯度进行求导计算，之后优化器会拿到这些梯度值并采用一些优化策略去更新模型参数，使得 loss 值下降。因此，优化器的作用就是利用梯度来更新模型中的可学习参数，使得模型输出与真实标签之间的差异更小，即让 loss 值下降。</p>

<p><strong>PyTorch 的优化器</strong>：<strong>管理</strong> 并 <strong>更新</strong> 模型中可学习参数 (权值或偏置) 的值，使得模型输出更接近真实标签。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-27-WX20201227-123649%402x.png" width="80%" /></p>

<ul>
  <li><strong>导数</strong>：函数在指定坐标轴上的变化率。</li>
  <li><strong>方向导数</strong>：指定方向上的变化率。</li>
  <li><strong>梯度</strong>：一个向量，方向为方向导数取得最大值的方向。</li>
</ul>

<h2 id="2-optimizer-的属性">2. Optimizer 的属性</h2>

<p><strong>PyTorch 中的 Optimizer 类</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">defaults</span> <span class="o">=</span> <span class="n">defaults</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="n">param_groups</span> <span class="o">=</span> <span class="p">[{</span><span class="s">'params'</span><span class="p">:</span> <span class="n">param_groups</span><span class="p">}]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>基本属性</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">defaults</code>：优化器超参数。</li>
  <li><code class="language-plaintext highlighter-rouge">state</code>：参数的缓存，如 <code class="language-plaintext highlighter-rouge">momentum</code> 的缓存。</li>
  <li><code class="language-plaintext highlighter-rouge">params_groups</code>：管理的参数组。</li>
  <li><code class="language-plaintext highlighter-rouge">_ step_count</code>：记录更新次数，学习率调整中使用。</li>
</ul>

<h2 id="3-optimizer-的方法">3. Optimizer 的方法</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s">'params'</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">detach_</span><span class="p">()</span>
                    <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">add_param_group</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_group</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>

<span class="n">param_set</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s">'params’]))

self.param_groups.append(param_group)
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>基本方法</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">zero_grad()</code>：清空所管理参数的梯度 (PyTorch 特性：张量梯度不自动清零)。</li>
  <li><code class="language-plaintext highlighter-rouge">step()</code>：执行一步更新。</li>
  <li><code class="language-plaintext highlighter-rouge">add_param_group()</code>：添加参数组。</li>
</ul>

<p>下节内容：优化器 (一)</p>
:ET