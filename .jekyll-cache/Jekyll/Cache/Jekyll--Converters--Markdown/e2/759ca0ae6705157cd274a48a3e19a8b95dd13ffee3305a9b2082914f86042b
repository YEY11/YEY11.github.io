I"<h1 id="lecture-06-线性模型选择与正则化">Lecture 06 线性模型选择与正则化</h1>

<p><strong>参考教材</strong>：</p>

<ul>
  <li><em>Gareth, J., Daniela, W., Trevor, H., &amp; Robert, T. (2013). An intruduction to statistical learning: with applications in R. Spinger.</em></li>
  <li><em>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Spinger Science &amp; Business Media.</em></li>
</ul>

<h2 id="1-引言">1. 引言</h2>

<p>回忆一下标准的线性回归模型：</p>

<script type="math/tex; mode=display">Y=\beta_0 + \beta_1 X_1 +\cdots + \beta_p X_p + \epsilon</script>

<p>它通常用于描述响应变量 $Y$ 和一系列预测变量 $X_1,X_2,\dots,X_p$ 之间的线性关系。并且我们在之前课程中介绍了一种用于拟合此模型的经典方法：最小二乘法。</p>

<p>在接下来的课程中，我们将考虑一些扩展线性模型框架的方法。在第 7 章的课程中，我们对线性模型进行了推广，以使其适用于关系为 <strong>非线性 (non-linear)</strong>，但形式仍然 <strong>可加 (additive)</strong> 的情况。在第 8 章的课程中，我们将考虑一些更通用的 <strong>非线性模型</strong>。</p>

<p>尽管线性模型非常简单，但其在可解释性方面具有明显优势，并且通常显示出良好的预测性能。因此，在正式介绍非线性模型之前，我们将先介绍几种可替代普通最小二乘拟合的一些其他拟合方法，这些方法是对简单线性模型的改进。</p>

<p>那么，为什么要采用其他拟合方法替代最小二乘法呢?</p>

<p>因为与最小二乘法相比，其他拟合方法具有更高的 <strong>预测准确率 (prediction accuracy)</strong> 和更好的 <strong>模型可解释性 (model interpretability)</strong>。</p>

<ul>
  <li>
    <p><strong>预测准确率</strong>：若响应变量和预测变量的真实关系近似线性，则最小二乘估计的偏差较低。若 $n\gg p$，即观测个数 $n$ 远大于变量个数 $p$，则到最小二乘估计的方差通常较低，从而在测试样本集上有较好表现。然而，在不满足 $n$ 远大于 $p$ 的情况下，最小二乘拟合可能会发生较大变化，发生过拟合，从而使模型在测试样本集上表现较差。此外，若 $p &gt; n$，最小二乘法得到的系数估计结果不唯一：此时方差为 <strong>无穷大 (infinite)</strong>，这种情况下最小二乘法会失效。通过 <strong>限制 (constraining)</strong> 或 <strong>收缩 (shrinking)</strong> 待估计系数，以牺牲偏差为代价，显著减小估计量方差。这种方法可以显著提高模型在测试样本集上的预测准确率。</p>
  </li>
  <li>
    <p><strong>模型可解释性</strong>：通过删除不相关的特征，即通过将相应的系数估计设置为零，我们可以获得更易于解释的模型。 我们将介绍一些自动执行特征选择的方法</p>
  </li>
</ul>

<p>在多元回归模型中，常常存在一个或多个预测变量与响应变量不存在线性关系的情况，包括一些增加了模型的复杂性、却与模型 <strong>无关 (irrelevant)</strong> 的变量。通过移除这些无关变量，即将其系数设置为 $0$，可以使模型的可解释性更强，但运用最小二乘法很难将系数缩减至 $0$。本节课中，我们将介绍几种自动进行 <strong>特征选择 (feature selection)</strong> 或 <strong>变量选择 (variable selection)</strong> 的方法，以便将无关变量从多元回归模型中剔除。</p>

<p>除了最小二乘法，还有多种方法可用于拟合线性模型，其中既有经典方法又有现代方法。本节课中我们主要讨论以下三类重要的方法：</p>

<ul>
  <li>
    <p><strong>子集选择 (Subset selection)</strong>：该方法从 $p$ 个预测变量中挑选出与响应变量相关的变量，构建一个变量子集，再对这个缩减后的变量集合应用最小二乘法拟合模型。</p>
  </li>
  <li>
    <p><strong>收缩 (Shrinkage)</strong>：该方法基于全部 $p$ 个变量进行模型拟合。但是，与最小二乘估计相比，该方法的估计系数可以缩减为 $0$。这种收缩，也称 <strong>正则化 (regularization)</strong>，具有减少方差的效果。根据收缩的类型，某些系数估计可以正好为 $0$。因此，收缩法也可以用于变量选择。</p>
  </li>
  <li>
    <p><strong>降维 (Dimension reduction)</strong>：此方法将 $p$ 个预测变量投影到一个 $M$-维子空间中，其中 $M&lt; p$。这是通过计算原始变量的 $M$ 个不同的 <strong>线性组合 (linear combination)</strong> 或者 <strong>投影 (projections)</strong> 实现的。然后，我们将这 $M$ 个投影作为新的预测变量，用最小二乘法拟合线性回归模型。</p>
  </li>
</ul>

<p>本节课中，我们将详细介绍上述方法及其you</p>

<p>这通常通过计算这 p 个 变簇的 M 种不阔的线性组合( lìne缸 combination) 或称投影 (p町ection) 来实现。将这 M 个不同的投影作为预测变f景，再使用最小二乘法拟告线性回归模裂。 2在直在将详细介绍上述方法，并介绍这些方法各自的优缺点。虽然本意讨论的是第 3 章中线 性阻妇模挫方法的拓展和改进，但其中涉及的概念饲样适用于其他方法，例如第 4 章中介绍的</p>

<p>分类模型。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-08-WX20201108-211220%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 13</span>： 预验证的流程。</span></p>

<p>下节内容：线性模型选择与正则化</p>
:ET