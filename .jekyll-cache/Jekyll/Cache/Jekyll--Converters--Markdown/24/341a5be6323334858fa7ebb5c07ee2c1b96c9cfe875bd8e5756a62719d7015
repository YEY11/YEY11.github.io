I"уш<h1 id="lecture-15-жЌџе¤±е‡Ѕж•°-дёЂ">Lecture 15 жЌџе¤±е‡Ѕж•° (дёЂ)</h1>

<p>ењЁе‰Ќе‡ иЉ‚иЇѕдё­пјЊж€‘д»¬е­¦д№ дє†жЁЎећ‹жЁЎеќ—дё­зљ„дёЂдє›зџҐиЇ†пјЊеЊ…ж‹¬е¦‚дЅ•жћ„е»єжЁЎећ‹д»ҐеЏЉжЂЋж ·иї›иЎЊжЁЎећ‹е€ќе§‹еЊ–гЂ‚жњ¬иЉ‚иЇѕж€‘д»¬е°†ејЂе§‹е­¦д№ жЌџе¤±е‡Ѕж•°жЁЎеќ—гЂ‚</p>

<h2 id="1-жЌџе¤±е‡Ѕж•°зљ„ж¦‚еїµ">1. жЌџе¤±е‡Ѕж•°зљ„ж¦‚еїµ</h2>

<p><strong>жЌџе¤±е‡Ѕж•° (Loss Function)</strong>пјљиЎЎй‡ЏжЁЎећ‹иѕ“е‡єдёЋзњџе®ћж ‡з­ѕд№‹й—ґзљ„е·®еј‚гЂ‚</p>

<p>дё‹йќўжЇдёЂдёЄдёЂе…ѓзєїжЂ§е›ћеЅ’зљ„ж‹џеђ€иї‡зЁ‹з¤єж„Џе›ѕпјљ</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-20-WX20201220-200705%402x.png" width="60%" /></p>

<p>е›ѕдё­зљ„з»їи‰Іж–№еќ—д»ЈиЎЁи®­з»ѓж ·жњ¬з‚№ $(x_i, y_i)$пјЊи“ќи‰Із›ґзєїд»ЈиЎЁи®­з»ѓеѕ—е€°зљ„жЁЎећ‹ $\hat y = w_0 + w_1 x$пјЊе…¶дё­пјЊ$w_0$ д»ЈиЎЁж€Єи·ќпјЊ$w_1 = \Delta y / \Delta x$ д»ЈиЎЁж–њзЋ‡гЂ‚еЏЇд»Ґзњ‹е€°пјЊжЁЎећ‹е№¶жІЎжњ‰е®ЊзѕЋењ°ж‹џеђ€жЇЏдёЂдёЄж•°жЌ®з‚№пјЊж‰Ђд»Ґж•°жЌ®з‚№е’ЊжЁЎећ‹д№‹й—ґе­ењЁдёЂдёЄ <strong>жЌџе¤± (Loss)</strong>пјЊиї™й‡Њж€‘д»¬й‡‡з”Ёећ‚з›ґж–№еђ‘дёЉжЁЎећ‹иѕ“е‡єдёЋзњџе®ћж•°жЌ®з‚№д№‹е·®зљ„з»ќеЇ№еЂј $|\hat y -y|$ дЅњдёєжЌџе¤±е‡Ѕж•°зљ„еє¦й‡ЏгЂ‚</p>

<p>еЏ¦е¤–пјЊеЅ“ж€‘д»¬и°€е€°жЌџе¤±е‡Ѕж•°ж—¶пјЊз»Џеёёдјљж¶‰еЏЉе€°д»Ґдё‹дё‰дёЄж¦‚еїµпјљ</p>

<ul>
  <li>
    <p><strong>жЌџе¤±е‡Ѕж•° (Loss Function)</strong>пјљи®Ўз®—еЌ•дёЄж ·жњ¬зљ„е·®еј‚гЂ‚</p>

\[\mathrm{Loss} = f(\hat y, y)\]
  </li>
  <li>
    <p><strong>д»Јд»·е‡Ѕж•° (Cost Function)</strong>пјљи®Ўз®—ж•ґдёЄи®­з»ѓй›† $\mathrm{Loss}$ зљ„е№іеќ‡еЂјгЂ‚</p>

\[\mathrm{Cost} = \dfrac{1}{n}\sum_{i=1}^{n} f(\hat y_i, y_i)\]
  </li>
  <li>
    <p><strong>з›®ж ‡е‡Ѕж•° (Objective Function)</strong>пјљжњЂз»€йњЂи¦ЃдјеЊ–зљ„з›®ж ‡пјЊйЂљеёёеЊ…еђ«д»Јд»·е‡Ѕж•°е’Њж­Је€™йЎ№гЂ‚</p>

\[\mathrm{Obj} = \mathrm{Cost} + \mathrm{Regularization}\]
  </li>
</ul>

<p>жіЁж„ЏпјЊд»Јд»·е‡Ѕж•°е№¶дёЌжЇи¶Ље°Џи¶ЉеҐЅпјЊе› дёєе­ењЁиї‡ж‹џеђ€зљ„йЈЋй™©гЂ‚ж‰Ђд»Ґж€‘д»¬йњЂи¦ЃеЉ дёЉдёЂдє›зє¦жќџ (еЌіж­Је€™йЎ№) жќҐйІж­ўжЁЎећ‹еЏеѕ—иї‡дєЋе¤Ќжќ‚иЂЊеЇји‡ґиї‡ж‹џеђ€пјЊеёёз”Ёзљ„жњ‰ L1 е’Њ L2 ж­Је€™йЎ№гЂ‚е› ж­¤пјЊд»Јд»·е‡Ѕж•°е’Њж­Је€™йЎ№жњЂз»€жћ„ж€ђдє†ж€‘д»¬зљ„з›®ж ‡е‡Ѕж•°гЂ‚</p>

<p>дё‹йќўж€‘д»¬жќҐзњ‹дёЂдё‹ PyTorch дё­зљ„ <code class="language-plaintext highlighter-rouge">_Loss</code> з±»пјљ</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">_Loss</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">or</span> <span class="nb">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="p">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="nb">reduce</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>еЏЇд»Ґзњ‹е€°пјЊ<code class="language-plaintext highlighter-rouge">_Loss</code> жЇз»§ж‰їдєЋ <code class="language-plaintext highlighter-rouge">Module</code> з±»зљ„пјЊж‰Ђд»Ґд»Ћжџђз§ЌзЁ‹еє¦дёЉж€‘д»¬еЏЇд»Ґе°† <code class="language-plaintext highlighter-rouge">_Loss</code> д№џи§†дёєдёЂдёЄзЅ‘з»ње±‚гЂ‚е®ѓзљ„е€ќе§‹еЊ–е‡Ѕж•°дё­дё»и¦Ѓжњ‰ 3 дёЄеЏ‚ж•°пјЊе…¶дё­ <code class="language-plaintext highlighter-rouge">size_average</code> е’Њ <code class="language-plaintext highlighter-rouge">reduce</code> иї™дё¤дёЄеЏ‚ж•°еЌіе°†ењЁеђЋз»­з‰€жњ¬дё­иў«и€ЌејѓпјЊе› дёє <code class="language-plaintext highlighter-rouge">reduction</code> еЏ‚ж•°е·Із»ЏеЏЇд»Ґе®ћзЋ°е‰Ќдё¤иЂ…зљ„еЉџиѓЅгЂ‚</p>

<h2 id="2-дє¤еЏ‰з†µжЌџе¤±е‡Ѕж•°">2. дє¤еЏ‰з†µжЌџе¤±е‡Ѕж•°</h2>

<p>ењЁе€†з±»д»»еЉЎдё­пјЊж€‘д»¬з»Џеёёй‡‡з”Ёзљ„жЇдє¤еЏ‰з†µжЌџе¤±е‡Ѕж•°гЂ‚ењЁе€†з±»д»»еЉЎдё­ж€‘д»¬еёёеёёйњЂи¦Ѓи®Ўз®—дёЌеђЊз±»е€«зљ„ж¦‚зЋ‡еЂјпјЊж‰Ђд»Ґдє¤еЏ‰з†µеЏЇд»Ґз”ЁжќҐиЎЎй‡Џдё¤дёЄж¦‚зЋ‡е€†еёѓд№‹й—ґзљ„е·®еј‚пјЊдє¤еЏ‰з†µеЂји¶ЉдЅЋиЇґжЋдё¤дёЄж¦‚зЋ‡е€†еёѓи¶ЉжЋҐиї‘гЂ‚</p>

<p>й‚Јд№€дёєд»Ђд№€дє¤еЏ‰з†µеЂји¶ЉдЅЋпјЊдё¤дёЄж¦‚зЋ‡е€†еёѓи¶ЉжЋҐиї‘е‘ўпјџиї™йњЂи¦Ѓд»Ће®ѓдёЋдїЎжЃЇз†µе’Њз›ёеЇ№з†µд№‹й—ґзљ„е…ізі»иЇґиµ·пјљ</p>

<p><span><center>дє¤еЏ‰з†µ $=$ дїЎжЃЇз†µ $+$ з›ёеЇ№з†µ</center></span></p>

<p>ж€‘д»¬е…€жќҐзњ‹жњЂеџєжњ¬зљ„ <strong>з†µ (Entropy)</strong> зљ„ж¦‚еїµпјљз†µе‡†зЎ®жќҐиЇґеє”иЇҐеЏ«еЃљ <strong>дїЎжЃЇз†µ (Information Entropy)</strong>пјЊе®ѓжЇз”±дїЎжЃЇи®єд№‹з€¶й¦™е†њд»Ћзѓ­еЉ›е­¦дё­еЂџй‰ґиї‡жќҐзљ„дёЂдёЄж¦‚еїµпјЊз”ЁдєЋжЏЏиї°жџђдёЄдє‹д»¶зљ„дёЌзЎ®е®љжЂ§пјљжџђдёЄдє‹д»¶дёЌзЎ®е®љжЂ§и¶Љй«пјЊе®ѓзљ„з†µе°±и¶Ље¤§гЂ‚дѕ‹е¦‚пјљвЂњжЋе¤©дё‹й›ЁвЂќ иї™дёЂдє‹д»¶и¦ЃжЇ” вЂњжЋе¤©е¤ЄйідјљеЌ‡иµ·вЂќ иї™дёЂдє‹д»¶зљ„з†µе¤§еѕ—е¤љпјЊе› дёєе‰ЌиЂ…зљ„дёЌзЎ®е®љжЂ§иѕѓй«гЂ‚иї™й‡Њж€‘д»¬йњЂи¦Ѓеј•е…Ґ <strong>и‡ЄдїЎжЃЇ</strong> зљ„ж¦‚еїµгЂ‚</p>

<ul>
  <li>
    <p><strong>и‡ЄдїЎжЃЇ (Self-information)</strong>пјљз”ЁдєЋиЎЎй‡ЏеЌ•дёЄдє‹д»¶зљ„дёЌзЎ®е®љжЂ§гЂ‚</p>

\[I(X) = -\log [P(X)]\]

    <p>е…¶дё­пјЊ$P(X)$ дёєдє‹д»¶ $X$ зљ„ж¦‚зЋ‡гЂ‚</p>
  </li>
  <li>
    <p><strong>з†µ (Entropy)</strong>пјљи‡ЄдїЎжЃЇзљ„жњџжњ›пјЊз”ЁдєЋжЏЏиї°ж•ґдёЄж¦‚зЋ‡е€†еёѓзљ„дёЌзЎ®е®љжЂ§гЂ‚дє‹д»¶зљ„дёЌзЎ®е®љжЂ§и¶Љй«пјЊе®ѓзљ„з†µе°±и¶Ље¤§гЂ‚</p>

\[H(P) = \mathrm{E}_{X\sim P}[I(X)] = \sum_{i=1}^{n}P(x_i)\log P(x_i)\]
  </li>
</ul>

<p>дёєдє†ж›ґеҐЅењ°зђ†и§Јз†µдёЋдє‹д»¶дёЌзЎ®е®љжЂ§зљ„е…ізі»пјЊж€‘д»¬жќҐзњ‹дёЂдёЄз¤єж„Џе›ѕпјљ</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-20-entropy.png" width="60%" /></p>

<p>дёЉйќўжЇдјЇеЉЄе€©е€†еёѓ (дё¤з‚№е€†еёѓ) зљ„дїЎжЃЇз†µпјЊеЏЇд»Ґзњ‹е€°пјЊеЅ“дє‹д»¶ж¦‚зЋ‡дёє $0.5$ ж—¶пјЊе®ѓзљ„дїЎжЃЇз†µжњЂе¤§пјЊе¤§зє¦ењЁ $0.69$ й™„иї‘пјЊеЌіж­¤ж—¶иЇҐдє‹д»¶зљ„дёЌзЎ®е®љжЂ§жЇжњЂе¤§зљ„гЂ‚жіЁж„ЏпјЊиї™й‡Њзљ„ $0.69$ жЇењЁдєЊе€†з±»жЁЎећ‹и®­з»ѓиї‡зЁ‹дё­з»Џеёёдјљзў°е€°зљ„дёЂдёЄ Loss еЂјпјљжњ‰ж—¶ењЁжЁЎећ‹и®­з»ѓе‡єй—®йўж—¶пјЊж— и®єж€‘д»¬е¦‚дЅ•иї›иЎЊиї­д»ЈпјЊжЁЎећ‹зљ„ Loss еЂје§‹з»€жЃ’е®љењЁ $0.69$пј›ж€–иЂ…ењЁжЁЎећ‹е€ље€ќе§‹еЊ–е®Њж€ђз¬¬дёЂж¬Ўиї­д»ЈеђЋпјЊе…¶ Loss еЂјд№џеѕ€еЏЇиѓЅжЇ $0.69$пјЊиї™иЎЁжЋж€‘д»¬зљ„жЁЎећ‹еЅ“е‰ЌжЇдёЌе…·е¤‡д»»дЅ•е€¤е€«иѓЅеЉ›зљ„пјЊе› дёєе…¶еЇ№дєЋдё¤дёЄз±»е€«дё­зљ„д»»дЅ•дёЂдёЄйѓЅи®¤дёєж¦‚зЋ‡жЇ $0.5$гЂ‚</p>

<p>дё‹йќўж€‘д»¬жќҐзњ‹дёЂдё‹з›ёеЇ№з†µзљ„ж¦‚еїµпјљ</p>

<ul>
  <li>
    <p><strong>з›ёеЇ№з†µ (Relative Entropy)</strong>пјљеЏ€з§° <strong>KL ж•Јеє¦ (Kullback-Leibler Divergence, KLD)</strong>пјЊз”ЁдєЋиЎЎй‡Џдё¤дёЄж¦‚зЋ‡е€†еёѓд№‹й—ґзљ„е·®еј‚ (ж€–иЂ…иЇґи·ќз¦»)гЂ‚жіЁж„ЏпјЊи™Ѕз„¶ KL ж•Јеє¦еЏЇд»ҐиЎЎй‡Џдё¤дёЄе€†еёѓд№‹й—ґзљ„и·ќз¦»пјЊдЅ†е®ѓжњ¬иє«е№¶дёЌжЇдёЂдёЄи·ќз¦»е‡Ѕж•°пјЊе› дёєи·ќз¦»е‡Ѕж•°е…·жњ‰еЇ№з§°жЂ§пјЊеЌі $P$ е€° $Q$ зљ„и·ќз¦»еї…йЎ»з­‰дєЋ $Q$ е€° $P$ зљ„и·ќз¦»пјЊиЂЊз›ёеЇ№з†µдёЌе…·е¤‡иї™з§ЌеЇ№з§°жЂ§гЂ‚</p>

\[D_{\mathrm{KL}}(P, Q) = \mathrm{E}_{X \sim P}\left[\log \dfrac{P(X)}{Q(X)}\right]\]

    <p>е…¶дё­пјЊ$P$ жЇж•°жЌ®зљ„зњџе®ће€†еёѓпјЊ$Q$ жЇжЁЎећ‹ж‹џеђ€зљ„е€†еёѓпјЊдєЊиЂ…е®љд№‰ењЁз›ёеђЊзљ„ж¦‚зЋ‡з©єй—ґдёЉгЂ‚ж€‘д»¬йњЂи¦Ѓз”Ёж‹џеђ€е€†еёѓ $Q$ еЋ»йЂјиї‘зњџе®ће€†еёѓ $P$пјЊж‰Ђд»Ґз›ёеЇ№з†µдёЌе…·е¤‡еЇ№з§°жЂ§гЂ‚</p>
  </li>
</ul>

<p>дё‹йќўж€‘д»¬е†ЌжќҐзњ‹дёЂдё‹дє¤еЏ‰з†µзљ„е…¬ејЏпјљ</p>

<ul>
  <li>
    <p><strong>дє¤еЏ‰з†µ (Cross Entropy)</strong>пјљз”ЁдєЋиЎЎй‡Џдё¤дёЄе€†еёѓд№‹й—ґзљ„з›ёдјјеє¦гЂ‚</p>

\[H(P,Q)= -\sum_{i=1}^{n}P(x_i)\log Q(x_i)\]
  </li>
</ul>

<p>дё‹йќўж€‘д»¬еЇ№з›ёеЇ№з†µзљ„е…¬ејЏиї›иЎЊе±•ејЂжЋЁеЇјеЏжЌўпјЊжќҐи§‚еЇџдёЂдё‹з›ёеЇ№з†µдёЋдїЎжЃЇз†µе’Њдє¤еЏ‰з†µд№‹й—ґзљ„е…ізі»пјљ</p>

\[\begin{aligned}
D_{\mathrm{KL}}(P, Q) &amp;= \mathrm{E}_{X \sim P}\left[\log \dfrac{P(X)}{Q(X)}\right] \\[2ex]
&amp;= \mathrm{E}_{X \sim P} [\log P(X) - \log Q(X) ] \\[2ex]
&amp;= \sum_{i=1}^{n} P(x_i) [\log P(x_i) - \log Q(x_i) ] \\[2ex]
&amp;= \sum_{i=1}^{n} P(x_i) \log P(x_i) - \sum_{i=1}^{n} P(x_i) \log Q(x_i) \\[2ex]
&amp;= H(P, Q) - H(P)
\end{aligned}\]

<p>ж‰Ђд»ҐпјЊ<strong>дє¤еЏ‰з†µз­‰дєЋдїЎжЃЇз†µеЉ дёЉз›ёеЇ№з†µ</strong>пјљ</p>

\[H(P, Q) = H(P) + D_{\mathrm{KL}}(P, Q)\]

<p>иї™й‡ЊпјЊ$P$ дёєи®­з»ѓй›†дё­зљ„ж ·жњ¬е€†еёѓпјЊ$Q$ дёєжЁЎећ‹з»™е‡єзљ„е€†еёѓгЂ‚ж‰Ђд»ҐењЁжњєе™Ёе­¦д№ дё­пјЊж€‘д»¬жњЂе°ЏеЊ–дє¤еЏ‰з†µе®ћй™…дёЉз­‰д»·дєЋжњЂе°ЏеЊ–з›ёеЇ№з†µпјЊе› дёєи®­з»ѓй›†жЇе›єе®љзљ„пјЊж‰Ђд»Ґ $H(P)$ ењЁиї™й‡ЊжЇдёЂдёЄеёёж•°гЂ‚</p>

<h4 id="nncrossentropyloss"><code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss</code></h4>

<p><strong>еЉџиѓЅ</strong>пјљ<code class="language-plaintext highlighter-rouge">nn.LogSoftmax()</code> дёЋ <code class="language-plaintext highlighter-rouge">nn.NLLLoss()</code> з»“еђ€пјЊиї›иЎЊдє¤еЏ‰з†µи®Ўз®—гЂ‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span>
    <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>дё»и¦ЃеЏ‚ж•°</strong>пјљ</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">weight</code>пјљеђ„з±»е€«зљ„ loss и®ѕзЅ®жќѓеЂјгЂ‚</li>
  <li><code class="language-plaintext highlighter-rouge">ignore_index</code>пјљеїЅз•ҐжџђдёЄз±»е€«пјЊдёЌи®Ўз®—е…¶ lossгЂ‚</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>пјљи®Ўз®—жЁЎејЏпјЊеЏЇдёє <code class="language-plaintext highlighter-rouge">none/sum/mean</code>гЂ‚
    <ul>
      <li><code class="language-plaintext highlighter-rouge">none</code>пјљйЂђдёЄе…ѓзґ и®Ўз®—гЂ‚</li>
      <li><code class="language-plaintext highlighter-rouge">sum</code>пјљж‰Ђжњ‰е…ѓзґ ж±‚е’ЊпјЊиї”е›ћж ‡й‡ЏгЂ‚</li>
      <li><code class="language-plaintext highlighter-rouge">mean</code>пјљеЉ жќѓе№іеќ‡пјЊиї”е›ћж ‡й‡ЏгЂ‚</li>
    </ul>
  </li>
</ul>

<p><strong>PyTorch дё­ <code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss</code> зљ„дє¤еЏ‰з†µи®Ўз®—е…¬ејЏ</strong>пјљ</p>

<ul>
  <li>
    <p>жІЎжњ‰й’€еЇ№еђ„з±»е€« loss и®ѕзЅ®жќѓеЂјзљ„жѓ…е†µпјљ</p>

\[\mathrm{loss}(x, class) = -\log \left(\dfrac{\exp(x[class])}{\sum_j \exp(x[j])} \right) = -x[class] + \log \left(\sum_j \exp(x[j])\right)\]
  </li>
  <li>
    <p>еЇ№еђ„з±»е€« loss и®ѕзЅ®жќѓеЂјзљ„жѓ…е†µпјљ</p>

\[\mathrm{loss}(x, class) = \mathrm{weight}[class] \left(-x[class] + \log \left(\sum_j \exp(x[j])\right)\right)\]
  </li>
</ul>

<p>жіЁж„ЏпјЊиї™й‡Њзљ„и®Ўз®—иї‡зЁ‹е’Њдє¤еЏ‰з†µе…¬ејЏе­ењЁдёЂдє›е·®еј‚пјљ</p>

\[H(P,Q)= -\sum_{i=1}^{n}P(x_i)\log Q(x_i)\]

<p>е› дёєиї™й‡Њж€‘д»¬е·Із»Џе°†дёЂдёЄе…·дЅ“ж•°жЌ®з‚№еЏ–е‡єпјЊж‰Ђд»Ґиї™й‡Њ $\Sigma$ ж±‚е’ЊејЏдёЌе†ЌйњЂи¦ЃпјЊе№¶дё” $P(x_i)=1$пјЊе› ж­¤е…¬ејЏеЏдёєпјљ</p>

\[H(P,Q)= -\log Q(x_i)\]

<p>з„¶еђЋпјЊдёєдє†дЅїиѕ“е‡єж¦‚зЋ‡ењЁ $[0,1]$ д№‹й—ґпјЊPyTorch ењЁиї™й‡ЊдЅїз”Ёдє†дёЂдёЄ Softmax е‡Ѕж•°еЇ№ж•°жЌ®иї›иЎЊдє†еЅ’дёЂеЊ–е¤„зђ†пјЊдЅїе…¶иђЅењЁдёЂдёЄж­Јеёёзљ„ж¦‚зЋ‡еЂјиЊѓе›ґе†…гЂ‚</p>

<p><strong>д»Јз Ѓз¤єдѕ‹</strong>пјљ</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># fake data
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>  <span class="c1"># жіЁж„Џ label ењЁиї™й‡Њеї…йЎ»и®ѕзЅ®дёєй•їж•ґећ‹
</span>
<span class="c1"># ------------------------ CrossEntropy loss: reduction ----------------------
# def loss function
</span><span class="n">loss_f_none</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss_f_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)</span>
<span class="n">loss_f_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)</span>

<span class="c1"># forward
</span><span class="n">loss_none</span> <span class="o">=</span> <span class="n">loss_f_none</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss_sum</span> <span class="o">=</span> <span class="n">loss_f_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss_mean</span> <span class="o">=</span> <span class="n">loss_f_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># view
</span><span class="k">print</span><span class="p">(</span><span class="s">"Cross Entropy Loss:</span><span class="se">\n</span><span class="s"> "</span><span class="p">,</span> <span class="n">loss_none</span><span class="p">,</span> <span class="n">loss_sum</span><span class="p">,</span> <span class="n">loss_mean</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>иѕ“е‡єз»“жћњпјљ</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>Cross Entropy Loss:
  tensor([1.3133, 0.1269, 0.1269]) tensor(1.5671) tensor(0.5224)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>еЏЇд»Ґзњ‹е€°пјЊ<code class="language-plaintext highlighter-rouge">reduction</code> еЏ‚ж•°йЎ№ењЁ <code class="language-plaintext highlighter-rouge">none</code> жЁЎејЏдё‹пјЊи®Ўз®—е‡єзљ„ 3 дёЄж ·жњ¬зљ„ loss еЂје€†е€«дёє $1.3133$гЂЃ$0.1269$ е’Њ $0.1269$пј›ењЁ <code class="language-plaintext highlighter-rouge">sum</code> жЁЎејЏдё‹пјЊи®Ўз®—е‡є 3 дёЄж ·жњ¬зљ„ loss д№‹е’Њдёє $1.5671$пј›ењЁ <code class="language-plaintext highlighter-rouge">mean</code> жЁЎејЏдё‹пјЊи®Ўз®—е‡є 3 дёЄж ·жњ¬зљ„ loss е№іеќ‡дёє $0.5224$гЂ‚</p>

<p>дё‹йќўж€‘д»¬д»Ґз¬¬дёЂдёЄж ·жњ¬зљ„ loss еЂјдёєдѕ‹пјЊйЂљиї‡ж‰‹еЉЁи®Ўз®—жќҐйЄЊиЇЃдёЂдё‹ж€‘д»¬е‰ЌйќўжЋЁеЇје‡єзљ„е…¬ејЏзљ„ж­ЈзЎ®жЂ§пјљ</p>

\[\mathrm{loss}(x, class) = -x[class] + \log \left(\sum_j \exp(x[j])\right)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">input_1</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">()[</span><span class="n">idx</span><span class="p">]</span>      <span class="c1"># [1, 2]
</span><span class="n">target_1</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">idx</span><span class="p">]</span>              <span class="c1"># [0]
</span>
<span class="c1"># з¬¬дёЂйЎ№
</span><span class="n">x_class</span> <span class="o">=</span> <span class="n">input_1</span><span class="p">[</span><span class="n">target_1</span><span class="p">]</span>

<span class="c1"># з¬¬дєЊйЎ№
</span><span class="n">sigma_exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">,</span> <span class="n">input_1</span><span class="p">)))</span>
<span class="n">log_sigma_exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma_exp_x</span><span class="p">)</span>

<span class="c1"># иѕ“е‡єloss
</span><span class="n">loss_1</span> <span class="o">=</span> <span class="o">-</span><span class="n">x_class</span> <span class="o">+</span> <span class="n">log_sigma_exp_x</span>

<span class="k">print</span><span class="p">(</span><span class="s">"з¬¬дёЂдёЄж ·жњ¬зљ„ loss дёє: "</span><span class="p">,</span> <span class="n">loss_1</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>иѕ“е‡єз»“жћњпјљ</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>з¬¬дёЂдёЄж ·жњ¬зљ„ loss дёє:  1.3132617
</pre></td></tr></tbody></table></code></pre></div></div>

<p>дё‹йќўж€‘д»¬жќҐзњ‹дёЂдё‹й’€еЇ№еђ„з±»е€« loss и®ѕзЅ®жќѓеЂјзљ„жѓ…е†µпјљ</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="c1"># def loss function
# еђ‘й‡Џй•їеє¦еє”иЇҐдёЋз±»е€«ж•°й‡ЏдёЂи‡ґпјЊе¦‚жћњ reduction еЏ‚ж•°дёє 'mean'пјЊй‚Јд№€ж€‘д»¬дёЌйњЂи¦Ѓе…іжіЁ
# weight зљ„е°єеє¦пјЊеЏЄйњЂи¦Ѓе…іжіЁеђ„з±»е€«зљ„ weight жЇ”дѕ‹еЌіеЏЇгЂ‚
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="c1"># weights = torch.tensor([0.7, 0.3], dtype=torch.float)
</span>
<span class="n">loss_f_none_w</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss_f_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)</span>
<span class="n">loss_f_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)</span>

<span class="c1"># forward
</span><span class="n">loss_none_w</span> <span class="o">=</span> <span class="n">loss_f_none_w</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss_sum</span> <span class="o">=</span> <span class="n">loss_f_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss_mean</span> <span class="o">=</span> <span class="n">loss_f_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># view
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">weights: "</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss_none_w</span><span class="p">,</span> <span class="n">loss_sum</span><span class="p">,</span> <span class="n">loss_mean</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>иѕ“е‡єз»“жћњпјљ</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>weights:  tensor([1., 2.])
tensor([1.3133, 0.2539, 0.2539]) tensor(1.8210) tensor(0.3642)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>еЇ№жЇ”д№‹е‰ЌжІЎжњ‰и®ѕзЅ®жќѓеЂјзљ„з»“жћњпјЊж€‘д»¬еЏ‘зЋ°пјЊењЁ <code class="language-plaintext highlighter-rouge">none</code> жЁЎејЏдё‹пјЊз”±дєЋз¬¬дёЂдёЄж ·жњ¬з±»е€«дёє 0пјЊиЂЊе…¶жќѓеЂјдёє $1$пјЊж‰Ђд»Ґз»“жћње’Њд№‹е‰ЌдёЂж ·пјЊйѓЅжЇ $1.3133$гЂ‚иЂЊз¬¬дєЊдёЄе’Њз¬¬дё‰дёЄж ·жњ¬з±»е€«дёє $1$пјЊжќѓеЂјдёє $2$пјЊж‰Ђд»Ґиї™й‡Њзљ„ loss жЇд№‹е‰Ќзљ„ $2$ еЂЌпјЊеЌі $0.2539$гЂ‚еЇ№дєЋ <code class="language-plaintext highlighter-rouge">sum</code> жЁЎејЏпјЊе…¶з»“жћњдёєдё‰дёЄж ·жњ¬зљ„ loss д№‹е’ЊпјЊеЌі $1.8210$гЂ‚иЂЊеЇ№дєЋ <code class="language-plaintext highlighter-rouge">mean</code> жЁЎејЏпјЊзЋ°ењЁдёЌе†ЌжЇз®ЂеЌ•ењ°е°†дё‰дёЄ loss з›ёеЉ ж±‚е№іеќ‡пјЊиЂЊжЇй‡‡з”Ёдє†еЉ жќѓе№іеќ‡зљ„и®Ўз®—ж–№ејЏпјље› дёєз¬¬дёЂдёЄж ·жњ¬жќѓеЂјдёє $1$пјЊз¬¬дєЊдёЄе’Њз¬¬дё‰дёЄж ·жњ¬жќѓеЂјйѓЅжЇ $2$пјЊж‰Ђд»ҐдёЂе…±жњ‰ $1+2+2=5$ д»ЅпјЊloss зљ„еЉ жќѓеќ‡еЂјдёє $1.8210 / 5 = 0.3642$гЂ‚</p>

<p>дё‹йќўж€‘д»¬йЂљиї‡ж‰‹еЉЁи®Ўз®—жќҐйЄЊиЇЃењЁи®ѕзЅ®жќѓеЂјзљ„жѓ…е†µдё‹пјЊ<code class="language-plaintext highlighter-rouge">mean</code> жЁЎејЏдё‹зљ„ loss и®Ўз®—ж–№ејЏжЇеђ¦ж­ЈзЎ®пјљ</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">weights_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">weights</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">x</span><span class="p">],</span> <span class="n">target</span><span class="p">.</span><span class="n">numpy</span><span class="p">())))</span>

<span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">loss_sep</span> <span class="o">=</span> <span class="n">loss_none</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">x_class</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">loss_sep</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">x_class</span><span class="p">]</span> <span class="o">/</span> <span class="n">weights_all</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">+=</span> <span class="n">tmp</span>

<span class="k">print</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>иѕ“е‡єз»“жћњпјљ</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>0.3641947731375694
</pre></td></tr></tbody></table></code></pre></div></div>

<p>еЏЇд»Ґзњ‹е€°пјЊж‰‹еЉЁи®Ўз®—зљ„з»“жћње’Њ PyTorch дё­и‡ЄеЉЁж±‚еЏ–зљ„з»“жћњдёЂи‡ґпјЊж‰Ђд»ҐеЇ№дєЋи®ѕзЅ®жќѓеЂјзљ„жѓ…е†µпјЊ<code class="language-plaintext highlighter-rouge">mean</code> жЁЎејЏдё‹зљ„ loss дёЌжЇз®ЂеЌ•зљ„ж±‚е’Њд№‹еђЋй™¤д»Ґж ·жњ¬дёЄж•°пјЊиЂЊжЇй™¤д»ҐжќѓеЂјзљ„д»Ѕж•°пјЊеЌіе®ћй™…и®Ўз®—зљ„жЇеЉ жќѓеќ‡еЂјгЂ‚</p>

<h2 id="3-nllbcebcewithlogits-loss">3. NLL/BCE/BCEWithLogits Loss</h2>

<h4 id="nnnllloss"><code class="language-plaintext highlighter-rouge">nn.NLLLoss</code></h4>

<p><strong>еЉџиѓЅ</strong>пјље®ћзЋ°иґџеЇ№ж•°дјјз„¶е‡Ѕж•°дё­зљ„ <strong>иґџеЏ·еЉџиѓЅ</strong>гЂ‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">NLLLoss</span><span class="p">(</span>
    <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>дё»и¦ЃеЏ‚ж•°</strong>пјљ</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">weight</code>пјљеђ„з±»е€«зљ„ loss и®ѕзЅ®жќѓеЂјгЂ‚</li>
  <li><code class="language-plaintext highlighter-rouge">ignore_index</code>пјљеїЅз•ҐжџђдёЄз±»е€«гЂ‚</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>пјљи®Ўз®—жЁЎејЏпјЊеЏЇдёє <code class="language-plaintext highlighter-rouge">none/sum/mean</code>гЂ‚
    <ul>
      <li><code class="language-plaintext highlighter-rouge">none</code>пјљйЂђдёЄе…ѓзґ и®Ўз®—гЂ‚</li>
      <li><code class="language-plaintext highlighter-rouge">sum</code>пјљж‰Ђжњ‰е…ѓзґ ж±‚е’ЊпјЊиї”е›ћж ‡й‡ЏгЂ‚</li>
      <li><code class="language-plaintext highlighter-rouge">mean</code>пјљеЉ жќѓе№іеќ‡пјЊиї”е›ћж ‡й‡ЏгЂ‚</li>
    </ul>
  </li>
</ul>

<p><strong>и®Ўз®—е…¬ејЏ</strong>пјљ</p>

\[\ell(x,y) = L = \{l_1,\dots,l_N\}^{\mathrm T}\;,\qquad l_n = -w_{y_n}x_{n,y_n}\]

<p><strong>д»Јз Ѓз¤єдѕ‹</strong>пјљ</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="rouge-code"><pre><span class="c1"># fake data, иї™й‡Њж€‘д»¬дЅїз”Ёзљ„иїжЇд№‹е‰Ќзљ„ж•°жЌ®пјЊжіЁж„Џ label ењЁиї™й‡Њеї…йЎ»и®ѕзЅ®дёє long
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

<span class="c1"># weights
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="c1"># NLL loss
</span><span class="n">loss_f_none_w</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss_f_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)</span>
<span class="n">loss_f_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)</span>

<span class="c1"># forward
</span><span class="n">loss_none_w</span> <span class="o">=</span> <span class="n">loss_f_none_w</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss_sum</span> <span class="o">=</span> <span class="n">loss_f_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss_mean</span> <span class="o">=</span> <span class="n">loss_f_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># view
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">weights: "</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"NLL Loss"</span><span class="p">,</span> <span class="n">loss_none_w</span><span class="p">,</span> <span class="n">loss_sum</span><span class="p">,</span> <span class="n">loss_mean</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>иѕ“е‡єз»“жћњпјљ</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>weights:  tensor([1., 1.])
NLL Loss tensor([-1., -3., -3.]) tensor(-7.) tensor(-2.3333)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>жіЁж„ЏпјЊиї™й‡Њ <code class="language-plaintext highlighter-rouge">nn.NLLLoss</code> е®ћй™…дёЉеЏЄжЇе®ћзЋ°дє†дёЂдёЄиґџеЏ·зљ„еЉџиѓЅгЂ‚еЇ№дєЋ <code class="language-plaintext highlighter-rouge">none</code> жЁЎејЏпјљиї™й‡Њз¬¬дёЂдёЄж ·жњ¬жЇз¬¬ 0 з±»пјЊж‰Ђд»Ґж€‘д»¬иї™й‡ЊеЏЄеЇ№з¬¬дёЂдёЄзҐћз»Џе…ѓиї›иЎЊи®Ўз®—пјЊеЏ–иґџеЏ·еѕ—е€° NLL Loss дёє $-1$пј›з¬¬дєЊдёЄж ·жњ¬жЇз¬¬ 1 з±»пјЊж€‘д»¬еЇ№з¬¬дєЊдёЄзҐћз»Џе…ѓиї›иЎЊи®Ўз®—пјЊеЏ–иґџеЏ·еѕ—е€° NLL Loss дёє $-3$пј›з¬¬дё‰дёЄж ·жњ¬д№џжЇз¬¬ 1 з±»пјЊж€‘д»¬еЇ№з¬¬дєЊдёЄзҐћз»Џе…ѓиї›иЎЊи®Ўз®—пјЊеЏ–иґџеЏ·еѕ—е€° NLL Loss дёє $-3$гЂ‚еЇ№дєЋ <code class="language-plaintext highlighter-rouge">sum</code> жЁЎејЏпјЊе°†дё‰дёЄж ·жњ¬зљ„ NLL Loss ж±‚е’ЊпјЊеѕ—е€° $-7$гЂ‚еЇ№дєЋ <code class="language-plaintext highlighter-rouge">mean</code> жЁЎејЏпјЊе°†дё‰дёЄж ·жњ¬зљ„ NLL Loss еЉ жќѓе№іеќ‡пјЊеѕ—е€° $-2.3333$гЂ‚</p>

<h4 id="nnbceloss"><code class="language-plaintext highlighter-rouge">nn.BCELoss</code></h4>

<p><strong>еЉџиѓЅ</strong>пјљдєЊе€†з±»дє¤еЏ‰з†µгЂ‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">BCELoss</span><span class="p">(</span>
    <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>дё»и¦ЃеЏ‚ж•°</strong>пјљ</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">weight</code>пјљеђ„з±»е€«зљ„ loss и®ѕзЅ®жќѓеЂјгЂ‚</li>
  <li><code class="language-plaintext highlighter-rouge">ignore_index</code>пјљеїЅз•ҐжџђдёЄз±»е€«гЂ‚</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>пјљи®Ўз®—жЁЎејЏпјЊеЏЇдёє <code class="language-plaintext highlighter-rouge">none/sum/mean</code>гЂ‚
    <ul>
      <li><code class="language-plaintext highlighter-rouge">none</code>пјљйЂђдёЄе…ѓзґ и®Ўз®—гЂ‚</li>
      <li><code class="language-plaintext highlighter-rouge">sum</code>пјљж‰Ђжњ‰е…ѓзґ ж±‚е’ЊпјЊиї”е›ћж ‡й‡ЏгЂ‚</li>
      <li><code class="language-plaintext highlighter-rouge">mean</code>пјљеЉ жќѓе№іеќ‡пјЊиї”е›ћж ‡й‡ЏгЂ‚</li>
    </ul>
  </li>
</ul>

<p><strong>и®Ўз®—е…¬ејЏ</strong>пјљ</p>

\[l_n = -w_n [y_n \cdot \log x_n + (1- y_n) \cdot \log(1-x_n)]\]

<p><strong>жіЁж„Џдє‹йЎ№</strong>пјљз”±дєЋдє¤еЏ‰з†µжЇиЎЎй‡Џдё¤дёЄж¦‚зЋ‡е€†еёѓд№‹й—ґзљ„е·®еј‚пјЊе› ж­¤иѕ“е…ҐеЂјеЏ–еЂјеї…йЎ»ењЁ $[0, 1]$гЂ‚</p>

<p><strong>д»Јз Ѓз¤єдѕ‹</strong>пјљ</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre><span class="c1"># fake data, иї™й‡Њж€‘д»¬и®ѕзЅ® 4 дёЄж ·жњ¬пјЊжіЁж„Џ label ењЁиї™й‡Њеї…йЎ»и®ѕзЅ®дёє float
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="n">target_bce</span> <span class="o">=</span> <span class="n">target</span>

<span class="c1"># itarget
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># е€©з”Ё Sigmoid е‡Ѕж•°е°†иѕ“е…ҐеЂјеЋ‹зј©е€° [0,1]
</span>
<span class="c1"># weights
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="c1"># BCE loss
</span><span class="n">loss_f_none_w</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss_f_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)</span>
<span class="n">loss_f_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)</span>

<span class="c1"># forward
</span><span class="n">loss_none_w</span> <span class="o">=</span> <span class="n">loss_f_none_w</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_bce</span><span class="p">)</span>
<span class="n">loss_sum</span> <span class="o">=</span> <span class="n">loss_f_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_bce</span><span class="p">)</span>
<span class="n">loss_mean</span> <span class="o">=</span> <span class="n">loss_f_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_bce</span><span class="p">)</span>

<span class="c1"># view
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">weights: "</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"BCE Loss"</span><span class="p">,</span> <span class="n">loss_none_w</span><span class="p">,</span> <span class="n">loss_sum</span><span class="p">,</span> <span class="n">loss_mean</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>иѕ“е‡єз»“жћњпјљ</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>weights:  tensor([1., 1.])
BCE Loss tensor([[0.3133, 2.1269],
        [0.1269, 2.1269],
        [3.0486, 0.0181],
        [4.0181, 0.0067]]) tensor(11.7856) tensor(1.4732)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>з”±дєЋиї™й‡Њж€‘д»¬жњ‰ 4 дёЄж ·жњ¬пјЊжЇЏдёЄж ·жњ¬жњ‰ 2 дёЄзҐћз»Џе…ѓпјЊе› ж­¤ењЁ <code class="language-plaintext highlighter-rouge">none</code> жЁЎејЏдё‹ж€‘д»¬иї™й‡Њеѕ—е€° 8 дёЄ lossпјЊеЌіжЇЏдёЂдёЄзҐћз»Џе…ѓдјљдёЂдёЂеЇ№еє”ењ°и®Ўз®— lossгЂ‚иЂЊ <code class="language-plaintext highlighter-rouge">sum</code> жЁЎејЏе°±жЇз®ЂеЌ•ењ°е°†иї™ 8 дёЄ loss иї›иЎЊз›ёеЉ пјЊ<code class="language-plaintext highlighter-rouge">mean</code> жЁЎејЏе°±жЇеЇ№иї™ 8 дёЄ loss ж±‚еЉ жќѓеќ‡еЂјгЂ‚</p>

<p>дё‹йќўж€‘д»¬йЂљиї‡ж‰‹еЉЁи®Ўз®—жќҐйЄЊиЇЃз¬¬дёЂдёЄзҐћз»Џе…ѓзљ„ BCE loss еЂјжЇеђ¦з­‰дєЋ $0.3133$пјљ</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">x_i</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">()[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span>  <span class="c1"># иЋ·еЏ–з¬¬дёЂдёЄзҐћз»Џе…ѓзљ„иѕ“е‡єеЂј
</span><span class="n">y_i</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span>  <span class="c1"># иЋ·еЏ–з¬¬дёЂдёЄзҐћз»Џе…ѓзљ„ж ‡з­ѕ
</span>
<span class="c1"># loss
# l_i = -[ y_i * np.log(x_i) + (1-y_i) * np.log(1-y_i) ]      # np.log(0) = nan
</span><span class="n">l_i</span> <span class="o">=</span> <span class="o">-</span><span class="n">y_i</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span> <span class="k">if</span> <span class="n">y_i</span> <span class="k">else</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y_i</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x_i</span><span class="p">)</span>

<span class="c1"># иѕ“е‡єloss
</span><span class="k">print</span><span class="p">(</span><span class="s">"BCE inputs: "</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"з¬¬дёЂдёЄ loss дёє: "</span><span class="p">,</span> <span class="n">l_i</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>иѕ“е‡єз»“жћњпјљ</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>BCE inputs:  tensor([[0.7311, 0.8808],
        [0.8808, 0.8808],
        [0.9526, 0.9820],
        [0.9820, 0.9933]])
з¬¬дёЂдёЄ loss дёє:  0.31326166
</pre></td></tr></tbody></table></code></pre></div></div>

<p>еЏЇд»Ґзњ‹е€°пјЊж‰‹еЉЁи®Ўз®—зљ„з»“жћњдёЋ PyTorch дё­ <code class="language-plaintext highlighter-rouge">nn.BCELoss</code> зљ„и®Ўз®—з»“жћњдёЂи‡ґгЂ‚</p>

<h4 id="nnbcewithlogitsloss"><code class="language-plaintext highlighter-rouge">nn.BCEWithLogitsLoss</code></h4>

<p><strong>еЉџиѓЅ</strong>пјљз»“еђ€ Sigmoid дёЋ дєЊе€†з±»дє¤еЏ‰з†µгЂ‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span>
    <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">,</span>
    <span class="n">pos_weight</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>дё»и¦ЃеЏ‚ж•°</strong>пјљ</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pos_weight</code>пјљж­Јж ·жњ¬зљ„жќѓеЂјпјЊз”ЁдєЋе№іиЎЎж­Јиґџж ·жњ¬гЂ‚
    <ul>
      <li>дѕ‹е¦‚пјљж­Јж ·жњ¬жњ‰ 100 дёЄпјЊиґџж ·жњ¬жњ‰ 300 дёЄпјЊж­Јиґџж ·жњ¬жЇ”дѕ‹дёє $1:3$гЂ‚е› ж­¤ж€‘д»¬еЏЇд»Ґе°†иЇҐйЎ№и®ѕдёє $3$пјЊиї™ж ·еЌіз­‰д»·дєЋж­Јиґџж ·жњ¬еђ„ 300 дёЄгЂ‚</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">weight</code>пјљеђ„з±»е€«зљ„ loss и®ѕзЅ®жќѓеЂјгЂ‚</li>
  <li><code class="language-plaintext highlighter-rouge">ignore_index</code>пјљеїЅз•ҐжџђдёЄз±»е€«гЂ‚</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>пјљи®Ўз®—жЁЎејЏпјЊеЏЇдёє <code class="language-plaintext highlighter-rouge">none/sum/mean</code>гЂ‚
    <ul>
      <li><code class="language-plaintext highlighter-rouge">none</code>пјљйЂђдёЄе…ѓзґ и®Ўз®—гЂ‚</li>
      <li><code class="language-plaintext highlighter-rouge">sum</code>пјљж‰Ђжњ‰е…ѓзґ ж±‚е’ЊпјЊиї”е›ћж ‡й‡ЏгЂ‚</li>
      <li><code class="language-plaintext highlighter-rouge">mean</code>пјљеЉ жќѓе№іеќ‡пјЊиї”е›ћж ‡й‡ЏгЂ‚</li>
    </ul>
  </li>
</ul>

<p><strong>и®Ўз®—е…¬ејЏ</strong>пјљ</p>

\[l_n = -w_n[y_n \cdot \log \sigma(x_n) + (1-y_n)\cdot \log (1-\sigma (x_n))]\]

<p><strong>жіЁж„Џдє‹йЎ№</strong>пјљзЅ‘з»њжњЂеђЋдёЌеЉ  Sigmoid е‡Ѕж•°гЂ‚</p>

<p><strong>д»Јз Ѓз¤єдѕ‹</strong>пјљ</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="n">target_bce</span> <span class="o">=</span> <span class="n">target</span>

<span class="c1"># inputs = torch.sigmoid(inputs)  # иї™й‡ЊеўћеЉ  sigmoid дјљдЅїеѕ—и®Ўз®—дёЌе‡†зЎ®пјЊе› дёєз›ёеЅ“дєЋеЉ дє†дё¤е±‚ sigmoid
</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="n">loss_f_none_w</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss_f_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)</span>
<span class="n">loss_f_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)</span>

<span class="c1"># forward
</span><span class="n">loss_none_w</span> <span class="o">=</span> <span class="n">loss_f_none_w</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_bce</span><span class="p">)</span>
<span class="n">loss_sum</span> <span class="o">=</span> <span class="n">loss_f_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_bce</span><span class="p">)</span>
<span class="n">loss_mean</span> <span class="o">=</span> <span class="n">loss_f_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_bce</span><span class="p">)</span>

<span class="c1"># view
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">weights: "</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss_none_w</span><span class="p">,</span> <span class="n">loss_sum</span><span class="p">,</span> <span class="n">loss_mean</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>иѕ“е‡єз»“жћњпјљ</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>weights:  tensor([1., 1.])
tensor([[0.3133, 2.1269],
        [0.1269, 2.1269],
        [3.0486, 0.0181],
        [4.0181, 0.0067]]) tensor(11.7856) tensor(1.4732)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>ж€‘д»¬жќҐзњ‹дёЂдё‹ <code class="language-plaintext highlighter-rouge">pos_weight</code> зљ„и®ѕзЅ®пјљ</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="rouge-code"><pre><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="n">target_bce</span> <span class="o">=</span> <span class="n">target</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">pos_w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>  <span class="c1"># е°† pos_weight и®ѕдёє 1 
</span>
<span class="n">loss_f_none_w</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_w</span><span class="p">)</span>
<span class="n">loss_f_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_w</span><span class="p">)</span>
<span class="n">loss_f_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_w</span><span class="p">)</span>

<span class="c1"># forward
</span><span class="n">loss_none_w</span> <span class="o">=</span> <span class="n">loss_f_none_w</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_bce</span><span class="p">)</span>
<span class="n">loss_sum</span> <span class="o">=</span> <span class="n">loss_f_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_bce</span><span class="p">)</span>
<span class="n">loss_mean</span> <span class="o">=</span> <span class="n">loss_f_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_bce</span><span class="p">)</span>

<span class="c1"># view
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">pos_weights: "</span><span class="p">,</span> <span class="n">pos_w</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss_none_w</span><span class="p">,</span> <span class="n">loss_sum</span><span class="p">,</span> <span class="n">loss_mean</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>иѕ“е‡єз»“жћњпјљ</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>pos_weights:  tensor([1.])
tensor([[0.3133, 2.1269],
        [0.1269, 2.1269],
        [3.0486, 0.0181],
        [4.0181, 0.0067]]) tensor(11.7856) tensor(1.4732)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>еЏЇд»Ґзњ‹е€°пјЊеЅ“ <code class="language-plaintext highlighter-rouge">pos_weight</code> и®ѕдёє $1$ ж—¶пјЊи®Ўз®—зљ„ loss з»“жћњдёЋд№‹е‰ЌдёЂж ·гЂ‚жЋҐдё‹жќҐж€‘д»¬е°† <code class="language-plaintext highlighter-rouge">pos_weight</code> ж”№дёє $3$ жќҐзњ‹дёЂдё‹з»“жћњдјље¦‚дЅ•еЏеЊ–пјљ</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">pos_w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>  <span class="c1"># е°† pos_weight и®ѕдёє 3
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>иѕ“е‡єз»“жћњпјљ</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>pos_weights:  tensor([3.])
tensor([[0.9398, 2.1269],
        [0.3808, 2.1269],
        [3.0486, 0.0544],
        [4.0181, 0.0201]]) tensor(12.7158) tensor(1.5895)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>дё‹иЉ‚е†…е®№пјљжЌџе¤±е‡Ѕж•° (дёЂ)</p>
:ET