I"æ<!-- æ•°å­¦å…¬å¼ -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-02-å¯¼è®º-2">Lecture 02 å¯¼è®º (2)</h1>
<h2 id="4-æ‹Ÿåˆä¸€ä¸ªå…³äº-gavote-çš„çº¿æ€§æ¨¡å‹">4. æ‹Ÿåˆä¸€ä¸ªå…³äº <code class="highlighter-rouge">gavote</code> çš„çº¿æ€§æ¨¡å‹</h2>

<p>ä¸€ä¸ªçº¿æ€§æ¨¡å‹ $\mathbf y=X\boldsymbol \beta+\boldsymbol \varepsilon$ å¯ä»¥åˆ©ç”¨ <strong>æœ€å°äºŒä¹˜æ³•ï¼ˆLeast Squares methodï¼ŒLSï¼‰</strong> æ¥æ‹Ÿåˆæ•°æ®ã€‚æ®æ­¤å¾—åˆ°çš„å‚æ•° $\boldsymbol \beta$ çš„ <strong>æœ€å°äºŒä¹˜ä¼°è®¡é‡ï¼ˆLS estimatorï¼‰</strong> ä¸ºï¼š</p>

<script type="math/tex; mode=display">\hat{\boldsymbol \beta}=(X^{\mathrm{T}}X)^{-1}X^{\mathrm{T}}\mathbf y</script>

<p>æœ€å°äºŒä¹˜æ³•é€šè¿‡æœ€å°åŒ–æ®‹å·®å’Œæ¥å¯¹æ¨¡å‹å‚æ•° $\boldsymbol \beta$ è¿›è¡Œä¼°è®¡ï¼Œä»è€Œæ‹Ÿåˆæ•°æ®ï¼š</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-03-05-WX20200305-182742%402x.png" width="80%" /></p>

<p>åœ¨ $\hat {\boldsymbol \beta}$ çš„æœ€å°äºŒä¹˜ä¼°è®¡é‡ä¸­ï¼Œ$X^{\mathrm{T}}X$ çš„ç»“æœæ˜¯ä¸€ä¸ª $p\times p$ çš„çŸ©é˜µï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ï¼ˆå³ $X$ ä¸ºä¸€ä¸ªæ»¡ç§©çŸ©é˜µæ—¶ï¼‰ï¼Œè¯¥çŸ©é˜µæ˜¯å¯é€†çš„ï¼›è€Œå½“ $X$ ä¸ºä¸€ä¸ªé™ç§©çŸ©é˜µæ—¶ï¼Œ$X^{\mathrm{T}}X$ ä¸å¯é€†ã€‚</p>

<p><br /></p>

<p>å‡è®¾æˆ‘ä»¬åœ¨å»ºæ¨¡æ—¶å°† <code class="highlighter-rouge">undercount</code> ä½œä¸ºå“åº”å˜é‡ï¼ˆresponse variableï¼‰ï¼Œå°†æ”¯æŒ Gore çš„é€‰æ°‘å æ¯” <code class="highlighter-rouge">pergore</code> å’Œéè£”ç¾å›½äººå æ¯” <code class="highlighter-rouge">perAA</code> ä½œä¸ºé¢„æµ‹å˜é‡ï¼ˆpredictor variablesï¼‰ï¼Œåˆ™å¯¹åº”çš„å›å½’æ–¹ç¨‹ä¸ºï¼š</p>

<script type="math/tex; mode=display">\mathsf{undercount}=\beta_0+\beta_1\mathsf{pergore}+\beta_2 \mathsf{perAA}+\varepsilon</script>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; lmod &lt;- lm(undercount ~ pergore + perAA, gavote); coef(lmod)
(Intercept)     pergore       perAA
 0.03237600  0.01097872  0.02853314
</code></pre></div></div>

<p>æ‰€ä»¥ï¼Œåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œä¸‰ä¸ªå‚æ•°çš„æœ€å°äºŒä¹˜ä¼°è®¡åˆ†åˆ«ä¸ºï¼š$\hat \beta_0=0.03237600,\;\hat \beta_1=0.01097872,\;\hat \beta_0=0.02853314$ã€‚</p>

<p>ç„¶è€Œï¼Œæœ€å°äºŒä¹˜æ³•è¯´åˆ°åº•åªæ˜¯ä¸€ç§è®¡ç®—æ•°å­¦æ–¹æ³•ï¼Œå®ƒè¿˜ä¸è¶³ä»¥å¥½åˆ°å¯¹æ‰€ä¼°è®¡çš„å‚æ•°ç»™å‡ºè¯„åˆ¤ã€‚æ‰€ä»¥äººä»¬å¯èƒ½ä¼šé—®ï¼šä¸ºä»€ä¹ˆè¦é‡‡ç”¨æœ€å°äºŒä¹˜æ³•è¿›è¡Œå‚æ•°ä¼°è®¡å‘¢ï¼Ÿé€šè¿‡å®ƒæ‰€å¾—åˆ°çš„å‚æ•°ä¼°è®¡çš„ä¼˜åº¦å¦‚ä½•ï¼Ÿè¦å›ç­”è¿™ä¸ªé—®é¢˜éœ€è¦ä¸€äº›ç»Ÿè®¡å­¦æ–¹æ³•ä½œä¸ºæ”¯æŒã€‚</p>

<p><strong>é«˜æ–¯-é©¬å°”å¯å¤«å®šç†ï¼ˆGaussâ€“Markov theoremï¼‰</strong> è¡¨æ˜ä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹çš„å‚æ•°çš„æœ€å°äºŒä¹˜ä¼°è®¡é‡ $\hat{\boldsymbol \beta}$ æ˜¯ä¸€ä¸ª <strong>æœ€ä½³çº¿æ€§æ— åä¼°è®¡é‡ï¼ˆbest linear unbiased estimator, BLUEï¼‰</strong>ã€‚</p>

<p>é¦–å…ˆï¼Œæœ€å°äºŒä¹˜ä¼°è®¡é‡æ˜¯ <strong>æ— åçš„ï¼ˆunbiasedï¼‰</strong>ï¼Œå³å®ƒçš„æœŸæœ›ç­‰äºçœŸå®å‚æ•°çš„æœŸæœ›ã€‚å…¶æ¬¡ï¼Œå®ƒæ˜¯ <strong>æœ€ä½³çš„ï¼ˆbestï¼‰</strong>ï¼Œå³å®ƒçš„æ–¹å·®åœ¨æ‰€æœ‰çš„æ— åä¼°è®¡é‡ä¸­æ˜¯æœ€å°çš„ã€‚æœ€åï¼Œå®ƒæ˜¯ <strong>çº¿æ€§çš„ï¼ˆlinearï¼‰</strong>ã€‚å› æ­¤ï¼Œé«˜æ–¯-é©¬å°”å¯å¤«å®šç†ä¸ºè¯„ä»·çº¿æ€§æ¨¡å‹ä¸‹çš„æœ€å°äºŒä¹˜ä¼°è®¡çš„ä¼˜åº¦æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚æ³¨æ„ï¼Œè¿™é‡Œæˆ‘ä»¬å¼ºè°ƒäº†æ˜¯çº¿æ€§æ¨¡å‹ï¼Œå¯¹äºåé¢å³å°†æ‰©å±•åˆ°çš„éçº¿æ€§æ¨¡å‹è€Œè¨€ï¼Œé«˜æ–¯-é©¬å°”å¯å¤«å®šç†å°†ä¸å†é€‚ç”¨ã€‚æ‰€ä»¥ï¼Œå¯¹äºéçº¿æ€§çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å¦å¤–çš„ç†è®ºæ¥è¯„ä»·å‚æ•°ä¼°è®¡çš„ä¼˜åº¦ã€‚</p>

<p>å¦‚æœ $\varepsilon$ è¢«å‡è®¾ä¸ºæ­£æ€ï¼Œé‚£ä¹ˆå¯ä»¥è¯æ˜ $\boldsymbol \beta$ çš„ <strong>æœ€å¤§ä¼¼ç„¶ä¼°è®¡é‡ï¼ˆmaximum likelihood estimator, MLEï¼‰</strong> ç­‰äºå…¶æœ€å°äºŒä¹˜ä¼°è®¡é‡ï¼Œå³ $\hat{\boldsymbol \beta}=(X^{\mathrm{T}}X)^{-1}X^{\mathrm{T}}\mathbf y$ã€‚</p>

<p><br /></p>

<p>æ¨¡å‹çš„é¢„æµ‹å€¼æˆ–è€… <strong>æ‹Ÿåˆå€¼ï¼ˆfitted valuesï¼‰</strong> ä¸º $\hat{\mathbf y}=X^{\mathrm{T}}\hat{\boldsymbol \beta}$</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; predict(lmod)
      APPLING      ATKINSON         BACON         BAKER       BALDWIN         BANKS  ...
   0.04133661    0.04329088    0.03961823    0.05241202    0.04795484    0.03601558  ...
</code></pre></div></div>

<p>æ¨¡å‹çš„ <strong>æ®‹å·®ï¼ˆresidualsï¼‰</strong> ä¸º $\hat{\boldsymbol \varepsilon}=\mathbf y-X^{\mathrm T}\hat{\boldsymbol \beta}=\mathbf y-\hat{\mathbf y}$</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; residuals(lmod)
      APPLING      ATKINSON         BACON         BAKER       BALDWIN         BANKS  ...
 3.694660e-02 -6.994927e-03  6.555058e-02  2.348407e-03  3.589940e-03  1.426726e-02  ...
</code></pre></div></div>

<p>æ¨¡å‹çš„ <strong>æ®‹å·®å¹³æ–¹å’Œï¼ˆresidual sum of squares, RSSï¼‰</strong>ï¼Œä¹Ÿè¢«ç§°ä¸º <strong>å¼‚å¸¸ï¼ˆdevianceï¼‰</strong>ï¼Œä¸º $\mathsf{RSS}=\hat{\boldsymbol \varepsilon}^{\mathrm{T}}\hat{\boldsymbol \varepsilon}$ï¼Œå®ƒè¡¡é‡äº†æ¨¡å‹å¯¹æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ã€‚</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; deviance(lmod)
[1] 0.09324918
</code></pre></div></div>

<p>æ®‹å·®çš„ <strong>è‡ªç”±åº¦ï¼ˆdegrees of freedom, dfï¼‰</strong> ä¸º $n-p$</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; df.residual(lmod)
[1] 156

&gt; nrow(gavote)-length(coef(lmod))
[1] 156
</code></pre></div></div>

<p>ä¸‹èŠ‚å†…å®¹ï¼šå¯¼è®ºï¼ˆç»­ï¼‰</p>
:ET