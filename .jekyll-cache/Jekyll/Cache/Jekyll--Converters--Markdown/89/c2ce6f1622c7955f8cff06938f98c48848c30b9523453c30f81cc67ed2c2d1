I"’<h1 id="lecture-14-æƒå€¼åˆå§‹åŒ–">Lecture 14 æƒå€¼åˆå§‹åŒ–</h1>

<p>åœ¨å‰å‡ èŠ‚è¯¾ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•æ­å»ºç½‘ç»œæ¨¡å‹ã€‚åœ¨ç½‘ç»œæ¨¡å‹æ­å»ºå¥½ä¹‹åï¼Œæœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„æ­¥éª¤ï¼Œå°±æ˜¯å¯¹æ¨¡å‹ä¸­çš„æƒå€¼è¿›è¡Œåˆå§‹åŒ–ï¼šæ­£ç¡®çš„æƒå€¼åˆå§‹åŒ–å¯ä»¥åŠ å¿«æ¨¡å‹çš„æ”¶æ•›ï¼Œè€Œä¸é€‚å½“çš„æƒå€¼åˆå§‹åŒ–å¯ä»¥ä¼šå¼•å‘æ¢¯åº¦æ¶ˆå¤±æˆ–è€…çˆ†ç‚¸ï¼Œæœ€ç»ˆå¯¼è‡´æ¨¡å‹æ— æ³•è®­ç»ƒã€‚æœ¬èŠ‚è¯¾ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•è¿›è¡Œæƒå€¼åˆå§‹åŒ–ã€‚</p>

<h2 id="1-æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸">1. æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸</h2>

<p>è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥ä¸ŠèŠ‚è¯¾ä¸­æåˆ°çš„ä¸€ä¸ªä¸‰å±‚çš„å…¨è¿æ¥ç½‘ç»œä¸ºä¾‹ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ç¬¬äºŒä¸ªéšè—å±‚ä¸­çš„æƒå€¼ $W_2$ çš„æ¢¯åº¦æ˜¯æ€ä¹ˆæ±‚å–çš„ã€‚</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-18-WX20201218-170939%402x.png" width="80%" /></p>

<p>ä»å…¬å¼è§’åº¦æ¥çœ‹ï¼Œä¸ºäº†é˜²æ­¢å‘ç”Ÿæ¢¯åº¦æ¶ˆå¤±æˆ–è€…çˆ†ç‚¸ï¼Œæˆ‘ä»¬å¿…é¡»ä¸¥æ ¼æ§åˆ¶ç½‘ç»œå±‚è¾“å‡ºå€¼çš„å°ºåº¦èŒƒå›´ï¼Œå³æ¯ä¸ªç½‘ç»œå±‚çš„è¾“å‡ºå€¼ä¸èƒ½å¤ªå¤§æˆ–è€…å¤ªå°ã€‚</p>

<p><strong>ä»£ç ç¤ºä¾‹</strong>ï¼š</p>

<p><strong>100 ä¸ªçº¿æ€§å±‚çš„ç®€å•å åŠ </strong>ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">tools.common_tools</span> <span class="kn">import</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># è®¾ç½®éšæœºç§å­
</span>

<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neural_num</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">neural_num</span><span class="p">,</span> <span class="n">neural_num</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span><span class="p">)])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">neural_num</span> <span class="o">=</span> <span class="n">neural_num</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">linear</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linears</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># normal: mean=0, std=1
</span>

<span class="n">layer_nums</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># ç½‘ç»œå±‚æ•°
</span><span class="n">neural_nums</span> <span class="o">=</span> <span class="mi">256</span>  <span class="c1"># æ¯å±‚çš„ç¥ç»å…ƒä¸ªæ•°
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># è¾“å…¥æ•°æ®çš„ batch size
</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">neural_nums</span><span class="p">,</span> <span class="n">layer_nums</span><span class="p">)</span>
<span class="n">net</span><span class="p">.</span><span class="n">initialize</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">neural_nums</span><span class="p">))</span>  <span class="c1"># normal: mean=0, std=1
</span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>è¾“å‡ºç»“æœï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre>tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=&lt;MmBackward&gt;) 
</pre></td></tr></tbody></table></code></pre></div></div>

<p>æˆ‘ä»¬å‘ç° <code class="language-plaintext highlighter-rouge">output</code> ä¸­çš„æ¯ä¸€ä¸ªå€¼éƒ½æ˜¯ <code class="language-plaintext highlighter-rouge">nan</code>ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ•°æ®å€¼å¯èƒ½éå¸¸å¤§æˆ–è€…éå¸¸å°ï¼Œå·²ç»è¶…å‡ºäº†å½“å‰ç²¾åº¦èƒ½å¤Ÿè¡¨ç¤ºçš„èŒƒå›´ã€‚æˆ‘ä»¬å¯ä»¥ä¿®æ”¹ä¸€ä¸‹ <code class="language-plaintext highlighter-rouge">forward</code> å‡½æ•°ï¼Œæ¥çœ‹ä¸€ä¸‹ä»€ä¹ˆæ—¶å€™æˆ‘ä»¬çš„æ•°æ®å˜ä¸ºäº† <code class="language-plaintext highlighter-rouge">nan</code>ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬é‡‡ç”¨æ ‡å‡†å·®ä½œä¸ºæŒ‡æ ‡æ¥è¡¡é‡æ•°æ®çš„å°ºåº¦èŒƒå›´ã€‚é¦–å…ˆæˆ‘ä»¬æ‰“å°å‡ºæ¯å±‚çš„æ ‡å‡†å·®ï¼Œæ¥ç€è¿›è¡Œä¸€ä¸ª <code class="language-plaintext highlighter-rouge">if</code> åˆ¤æ–­ï¼Œå¦‚æœ <code class="language-plaintext highlighter-rouge">x</code> çš„æ ‡å‡†å·®å˜ä¸º <code class="language-plaintext highlighter-rouge">nan</code> äº†åˆ™åœæ­¢å‰å‘ä¼ æ’­ã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">linear</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linears</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="s">"layer:{}, std:{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">std</span><span class="p">()))</span>  <span class="c1"># æ‰“å°æ¯å±‚çš„æ ‡å‡†å·®
</span>        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">std</span><span class="p">()):</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"output is nan in {} layers"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
            <span class="k">break</span>  <span class="c1"># å¦‚æœ x çš„æ ‡å‡†å·®å˜ä¸º nan åˆ™åœæ­¢å‰å‘ä¼ æ’­
</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>è¾“å‡ºç»“æœï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td><td class="rouge-code"><pre>layer:0, std:15.959932327270508
layer:1, std:256.6237487792969
layer:2, std:4107.24560546875
layer:3, std:65576.8125
layer:4, std:1045011.875
layer:5, std:17110408.0
layer:6, std:275461408.0
layer:7, std:4402537984.0
layer:8, std:71323615232.0
layer:9, std:1148104736768.0
layer:10, std:17911758454784.0
layer:11, std:283574846619648.0
layer:12, std:4480599809064960.0
layer:13, std:7.196814275405414e+16
layer:14, std:1.1507761512626258e+18
layer:15, std:1.853110740188555e+19
layer:16, std:2.9677725826641455e+20
layer:17, std:4.780376223769898e+21
layer:18, std:7.613223480799065e+22
layer:19, std:1.2092652108825478e+24
layer:20, std:1.923257075956356e+25
layer:21, std:3.134467063655912e+26
layer:22, std:5.014437766285408e+27
layer:23, std:8.066615144249704e+28
layer:24, std:1.2392661553516338e+30
layer:25, std:1.9455688099759845e+31
layer:26, std:3.0238180658999113e+32
layer:27, std:4.950357571077011e+33
layer:28, std:8.150925520353362e+34
layer:29, std:1.322983152787379e+36
layer:30, std:2.0786820453988485e+37
layer:31, std:nan
output is nan in 31 layers
tensor([[        inf, -2.6817e+38,         inf,  ...,         inf,
                 inf,         inf],
        [       -inf,        -inf,  1.4387e+38,  ..., -1.3409e+38,
         -1.9659e+38,        -inf],
        [-1.5873e+37,         inf,        -inf,  ...,         inf,
                -inf,  1.1484e+38],
        ...,
        [ 2.7754e+38, -1.6783e+38, -1.5531e+38,  ...,         inf,
         -9.9440e+37, -2.5132e+38],
        [-7.7184e+37,        -inf,         inf,  ..., -2.6505e+38,
                 inf,         inf],
        [        inf,         inf,        -inf,  ...,        -inf,
                 inf,  1.7432e+38]], grad_fn=&lt;MmBackward&gt;)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>å¯ä»¥çœ‹åˆ°ï¼Œå½“è¿›è¡Œåˆ° 31 å±‚çš„æ—¶å€™ï¼Œæ•°æ®çš„æ ‡å‡†å·®å°±å·²ç»å˜ä¸º <code class="language-plaintext highlighter-rouge">nan</code> äº†ã€‚æˆ‘ä»¬çœ‹åˆ°ï¼Œåœ¨ç¬¬ 31 å±‚çš„æ—¶å€™ï¼Œæ•°æ®çš„å€¼éƒ½éå¸¸å¤§æˆ–è€…éå¸¸å°ï¼Œå†å¾€åä¼ æ’­ï¼Œè®¡ç®—æœºå½“å‰çš„ç²¾åº¦å°±å·²ç»æ²¡åŠæ³•å»è¡¨ç¤ºè¿™äº›ç‰¹åˆ«å¤§æˆ–è€…ç‰¹åˆ«å°çš„æ•°æ®äº†ã€‚å¦å¤–ï¼Œå¯ä»¥çœ‹åˆ°æ¯ä¸€å±‚ç½‘ç»œçš„æ ‡å‡†å·®éƒ½æ˜¯é€æ¸å¢å¤§çš„ï¼Œç›´åˆ°ç¬¬ 31 å±‚ï¼Œå¤§çº¦åœ¨ $10^{38} \sim 10^{39}$ ä¹‹é—´ï¼Œè€Œè¿™å·²ç»è¶…å‡ºäº†æˆ‘ä»¬å½“å‰ç²¾åº¦å¯ä»¥è¡¨ç¤ºçš„æ•°æ®èŒƒå›´ã€‚</p>

<p>ä¸‹é¢æˆ‘ä»¬é€šè¿‡æ–¹å·®çš„å…¬å¼æ¨å¯¼æ¥è§‚å¯Ÿä¸ºä»€ä¹ˆç½‘ç»œå±‚è¾“å‡ºçš„æ ‡å‡†å·®ä¼šè¶Šæ¥è¶Šå¤§ï¼Œæœ€ç»ˆè¶…å‡ºå¯è¡¨ç¤ºçš„èŒƒå›´ã€‚å‡è®¾ $X$ å’Œ $Y$ æ˜¯ä¸¤ä¸ªç›¸äº’ç‹¬ç«‹çš„éšæœºå˜é‡ï¼Œæˆ‘ä»¬çŸ¥é“ï¼š</p>

\[\begin{aligned}
\mathrm{E}(X*Y) &amp;= \mathrm{E}(X) * \mathrm{E}(Y) \\[2ex]
\mathrm{Var}(X) &amp;= \mathrm{E}(X^2) - [\mathrm{E}(X)]^2 \\[2ex]
\mathrm{Var}(X+Y) &amp;= \mathrm{Var}(X) + \mathrm{Var}(Y)
\end{aligned}\]

<p>ç„¶åï¼Œæˆ‘ä»¬æœ‰ï¼š</p>

\[\mathrm{Var}(X*Y) = \mathrm{Var}(X) * \mathrm{Var}(Y) + \mathrm{Var}(X) * [\mathrm{E}(Y)]^2 + \mathrm{Var}(Y) * [\mathrm{E}(X)]^2\]

<p>å¦‚æœ $\mathrm{E}(X) =0,\;\mathrm{E}(Y) =0$ï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰ï¼š</p>

\[\mathrm{Var}(X*Y) = \mathrm{Var}(X) * \mathrm{Var}(Y)\]

<p>ä¸‹é¢æˆ‘ä»¬æ¥è®¡ç®—ç½‘ç»œå±‚ç¥ç»å…ƒçš„æ ‡å‡†å·®ï¼š</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-19-WX20201219-111822%402x.png" width="50%" /></p>

\[H_{11} = \sum_{i=0}^{n}X_i * W_{1i}\]

<p>ç”±äº $X$ å’Œ $W$ éƒ½æ˜¯å‡å€¼ä¸º $0$ï¼Œæ ‡å‡†å·®ä¸º $1$ï¼Œæˆ‘ä»¬æœ‰ï¼š</p>

\[\begin{aligned}
\mathrm{Var}(H_{11}) &amp;= \sum_{i=0}^{n} \mathrm{Var}(X_{i}) * \mathrm{Var}(W_{1i}) \\
&amp;= n * (1 * 1) \\
&amp;= n
\end{aligned}\]

<p>æ‰€ä»¥ï¼Œ</p>

\[\mathrm{Std}(H_{11}) = \sqrt{\mathrm{Var}(H_{11})} = \sqrt{n}\]

<p>å¯ä»¥çœ‹åˆ°ï¼Œç¬¬ä¸€ä¸ªéšè—å±‚ä¸­ç¥ç»å…ƒçš„æ–¹å·®å˜ä¸ºäº† $n$ï¼Œè€Œè¾“å…¥ $X$ çš„æ–¹å·®ä¸º $1$ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»è¿‡ç¬¬ä¸€ä¸ªç½‘ç»œå±‚ $H_1$ çš„å‰å‘ä¼ æ’­ï¼Œæ•°æ®çš„æ–¹å·®æ‰©å¤§äº† $n$ å€ï¼Œæ ‡å‡†å·®æ‰©å¤§äº† $\sqrt{n}$ å€ã€‚åŒç†ï¼Œå¦‚æœç»§ç»­ä¼ æ’­åˆ°ä¸‹ä¸€ä¸ªéšè—å±‚ $H_2$ï¼Œé€šè¿‡å…¬å¼æ¨å¯¼ï¼Œå¯çŸ¥è¯¥å±‚ç¥ç»å…ƒçš„æ ‡å‡†å·®ä¸º $n$ã€‚è¿™æ ·ä¸æ–­ä¼ æ’­ä¸‹å»ï¼Œæ¯ç»è¿‡ä¸€å±‚ï¼Œè¾“å‡ºæ•°æ®çš„å°ºåº¦èŒƒå›´éƒ½å°†ä¸æ–­æ‰©å¤§ $\sqrt{n}$ å€ï¼Œæœ€ç»ˆå°†è¶…å‡ºæˆ‘ä»¬çš„ç²¾åº¦å¯è¡¨ç¤ºçš„èŒƒå›´ï¼Œå˜ä¸º <code class="language-plaintext highlighter-rouge">nan</code>ã€‚</p>

<p>åœ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®çš„æ¯å±‚ç½‘ç»œä¸­ç¥ç»å…ƒä¸ªæ•° $n=256$ï¼Œæ‰€ä»¥ $\sqrt{n} = 16$ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å‰é¢è¾“å‡ºç»“æœä¸­çš„æ¯ä¸ªç½‘ç»œå±‚è¾“å‡ºçš„æ ‡å‡†å·®æ˜¯å¦ç¬¦åˆè¿™ä¸€è§„å¾‹ï¼š</p>

<ul>
  <li>ç¬¬ 0 å±‚æ•°æ®æ ‡å‡†å·®ä¸º $15.959932327270508 \approx 16$</li>
  <li>ç¬¬ 1 å±‚æ•°æ®æ ‡å‡†å·®ä¸º $256.6237487792969 \approx 16^2=256$</li>
  <li>ç¬¬ 2 å±‚æ•°æ®æ ‡å‡†å·®ä¸º $4107.24560546875 \approx 16^3 = 4096$</li>
  <li>ç¬¬ 3 å±‚æ•°æ®æ ‡å‡†å·®ä¸º $65576.8125 \approx 16^4 = 65536$</li>
  <li>â€¦â€¦</li>
</ul>

<p>æ¯ç»è¿‡ä¸€å±‚ï¼Œæ•°æ®çš„æ ‡å‡†å·®éƒ½ä¼šæ‰©å¤§ $16$ å€ï¼Œç»è¿‡ä¸€å±‚å±‚ä¼ æ’­åï¼Œæ•°æ®çš„æ ‡å‡†å·®å°†å˜å¾—éå¸¸å¤§ï¼Œæœ€ç»ˆåœ¨ç¬¬ 31 å±‚æ—¶è¶…å‡ºäº†ç²¾åº¦å¯è¡¨ç¤ºçš„èŒƒå›´ï¼Œå³ä¸º <code class="language-plaintext highlighter-rouge">nan</code>ã€‚</p>

<p>ä¸‹é¢æˆ‘ä»¬å°†æ¯å±‚ç¥ç»å…ƒä¸ªæ•°ä¿®æ”¹ä¸º $n=400$ï¼Œæ‰€ä»¥ $\sqrt{n}=20$ï¼Œè§‚å¯Ÿç»“æœæ˜¯å¦ä¼šå‘ç”Ÿç›¸åº”çš„å˜åŒ–ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="n">layer_nums</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># ç½‘ç»œå±‚æ•°
</span><span class="n">neural_nums</span> <span class="o">=</span> <span class="mi">400</span>  <span class="c1"># æ¯å±‚çš„ç¥ç»å…ƒä¸ªæ•°
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># è¾“å…¥æ•°æ®çš„ batch size
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>è¾“å‡ºç»“æœï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="rouge-code"><pre>layer:0, std:20.191545486450195
layer:1, std:406.2967834472656
layer:2, std:8196.0322265625
layer:3, std:164936.546875
layer:4, std:3324399.75
layer:5, std:65078964.0
layer:6, std:1294259712.0
layer:7, std:25718734848.0
layer:8, std:509478502400.0
layer:9, std:10142528569344.0
layer:10, std:204187744862208.0
layer:11, std:4146330289045504.0
layer:12, std:8.175371463688192e+16
layer:13, std:1.6178185228915835e+18
layer:14, std:3.201268126493075e+19
layer:15, std:6.43244420071468e+20
layer:16, std:1.2768073112864894e+22
layer:17, std:2.5327442663597998e+23
layer:18, std:4.97064812888673e+24
layer:19, std:9.969679340542473e+25
layer:20, std:1.9616922876332235e+27
layer:21, std:3.926491184057203e+28
layer:22, std:7.928349353787082e+29
layer:23, std:1.5731294716685355e+31
layer:24, std:3.156214979388958e+32
layer:25, std:6.18353463606124e+33
layer:26, std:1.2453666891690611e+35
layer:27, std:2.467429285844339e+36
layer:28, std:4.977222187097705e+37
layer:29, std:nan
output is nan in 29 layers
tensor([[-inf, inf, inf,  ..., -inf, nan, nan],
        [nan, nan, inf,  ..., -inf, -inf, nan],
        [nan, -inf, nan,  ..., inf, nan, nan],
        ...,
        [nan, -inf, -inf,  ..., -inf, nan, nan],
        [inf, -inf, nan,  ..., inf, -inf, nan],
        [inf, nan, inf,  ..., inf, nan, inf]], grad_fn=&lt;MmBackward&gt;)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>å¯ä»¥çœ‹åˆ°ï¼š</p>

<ul>
  <li>ç¬¬ 0 å±‚æ•°æ®æ ‡å‡†å·®ä¸º $20.191545486450195 \approx 20$</li>
  <li>ç¬¬ 1 å±‚æ•°æ®æ ‡å‡†å·®ä¸º $406.2967834472656 \approx 20^2=400$</li>
  <li>ç¬¬ 2 å±‚æ•°æ®æ ‡å‡†å·®ä¸º $8196.0322265625 \approx 20^3 = 8000$</li>
  <li>ç¬¬ 3 å±‚æ•°æ®æ ‡å‡†å·®ä¸º $164936.546875 \approx 20^4 = 160000$</li>
  <li>â€¦â€¦</li>
</ul>

<p>æ¯ç»è¿‡ä¸€å±‚ï¼Œæ•°æ®çš„æ ‡å‡†å·®å¤§çº¦ä¼šæ‰©å¤§ $20$ å€ï¼Œæœ€ç»ˆåœ¨ç¬¬ 29 å±‚æ—¶è¶…å‡ºäº†ç²¾åº¦å¯è¡¨ç¤ºçš„èŒƒå›´ï¼Œå˜ä¸º <code class="language-plaintext highlighter-rouge">nan</code>ã€‚</p>

<p>ä»å‰é¢çš„å…¬å¼ä¸­å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªç½‘ç»œå±‚è¾“å‡ºæ•°æ®çš„æ ‡å‡†å·®ç”±ä¸‰ä¸ªå› ç´ å†³å®šï¼šç½‘ç»œå±‚çš„ç¥ç»å…ƒä¸ªæ•° $n$ã€è¾“å…¥å€¼ $X$ çš„æ–¹å·® $\mathrm{Var}(X)$ï¼Œä»¥åŠç½‘ç»œå±‚æƒå€¼ $W$ çš„æ–¹å·® $\mathrm{Var}(W)$ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›è®©ç½‘ç»œå±‚è¾“å‡ºæ•°æ®çš„æ–¹å·®ä¿æŒå°ºåº¦ä¸å˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»ä»¤å…¶æ–¹å·®ç­‰äº $1$ï¼Œå³ï¼š</p>

\[\mathrm{Var}(H_{1}) = n * \mathrm{Var}(X) * \mathrm{Var}(W) = 1\]

<p>å› æ­¤ï¼Œ</p>

\[\mathrm{Var}(W) = \dfrac{1}{n} \quad \Longrightarrow \quad \mathrm{Std}(W) = \sqrt{\dfrac{1}{n}}\]

<p>æ‰€ä»¥ï¼Œå½“æˆ‘ä»¬å°†ç½‘ç»œå±‚æƒå€¼çš„æ ‡å‡†å·®è®¾ä¸º $\sqrt{\frac{1}{n}}$ æ—¶ï¼Œè¾“å‡ºæ•°æ®çš„æ ‡å‡†å·®å°†å˜ä¸º $1$ã€‚</p>

<p>ä¸‹é¢æˆ‘ä»¬ä¿®æ”¹ä»£ç ï¼Œä½¿ç”¨ä¸€ä¸ªå‡å€¼ä¸º $0$ï¼Œæ ‡å‡†å·®ä¸º $\sqrt{\frac{1}{n}}$ çš„åˆ†å¸ƒæ¥åˆå§‹åŒ–æƒå€¼çŸ©é˜µ $W$ï¼Œè§‚å¯Ÿç½‘ç»œå±‚è¾“å‡ºæ•°æ®çš„æ ‡å‡†å·®ä¼šå¦‚ä½•å˜åŒ–ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre> <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="bp">self</span><span class="p">.</span><span class="n">neural_num</span><span class="p">))</span>    <span class="c1"># normal: mean=0, std=sqrt(1/n)
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>è¾“å‡ºç»“æœï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
</pre></td><td class="rouge-code"><pre>layer:0, std:0.9974957704544067
layer:1, std:1.0024365186691284
layer:2, std:1.002745509147644
layer:3, std:1.0006227493286133
layer:4, std:0.9966009855270386
layer:5, std:1.019859790802002
layer:6, std:1.026173710823059
layer:7, std:1.0250457525253296
layer:8, std:1.0378952026367188
layer:9, std:1.0441951751708984
layer:10, std:1.0181655883789062
layer:11, std:1.0074602365493774
layer:12, std:0.9948930144309998
layer:13, std:0.9987586140632629
layer:14, std:0.9981392025947571
layer:15, std:1.0045733451843262
layer:16, std:1.0055204629898071
layer:17, std:1.0122840404510498
layer:18, std:1.0076017379760742
layer:19, std:1.000280737876892
layer:20, std:0.9943006038665771
layer:21, std:1.012800931930542
layer:22, std:1.012657642364502
layer:23, std:1.018149971961975
layer:24, std:0.9776086211204529
layer:25, std:0.9592394828796387
layer:26, std:0.9317858815193176
layer:27, std:0.9534041881561279
layer:28, std:0.9811319708824158
layer:29, std:0.9953019022941589
layer:30, std:0.9773916006088257
layer:31, std:0.9655940532684326
layer:32, std:0.9270440936088562
layer:33, std:0.9329946637153625
layer:34, std:0.9311841726303101
layer:35, std:0.9354336261749268
layer:36, std:0.9492132067680359
layer:37, std:0.9679954648017883
layer:38, std:0.9849981665611267
layer:39, std:0.9982335567474365
layer:40, std:0.9616852402687073
layer:41, std:0.9439758658409119
layer:42, std:0.9631161093711853
layer:43, std:0.958673894405365
layer:44, std:0.9675614237785339
layer:45, std:0.9837557077407837
layer:46, std:0.9867278337478638
layer:47, std:0.9920817017555237
layer:48, std:0.9650403261184692
layer:49, std:0.9991624355316162
layer:50, std:0.9946174025535583
layer:51, std:0.9662044048309326
layer:52, std:0.9827387928962708
layer:53, std:0.9887880086898804
layer:54, std:0.9932605624198914
layer:55, std:1.0237400531768799
layer:56, std:0.9702046513557434
layer:57, std:1.0045380592346191
layer:58, std:0.9943899512290955
layer:59, std:0.9900636076927185
layer:60, std:0.99446702003479
layer:61, std:0.9768352508544922
layer:62, std:0.9797843098640442
layer:63, std:0.9951220750808716
layer:64, std:0.9980446696281433
layer:65, std:1.0086933374404907
layer:66, std:1.0276142358779907
layer:67, std:1.0429234504699707
layer:68, std:1.0197855234146118
layer:69, std:1.0319130420684814
layer:70, std:1.0540012121200562
layer:71, std:1.026781439781189
layer:72, std:1.0331352949142456
layer:73, std:1.0666675567626953
layer:74, std:1.0413838624954224
layer:75, std:1.0733673572540283
layer:76, std:1.0404183864593506
layer:77, std:1.0344083309173584
layer:78, std:1.0022705793380737
layer:79, std:0.99835205078125
layer:80, std:0.9732587337493896
layer:81, std:0.9777462482452393
layer:82, std:0.9753198623657227
layer:83, std:0.9938382506370544
layer:84, std:0.9472599029541016
layer:85, std:0.9511011242866516
layer:86, std:0.9737769961357117
layer:87, std:1.005651831626892
layer:88, std:1.0043526887893677
layer:89, std:0.9889539480209351
layer:90, std:1.0130352973937988
layer:91, std:1.0030947923660278
layer:92, std:0.9993206262588501
layer:93, std:1.0342745780944824
layer:94, std:1.031973123550415
layer:95, std:1.0413124561309814
layer:96, std:1.0817031860351562
layer:97, std:1.128799557685852
layer:98, std:1.1617802381515503
layer:99, std:1.2215303182601929
tensor([[-1.0696, -1.1373,  0.5047,  ..., -0.4766,  1.5904, -0.1076],
        [ 0.4572,  1.6211,  1.9659,  ..., -0.3558, -1.1235,  0.0979],
        [ 0.3908, -0.9998, -0.8680,  ..., -2.4161,  0.5035,  0.2814],
        ...,
        [ 0.1876,  0.7971, -0.5918,  ...,  0.5395, -0.8932,  0.1211],
        [-0.0102, -1.5027, -2.6860,  ...,  0.6954, -0.1858, -0.8027],
        [-0.5871, -1.3739, -2.9027,  ...,  1.6734,  0.5094, -0.9986]],
       grad_fn=&lt;MmBackward&gt;)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>å¯ä»¥çœ‹åˆ°ï¼Œç¬¬ 99 å±‚çš„è¾“å‡ºå€¼éƒ½åœ¨ä¸€ä¸ªæ¯”è¾ƒæ­£å¸¸çš„èŒƒå›´ï¼Œå¹¶ä¸”æ¯ä¸€å±‚è¾“å‡ºæ•°æ®çš„æ ‡å‡†å·®éƒ½åœ¨ $1$ é™„è¿‘ï¼Œæ‰€ä»¥ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªæ¯”è¾ƒç†æƒ³çš„è¾“å‡ºæ•°æ®åˆ†å¸ƒã€‚ä»£ç å®éªŒçš„ç»“æœä¹ŸéªŒè¯äº†æˆ‘ä»¬å‰é¢å…¬å¼æ¨å¯¼çš„æ­£ç¡®æ€§ï¼šé€šè¿‡é‡‡ç”¨åˆé€‚çš„æƒå€¼åˆå§‹åŒ–æ–¹æ³•ï¼Œå¯ä»¥ä½¿å¾—å¤šå±‚å…¨è¿æ¥ç½‘ç»œçš„è¾“å‡ºå€¼çš„æ•°æ®å°ºåº¦ç»´æŒåœ¨ä¸€å®šèŒƒå›´å†…ï¼Œè€Œä¸ä¼šå˜å¾—è¿‡å¤§æˆ–è€…è¿‡å°ã€‚</p>

<p>åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æƒé‡åˆå§‹åŒ–ä¿è¯äº†æ¯å±‚è¾“å‡ºæ•°æ®çš„æ–¹å·®ä¸º $1$ï¼Œä½†æ˜¯è¿™é‡Œæˆ‘ä»¬è¿˜æ²¡æœ‰è€ƒè™‘æ¿€æ´»å‡½æ•°çš„å­˜åœ¨ã€‚ä¸‹é¢æˆ‘ä»¬çœ‹ä¸€ä¸‹ <strong>å…·æœ‰æ¿€æ´»å‡½æ•°æ—¶çš„æƒå€¼åˆå§‹åŒ–</strong>ã€‚æˆ‘ä»¬åœ¨å‰å‘ä¼ æ’­ <code class="language-plaintext highlighter-rouge">forward</code> å‡½æ•°ä¸­åŠ å…¥ tanh æ¿€æ´»å‡½æ•°ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">linear</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linears</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="s">"layer:{}, std:{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">std</span><span class="p">()))</span>  <span class="c1"># æ‰“å°æ¯å±‚çš„æ ‡å‡†å·®
</span>        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">std</span><span class="p">()):</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"output is nan in {} layers"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>è¾“å‡ºç»“æœï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
</pre></td><td class="rouge-code"><pre>layer:0, std:0.6273701786994934
layer:1, std:0.48910173773765564
layer:2, std:0.4099564850330353
layer:3, std:0.35637012124061584
layer:4, std:0.32117360830307007
layer:5, std:0.2981105148792267
layer:6, std:0.27730831503868103
layer:7, std:0.2589356303215027
layer:8, std:0.2468511462211609
layer:9, std:0.23721906542778015
layer:10, std:0.22171513736248016
layer:11, std:0.21079954504966736
layer:12, std:0.19820132851600647
layer:13, std:0.19069305062294006
layer:14, std:0.18555502593517303
layer:15, std:0.17953835427761078
layer:16, std:0.17485806345939636
layer:17, std:0.1702701896429062
layer:18, std:0.16508983075618744
layer:19, std:0.1591130942106247
layer:20, std:0.15480300784111023
layer:21, std:0.15263864398002625
layer:22, std:0.148549422621727
layer:23, std:0.14617665112018585
layer:24, std:0.13876432180404663
layer:25, std:0.13316625356674194
layer:26, std:0.12660598754882812
layer:27, std:0.12537942826747894
layer:28, std:0.12535445392131805
layer:29, std:0.12589804828166962
layer:30, std:0.11994210630655289
layer:31, std:0.11700887233018875
layer:32, std:0.11137297749519348
layer:33, std:0.11154612898826599
layer:34, std:0.10991233587265015
layer:35, std:0.10996390879154205
layer:36, std:0.10969001054763794
layer:37, std:0.10975216329097748
layer:38, std:0.11063200235366821
layer:39, std:0.11021336913108826
layer:40, std:0.10465587675571442
layer:41, std:0.10141163319349289
layer:42, std:0.1026025265455246
layer:43, std:0.10079070925712585
layer:44, std:0.10096712410449982
layer:45, std:0.10117629915475845
layer:46, std:0.10145658999681473
layer:47, std:0.09987485408782959
layer:48, std:0.09677786380052567
layer:49, std:0.099615179002285
layer:50, std:0.09867013245820999
layer:51, std:0.09398546814918518
layer:52, std:0.09388342499732971
layer:53, std:0.09352942556142807
layer:54, std:0.09336657077074051
layer:55, std:0.0948176234960556
layer:56, std:0.08856320381164551
layer:57, std:0.09024856984615326
layer:58, std:0.088644839823246
layer:59, std:0.08766943216323853
layer:60, std:0.08726289123296738
layer:61, std:0.08623495697975159
layer:62, std:0.08549778908491135
layer:63, std:0.0855521708726883
layer:64, std:0.0853666365146637
layer:65, std:0.08462794870138168
layer:66, std:0.0852193832397461
layer:67, std:0.08562126755714417
layer:68, std:0.08368431031703949
layer:69, std:0.08476374298334122
layer:70, std:0.0853630006313324
layer:71, std:0.08237560093402863
layer:72, std:0.08133518695831299
layer:73, std:0.08416958898305893
layer:74, std:0.08226992189884186
layer:75, std:0.08379074186086655
layer:76, std:0.08003697544336319
layer:77, std:0.07888862490653992
layer:78, std:0.07618380337953568
layer:79, std:0.07458437979221344
layer:80, std:0.07207276672124863
layer:81, std:0.07079190015792847
layer:82, std:0.0712786465883255
layer:83, std:0.07165777683258057
layer:84, std:0.06893909722566605
layer:85, std:0.0690247192978859
layer:86, std:0.07030878216028214
layer:87, std:0.07283661514520645
layer:88, std:0.07280214875936508
layer:89, std:0.07130246609449387
layer:90, std:0.07225215435028076
layer:91, std:0.0712454542517662
layer:92, std:0.07088854163885117
layer:93, std:0.0730612576007843
layer:94, std:0.07276967912912369
layer:95, std:0.07259567081928253
layer:96, std:0.07586522400379181
layer:97, std:0.07769150286912918
layer:98, std:0.07842090725898743
layer:99, std:0.08206238597631454
tensor([[-0.1103, -0.0739,  0.1278,  ..., -0.0508,  0.1544, -0.0107],
        [ 0.0807,  0.1208,  0.0030,  ..., -0.0385, -0.1887, -0.0294],
        [ 0.0321, -0.0833, -0.1482,  ..., -0.1133,  0.0206,  0.0155],
        ...,
        [ 0.0108,  0.0560, -0.1099,  ...,  0.0459, -0.0961, -0.0124],
        [ 0.0398, -0.0874, -0.2312,  ...,  0.0294, -0.0562, -0.0556],
        [-0.0234, -0.0297, -0.1155,  ...,  0.1143,  0.0083, -0.0675]],
       grad_fn=&lt;TanhBackward&gt;)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>å¯ä»¥çœ‹åˆ°ï¼Œéšç€ç½‘ç»œå±‚çš„å‰å‘ä¼ æ’­ï¼Œæ¯å±‚è¾“å‡ºå€¼çš„æ ‡å‡†å·®è¶Šæ¥è¶Šå°ï¼Œæœ€ç»ˆå¯èƒ½ä¼šå¯¼è‡´ <strong>æ¢¯åº¦æ¶ˆå¤±</strong>ï¼Œè¿™å¹¶ä¸æ˜¯æˆ‘ä»¬æ‰€å¸Œæœ›çœ‹åˆ°çš„ã€‚</p>

<h2 id="2-xavier-åˆå§‹åŒ–">2. Xavier åˆå§‹åŒ–</h2>

<p><strong>å‚è€ƒæ–‡çŒ®</strong>ï¼š<em><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?source=post_page---------------------------">Understanding the difficulty of training deep feedforward neural networks</a></em></p>

<p>é’ˆå¯¹ä¸Šé¢å…·æœ‰æ¿€æ´»å‡½æ•°æƒ…å†µçš„é—®é¢˜ï¼Œ2010 å¹´ Xavier åœ¨ä¸€ç¯‡è®ºæ–‡ä¸­è¯¦ç»†æ¢è®¨äº†åœ¨å…·æœ‰æ¿€æ´»å‡½æ•°çš„æƒ…å†µä¸‹åº”è¯¥å¦‚ä½•åˆå§‹åŒ–çš„é—®é¢˜ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œç»“åˆæ–¹å·®ä¸€è‡´æ€§åŸåˆ™ (å³è®©æ¯å±‚ç½‘ç»œè¾“å‡ºå€¼çš„æ–¹å·®å°½é‡åœ¨ $1$ é™„è¿‘)ï¼ŒåŒæ—¶ä½œè€…å¯¹ Sigmoidã€tanh è¿™ç±»é¥±å’Œæ¿€æ´»å‡½æ•°è¿›è¡Œåˆ†æã€‚</p>

<p><strong>æ–¹å·®ä¸€è‡´æ€§</strong>ï¼šä¿æŒæ•°æ®å°ºåº¦ç»´æŒåœ¨æ°å½“èŒƒå›´ï¼Œé€šå¸¸æ–¹å·®ä¸º $1$ã€‚</p>

<p><strong>æ¿€æ´»å‡½æ•°</strong>ï¼šé¥±å’Œå‡½æ•°ï¼Œå¦‚ Sigmoidã€Tanhã€‚</p>

<p>é€šè¿‡è®ºæ–‡ä¸­çš„å…¬å¼æ¨å¯¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä»¥ä¸‹ä¸¤ä¸ªç­‰å¼ï¼š</p>

\[n_i * \mathrm{Var}(W) =1\]

\[n_{i+1} * \mathrm{Var}(W) =1\]

<p>å…¶ä¸­ï¼Œ$n_i$ æ˜¯è¾“å…¥å±‚çš„ç¥ç»å…ƒä¸ªæ•°ï¼Œ$n_{i+1}$ æ˜¯è¾“å‡ºå±‚çš„ç¥ç»å…ƒä¸ªæ•°ã€‚å³æˆ‘ä»¬åŒæ—¶è€ƒè™‘äº†å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„æ•°æ®å°ºåº¦é—®é¢˜ã€‚</p>

<p>åŒæ—¶ï¼Œç»“åˆæ–¹å·®ä¸€è‡´æ€§åŸåˆ™ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æƒå€¼ $W$ çš„æ–¹å·®ä¸ºï¼š</p>

\[\mathrm{Var}(W) = \dfrac{2}{n_i + n_{i+1}}\]

<p>é€šå¸¸ï¼ŒXavier é‡‡ç”¨çš„æ˜¯å‡åŒ€åˆ†å¸ƒã€‚ä¸‹é¢æˆ‘ä»¬æ¥æ¨å¯¼å‡åŒ€åˆ†å¸ƒæ˜¯ä¸Šé™å’Œä¸‹é™ï¼Œè¿™é‡Œæˆ‘ä»¬å‡è®¾ä¸Šé™æ˜¯ $a$ï¼Œé‚£ä¹ˆä¸‹é™ä¸º $-a$ï¼Œå› ä¸ºæˆ‘ä»¬é€šå¸¸é‡‡ç”¨çš„æ˜¯é›¶å‡å€¼ï¼Œæ‰€ä»¥ä¸Šä¸‹é™ä¹‹é—´æ˜¯å¯¹ç§°å…³ç³»ã€‚</p>

\[W \sim U[-a, a]\]

<p>æ ¹æ®å‡åŒ€åˆ†å¸ƒçš„æ–¹å·®å…¬å¼ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š</p>

\[\mathrm{Var}(W) = \dfrac{(-a-a)^2}{12} = \dfrac{(2a)^2}{12} = \dfrac{a^2}{3}\]

<p>ç„¶åï¼Œæˆ‘ä»¬æœ‰ï¼š</p>

\[\dfrac{2}{n_i + n_{i+1}} = \dfrac{a^2}{3} \quad \Longrightarrow \quad a = \dfrac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}}\]

<p>æ‰€ä»¥ï¼Œ</p>

\[W \sim U\left[- \dfrac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}}, \dfrac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}} \right]\]

<p><strong>ä»£ç ç¤ºä¾‹</strong>ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">6</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">neural_num</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">neural_num</span><span class="p">))</span>

                <span class="n">tanh_gain</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s">'tanh'</span><span class="p">)</span>
                <span class="n">a</span> <span class="o">*=</span> <span class="n">tanh_gain</span>

                <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="o">-</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<p>ä¸‹èŠ‚å†…å®¹ï¼šæƒå€¼åˆå§‹åŒ–</p>
:ET