I"Ğ<h1 id="lecture-02-å‡ ä½•è§‚ç‚¹å’Œé«˜ç»´æ•°æ®">Lecture 02 å‡ ä½•è§‚ç‚¹å’Œé«˜ç»´æ•°æ®</h1>

<p><strong>å‚è€ƒæ•™æ</strong>ï¼š</p>

<ul>
  <li><em>Hardle, W. and Simar, L (2015). Applied multivariate statistical analysis, 4th edition.</em></li>
  <li><em>Hastie, T. Tibshirani, R. and Friedman, J. (2009). The elements of statistical learning, 2nd edition</em></li>
</ul>

<h2 id="1-å‡ ä½•è§‚ç‚¹">1. å‡ ä½•è§‚ç‚¹</h2>
<h3 id="11-è·ç¦»">1.1 è·ç¦»</h3>

<ul>
  <li>
    <p>ä¸¤ä¸ªå‘é‡ $x,y\in \mathbb R^p$ ä¹‹é—´çš„ <strong>æ¬§å‡ é‡Œå¾—è·ç¦» (Euclidian distance)</strong> $d(x,y)$ è¢«å®šä¹‰ä¸ºï¼š</p>

    <script type="math/tex; mode=display">d(x,y)=\sqrt{\sum_{i=1}^{p}(x_i-y_i)^2}=\sqrt{(x-y)^{\mathrm T}(x-y)}</script>

    <p>äºŒç»´ç©ºé—´ $\mathbb R^2$ ä¸­çš„ä¾‹å­ï¼Œå…¶ä¸­ $x=(x_1,x_2)$ï¼Œ$y=(y_1,y_2)$ï¼š</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-25-WX20200825-142548%402x.png" width="50%" /></p>

    <p><span><center> <span style="font-size:10pt"> <span style="color:steelblue;font-weight:bold">å›¾ 1</span>ï¼šè·ç¦» $d$ </span></center></span></p>

    <p><br /></p>
  </li>
  <li>
    <p>è¯¥è·ç¦»çš„ä¸€ä¸ª <strong>åŠ æƒç‰ˆæœ¬</strong> å¯ä»¥è¢«å®šä¹‰ä¸ºï¼š</p>

    <script type="math/tex; mode=display">d(x,y)=\sqrt{\sum_{i=1}^{p}w_i(x_i-y_i)^2}=\sqrt{(x-y)^{\mathrm T}\mathcal W(x-y)}</script>

    <p>å…¶ä¸­ï¼Œæ¯ä¸ª $w_i&gt;0$ï¼Œå¹¶ä¸” $\mathcal W=\mathrm{diag}(w_1,\dots,w_p)$ã€‚</p>

    <p><br /></p>
  </li>
  <li>
    <p>è¿™å¯ä»¥è¢«è¿›ä¸€æ­¥æ¨å¹¿åˆ°ä¸‹é¢çš„è·ç¦»ï¼š</p>

    <script type="math/tex; mode=display">d(x,y)=\sqrt{(x-y)^{\mathrm T}\mathcal A(x-y)}</script>

    <p>å…¶ä¸­ï¼Œ$\mathcal A$ æ˜¯ä¸€ä¸ª <strong>æ­£å®š</strong> çŸ©é˜µã€‚</p>
  </li>
</ul>

<h3 id="12-èŒƒæ•°">1.2 èŒƒæ•°</h3>

<ul>
  <li>
    <p>ä¸€ä¸ªå‘é‡ $x\in \mathbb R^p$ çš„ (æ¬§å‡ é‡Œå¾—) <strong>èŒƒæ•°</strong> è¢«å®šä¹‰ä¸ºï¼š</p>

    <script type="math/tex; mode=display">|</script>
  </li>
</ul>

<p>ä¸‹èŠ‚å†…å®¹ï¼šå‡ ä½•è§‚ç‚¹å’Œé«˜ç»´æ•°æ®</p>
:ET