I"<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-10-更高效的强化学习回报设计和-q-函数-逼近">Lecture 10 更高效的强化学习：回报设计和 $Q$-函数 逼近</h1>

<p><strong>主要内容：</strong></p>
<ol>
  <li>动机</li>
  <li>$Q$-函数 逼近</li>
  <li>回报设计和初始化 $Q$-函数</li>
  <li>总结</li>
</ol>

<h2 id="1-动机">1. 动机</h2>
<h3 id="11-学习成果">1.1 学习成果</h3>
<ol>
  <li>给定一些已知特征，手动应用线性 $Q$-函数 逼近来求解小规模 MDP 问题。</li>
  <li>选择合适的特征，设计并实现 $Q$-函数逼近，以实现无模型强化学习技术，从而自动求解中等规模的 MDP 问题。</li>
  <li>比较各种函数逼近方法的优缺点。</li>
  <li>比较和对比线性函数逼近与基于深度神经网络的函数逼近。</li>
  <li>说明如何使用回报设计来帮助无模型强化学习方法达到收敛。</li>
  <li>将回报设计手动应用于给定的潜在函数，以解决小规模 MDP 问题。</li>
  <li>设计并实现潜在函数，以自动解决中等规模的 MDP 问题。</li>
  <li>对比和比较回报设计与 $Q$-函数 初始化。</li>
</ol>

<h3 id="12-强化学习的一些弱项">1.2 强化学习的一些弱项</h3>
<p>在之前的讲座中，我们了解了强化学习中基本的时序差分（TD）方法：$Q$-学习 和 SARSA。如前所述，这两种方法在以下一些基本方面存在一些缺点：</p>

<ul>
  <li>不同于蒙特卡洛方法中在获得回报后将该回报进行反向传播，TD 方法使用 bootstrapping（它们使用 $Q(s,a)$ 来估计未来折扣回报），这意味着对于回报稀疏问题，它可能需要很长时间才能将回报传播到整个 $Q$-函数。</li>
  <li>两种方法都估计了一个 $Q$-函数 $Q(s,a)$，而对此最简单的建模方法是通过 $Q$-表。但是，这需要我们维护一个大小为 $|A|\times |S|$ 的表，而对于任何非平凡的问题来说，这样一张表都显得过于庞大了。</li>
</ul>

<p>使用Q表需要我们多次访问每个可达状态，并多次应用每个动作以获得对Q（s，a）的良好估计。因此，即使我们从未访问过状态s，即使我们访问过与s非常相似的状态，也无法估计Q（s，a）。</p>

<p>奖励可能很少，这意味着很少有状态/动作会导致非零奖励。这是有问题的，因为起初，强化学习算法的行为完全是随机的，并且很难找到好的回报。还记得上一讲的Freeway演示吗？</p>

<p>下节内容：更高效的强化学习：回报设计和 $Q$-函数 逼近</p>

:ET