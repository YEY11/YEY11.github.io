I")<h1 id="lecture-07-非线性模型">Lecture 07 非线性模型</h1>

<p><strong>参考教材</strong>：</p>

<ul>
  <li><em>Gareth, J., Daniela, W., Trevor, H., &amp; Robert, T. (2013). An intruduction to statistical learning: with applications in R. Spinger.</em></li>
  <li><em>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Spinger Science &amp; Business Media.</em></li>
</ul>

<h2 id="1-引言">1. 引言</h2>

<p>目前为止，我们讨论的内容大多集中于线性模型。相比于其他模型而言，线性模型更易于描述、实现简单、解释性和推断理论都相对成熟。然而，我们也不能回避标准线性回归模型在预测上明显不足的问题。这是因为模型的线性假设通常只是对真实函数的一种近似，有时这种近似效果并不理想。</p>

<p>上节课中，我们介绍了一些用于提升线性回归模型预测效果的模型，例如：基于最小二乘的岭回归、Lasso、PCR 和 PLS。这些模型在降低线性模型复杂度的同时也降低了估计的方差。但事实上，线性模型的形式仍未改变。</p>

<p>本节课中，我们将介绍一些非线性模型，它们在保证良好的可解释性的前提下，通过放松线性假设对原始线性模进行简单推广，例如：</p>

<ul>
  <li>
    <p><strong>多项式回归 (Polynomial regression)</strong>：用原始预测变量的幂作为新的预测变量以替代原始变量。例如：一个三次回归模型包含三个预测变量 $X,X^2,X^3$。这是一种简单实用的表达数据非线性关系的模型。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>阶梯函数 (Step functions)</strong>：将某个预测变量的取值空间分割成 $K$ 个不同区域，以此来生成一个新的定性变量，分段拟合一个常量函数。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>回归样条 (Regression spline)</strong>：该方法在形式上比多项式回归和阶梯拟合方法更灵活，实际上回归样条可以视为前两类方法的推广。首先将 $X$ 的取值范围分割成 $K$ 个区域，在每个区域上分别独立拟合一个多项式函数。然而，通常会对这些多项式函数进行一些限制以保证在区域边界或 <strong>结点 (knots)</strong> 处的连接是光滑的。只要将 $X$ 的取值区间划分为足够多的区域，此方法就能够产生灵活度很高的拟合。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>光滑样条 (Smoothing splines)</strong>：与回归样条类似，但是产生机制略有不同，一般是通过最小化一个带光滑惩罚项的残差平方和的式子来得到光滑样条的结果。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>局部回归 (Local regression)</strong>：与样条方法类似，但是存在一个重大差别：局部回归中的区域之间是允许重叠的，并且这种重叠将以一种非常光滑的方式完成。</p>

    <p><br /></p>
  </li>
  <li>
    <p><strong>广义可加模型 (Generalized additive models)</strong>：实际上是将上述模型推广到多个预测变量的情况。</p>
  </li>
</ul>

<p>上述方法都具有很高的灵活性，并且不会丢失线性模型的简单性和可解释性。</p>

<h2 id="2-多项式回归">2. 多项式回归</h2>

<p>为了体现响应变量和预测变量之间的非线性关系，将线性模型推广的最自然的方法是将标准线性模型替换为一个多项式函数：</p>

<script type="math/tex; mode=display">y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \dots + \beta_d x_i^d + \epsilon_i</script>

<p>其中，$\epsilon_i$ 是误差项。这种方法称为 <strong>多项式回归</strong>。对于阶数比较大的 $d$，多项式回归将呈现明显的非线性曲线。</p>

<p>注意到其本质上可视为预测变量 $x_i,x_i^2,x_i^3,\dots,x_i^d$ 的标准线性模型，因此用最小二乘回归的方法就能得到其系数的估计。对多项式阶数 $d$ 的选择不宜过大，一般不大于 $3$ 或者 $4$，这是因为 $d$ 越大，多项式曲线就会变得过于灵活，以至于出现一些奇怪的形状，尤其是在 $X$ 变量的边界附近。</p>

<p>图 1 的左图是 <code class="language-plaintext highlighter-rouge">Wage</code> 数据集中的 <code class="language-plaintext highlighter-rouge">wage</code> 变量关于 <code class="language-plaintext highlighter-rouge">age</code> 变量的散点图，其中包含了居住在美国亚特兰大中部地区男性的收入和人口信息。图中蓝色实线是使用最小二乘法拟合的 $4$ 阶多项式回归的结果。尽管表面上看，这个模型与其他线性回归模型并无明显差异，但每个变量的系数不再是模型关注的重点。相反，通过观察 <code class="language-plaintext highlighter-rouge">age</code> 在 $18$ 岁到 $80$ 岁之间的 $62$ 个观测值的函数拟合结果，可以帮助我们更好地理解 <code class="language-plaintext highlighter-rouge">age</code> 和 <code class="language-plaintext highlighter-rouge">wage</code> 的关系。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-11-09-WX20201110-003200%402x.png" width="80%" /></p>

<p><span style="margin:auto; display:table; font-size:10pt"> <span style="color:steelblue;font-weight:bold">图 1</span>：<code class="language-plaintext highlighter-rouge">Wage</code> 数据集。<strong>左图</strong>：实线表示 <code class="language-plaintext highlighter-rouge">wage</code> (单位：千美元) 关于 <code class="language-plaintext highlighter-rouge">age</code> 的 $4$ 阶多项式模型曲线，用最小二乘法拟合；虚线表示 $95\%$ 置信区间。<strong>右图</strong>：针对二元变量 <code class="language-plaintext highlighter-rouge">wage &gt;250</code> 的逻辑回归模型建模结果，通常采用 $4$ 阶多项式，蓝色实线表示 <code class="language-plaintext highlighter-rouge">wage &gt;250</code> 的后验概率，虚线则是估计的 $95\%$ 置信区间。</span></p>

<p>首先创建一些新的变量 $X_1=X, X_2=X^2,\dots$，然后按照多元线性回归的方式拟合它们。</p>

<p>这里，我们真正关心的并非回归系数，而是在任意一个 $x_0$ 处的拟合函数值：</p>

<script type="math/tex; mode=display">\hat f(x_0) = \hat \beta_0 + \hat \beta_1 x_0+ \hat \beta_2 x_0^2+ \hat \beta_3 x_0^3+ \hat \beta_4 x_0^4</script>

<p>由于 $\hat f(x_0)$ 是一个关于 $\hat \beta_{\ell}$ 的线性函数，我们可以得到一个在任意 $x_0$ 处的 <strong>点方差 (pointwise-variances)</strong> $\mathrm{Var}[\hat f(x_0)]$ 的简单表示。在图 1 中，我们已经计算了 $x_0$ 值组成的网格上的拟合值和 <strong>点标准误 (pointwise standard errors)</strong>。并且画出了出拟合值曲线以及距离拟合值两倍标准误的曲线，即 $\hat f(x_0) \pm 2 \cdot \mathrm{se}[\hat f(x_0)]$。之所以取两倍标准误是因为为对于正态分布的误差项来说，这个值对应的大约是 $95\%$ 置信区间。</p>

<p>通常，我们要么将阶数 $d$ 固定在一个合理的较低值，要么使用交叉验证来选择 $d$。</p>

<p>从图 1 可以看出，工资好像是来自于两个不同的总体：一个总体是年收入高于 $250$ 千美元的高收入组，而另一个则是低收入组。将 <code class="language-plaintext highlighter-rouge">wage</code> 看作一个二元变量就能将数据分成两个组。这样以 <code class="language-plaintext highlighter-rouge">age</code> 的多项式函数作为预测变量的逻辑回归就能用来预测这个二元响应变量。换句话说，实际上需要拟合的是下面这个模型：</p>

<script type="math/tex; mode=display">\Pr(y_i > 250 \mid x_i) = \dfrac{\exp(\beta_0 + \beta_1 x_i + \beta_2 x_i^2 +  \dots + \beta_d x_i^d)}{1 + \exp(\beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \dots + \beta_d x_i^d)}</script>

<p>为了获得其置信区间，只需要计算相关的 $\mathrm{logit}$ 函数的上下限，然后取反即可得到概率的置信区间。</p>

<p>并且，我们可以将多个变量分开处理，只需将变量堆叠到一个矩阵中，然后将进行分区计算即可 (参见后面的 GAM)。</p>

<p><strong>注意</strong>：多项式函数具有臭名昭著的 <strong>尾部行为 (tail behavior)</strong>，这会导致推断非常困难。</p>

<p>在 R 中，我们可以在 <code class="language-plaintext highlighter-rouge">formula</code> 参数中使用 <code class="language-plaintext highlighter-rouge">y ~ poly(x, degree=3)</code> 进行多项式拟合。</p>

<h2 id="3-阶梯函数">3. 阶梯函数</h2>

<p>在线性模型中使用特征变量的多项式形式作为预测变量得到了在 $X$ 取值空间全局皆非线性的拟合函数。如果不希望得到全局的模型，可以使用 <strong>阶梯函数</strong> 拟合。这里，把 $X$ 的取值范围分割成一些区间，每个区间上拟合一个不同的常数。这相当于 <strong>将一个连续变量转换成一个有序的分类变量</strong>。</p>

<p>具体来说，首先在 $X$ 取值空间上创建 $K$ 个分割点 $c_1,c_2,\dots,c_K$，然后构造以下 $K+1$ 个新变量：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
C_0(X) &= I(X < c_1) \\[2ex]
C_1(X) &= I(c_1 \le X < c_2) \\[2ex]
C_2(X) &= I(c_2 \le X < c_3) \\[2ex]
&\; \vdots \\[2ex]
C_{K-1}(X) &= I(c_{K-1} \le X < c_K) \\[2ex]
C_K(X) &= I(c_K \le X)
\end{align} %]]></script>

<p>其中，$I(\cdot)$ 是指示函数，当条件成立时返回 $1$，否则返回 $0$。例如，当 $c_K \le X$ 时，$I(c_K \le X)$ 等于 $1$，否则等于 $0$。这样定义的变量有时候也称为 <strong>虚拟变量</strong>。注意，由于 $X$ 只能落在 $K + 1$ 个区间中的某一个，于是对任意 $X$ 的取值，都有 $C_0(X) + C_1(X) +\cdots + C_K(X) = 1$。然后，我们可以将 $C_1(X),C_2(X),\dots, C_K(X)$ 作为预测变量，使用最小二乘法来拟合一个线性模型：</p>

<script type="math/tex; mode=display">y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i) + \cdots + \beta_K C_K(x_i) +\epsilon_i</script>

<p>对于 $X$ 的一个给定值，$C_1(X),C_2(X),\dots, C_K(X)$ 中至多只有一项系数非零。注意到，当 $X &lt; c_1$ 时，式中的每个预测变量都为零，所以 $\beta_0$ 即为 $X &lt; c_1$ 时响应 $Y$ 的平均值。相应地，</p>

<p>t</p>

<p>1</p>

<p>时提</p>

<p>IY 的平均值。</p>

<p>下节内容：非线性模型</p>
:ET