I"<h1 id="lecture-11-上下文表示">Lecture 11 上下文表示</h1>

<p>这节课我们学习 <strong>上下文表示（Contextual Representation）</strong>，即单词在上下文中的含义。</p>

<h2 id="1-上下文表示">1. 上下文表示</h2>
<h3 id="11-词向量嵌入">1.1 词向量/嵌入</h3>
<p>在之前的章节中，我们已经学习过 <strong>词向量/嵌入（Word Vectors/Embeddings）</strong>，我们还学习了如何通过基于计数的方法来得到词向量。</p>

<ul>
  <li>每个单词 type 都有一个表示。
    <ul>
      <li>Word2Vec</li>
    </ul>
  </li>
  <li>无论单词的上下文是什么，我们得到的单词表示都是相同的。<br />
通过这种方式，无论这些单词在句子中是如何被使用的或者出现在句子中的哪个地方，以及它们的相邻单词是什么，模型学习到的每个单词 type 的词向量/嵌入都只有一种表示。我们称之为 <strong>上下文无关词向量/嵌入（Contextual Independent Word Vectors/Embeddings）</strong>。</li>
  <li>这种上下文无关词向量没有捕获到 <strong>单词的多义性（multiple senses of words）</strong>。<br />
例如：对于单词 “$\textit{duck}$”，其既可以表示鸭子这种动物，也可以表示躲避这一动作。而我们之前的词向量没有办法捕获到这两种含义之间的差异，因为对于同一单词我们只有一种向量表示。</li>
  <li><strong>上下文表示（Contextual representation）$=$ 基于上下文的单词表示</strong><br />
如果一个单词在两个句子中的含义不同，那么我们将得到该单词的两种不同的上下文表示。</li>
  <li>但是，更重要的是，我们发现预训练的上下文表示在大部分下游任务中的表现都 <strong>相当出色</strong>。这种基于上下文的单词表示已经在目前的 NLP 系统中充当着基石的角色。</li>
</ul>

<h3 id="12-rnn-语言模型">1.2 RNN 语言模型</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-17-WX20200617-155248%402x.png" width="80%" /></p>

<p>所以，我们应当如何学习到这种上下文表示呢？</p>

<p>这里，我们有一个 RNN 语言模型：“$\textit{a cow eats grass}$”。这里，RNN 模型试图预测下一个单词：给定单词 “$\textit{a}$”，RNN 模型试图预测下一个单词 “$\textit{cow}$”；给定单词 “$\textit{cow}$”，它试图预测下一个单词 “$\textit{eats}$” 等等。</p>

<p>下面是一个简单的 RNN 语言模型：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-17-WX20200617-160232%402x.png" width="90%" /></p>

<p>模型一共 3 层：输入层是单词的 one-hot 向量；隐藏层作为中间层；输出层用于预测下一个单词。其中，隐藏层的计算公式如右边所示：其接受前一个时间步（time-step）的隐藏状态 $s_{i-1}$，并结合当前输入 $x_i$，然后加上一个偏置项 $b$，然后输入一个非线性激活函数 $\tanh$，然后我们得到当前时间步的隐藏状态 $s_i$；之后，我们将当前时间步的隐藏状态 $s_i$ 输入到一个 $\mathrm{softmax}$ 函数中，得到词汇表中的所有单词的在当前时间步的概率分布。</p>

<p>我们知道，词嵌入对应上面的矩阵 $W_x$，我们可以将隐藏状态 $s_i$ 从某种程度上解释为当前单词的上下文表示。为什么可以这样解释呢？假设当前输入单词为 “$\textit{eats}$”，我们计算出该单词的隐藏状态，该 隐藏状态不仅捕获了单词 “$\textit{eats}$” 的信息，而且还包括之前见过的历史单词：“$\textit{a}$” 和 “$\textit{cow}$”。所以，我们可以将 RNN 语言模型中的隐藏状态从某种程度上视为一种上下文单词表示。</p>

<p>那么，问题解决了吗？</p>

<ul>
  <li>
    <p>几乎解决了，但是还没有完全解决。因为该 RNN 语言模型得到的单词的上下文表示仅仅捕获了该单词左边的上下文。<br />
例如：对于单词 “$\textit{cow}$”，其隐藏状态仅仅捕获了其前面出现过的单词 “$\textit{a}$” 的信息，而没有捕获到其后面出现的单词 “$\textit{eats}$” 的信息。</p>
  </li>
  <li>
    <p>解决方案：使用 <strong>双向 RNN（bidirectional RNN）</strong>模型替代。</p>
  </li>
</ul>

<h3 id="13-双向-rnn">1.3 双向 RNN</h3>

<p>现在，我们来看一下如何利用双向 RNN 模型来捕获当前单词左右两侧的上下文信息，从而得到当前单词的上下文表示。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-06-17-WX20200617-162852%402x.png" width="90%" /></p>

<p>我们有一个简单的 RNN1，和之前一样，我们有句子 “$\textit{a cow eats grass}$”。其中，$s_0, s_1, s_2, s_3$ 表示每个单词 $x_i$ 的前一个时间步的隐藏状态，即 $s_{i-1}$；输出的当前隐藏状态 $s_i$ 捕获了基于之前单词的上下文表示。然后，我们可以简单地添加一个反向 RNN2，从右向左进行，以捕获当前单词右边的上下文单词，同样，我们会得到一个输出的当前隐藏状态 $u_i$。然后，我们可以连接前向和后向两个 RNN 的隐藏状态 $s_i$ 和 $u_i$，从而得到一个同时捕获了当前单词两侧上下文单词信息的词表示。</p>

<p>还是以单词 “$\textit{cow}$” 为例，我们的预测单词为 “$\textit{eats}$”。这里，当前单词 “$\textit{cow}$” 的上下文表示由两部分构成：其中指向预测单词 “$\textit{eats}$” 的蓝色箭头表示隐藏状态 $s_2$ 捕获的左边的上下文单词 “$\textit{a}$” 的信息，而指向预测单词 “$\textit{eats}$” 的红色箭头表示隐藏状态 $u_1$ 捕获的右边的上下文单词 “$\textit{eats}$” 和 “$\textit{grass}$” 的信息。</p>

<p>所以，通过双向 RNN 模型，我们可以得到同时包含当前单词两侧信息的上下文表示，并且，无需另外设计新的模型或者架构。</p>

<h2 id="2-elmo">2. ELMo</h2>

<p>双向 RNN 这种思路也启发了 <strong>ELMo</strong> 模型：它是一种非常流畅自然的单词上下文表示模型，并且在大部分的 NLP 任务中都取得了非常好的效果。</p>

<h3 id="21-elmo基于语言模型的嵌入">2.1 ELMo：基于语言模型的嵌入</h3>

<p><strong>ELMo</strong> 表示</p>

<h2 id="4-扩展阅读">4. 扩展阅读</h2>
<ul>
  <li>JM3, Ch 6</li>
</ul>

<p>下节内容：上下文表示</p>

:ET