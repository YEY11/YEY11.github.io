I"<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-08-nlp-深度学习循环网络">Lecture 08 NLP 深度学习：循环网络</h1>

<p>上节课我们介绍了前馈神经网络，本节课我们将学习循环神经网络。</p>

<h2 id="1-n-gram-语言模型">1. N-gram 语言模型</h2>
<p>让我们再次回到 n-gram 语言模型。</p>
<ul>
  <li>可以基于词频计数（以及平滑技术）实现 n-gram 语言模型。<br />
给定一个语料库，基于其中 bigram/trigram 的频数实现语言模型，并且利用平滑技术处理稀疏性问题和未知单词 gram 的问题。</li>
  <li>可以基于前馈神经网络实现。<br />
假设给定 $(n-1)$ 个上下文单词，任务是预测下一个单词。我们可以将其转换为一个分类问题：输出单词是整个词汇表中的某个单词。和一般的分类问题的不同之处在于，这里可能的输出集合非常大。但是除此之外，它和其他分类问题本质上没有区别，所以我们可以基于前馈神经网络实现它。</li>
</ul>

<p>让我们来看一个使用 trigram 模型生成句子的例子：</p>

<script type="math/tex; mode=display">\textit{I saw a table is round and about}</script>

<p>对于人类而言，上面生成的句子看上去语义不通。但是从 trigram 模型的角度来看，这个句子并没有什么奇怪的，因为如果我们逐个观察 trigram 模型的上下文，会发现每个 trigram 看上去都说得通：</p>

<script type="math/tex; mode=display">\textit{I saw a / saw a table / a table is / table is round / is round and / round and about}</script>

<p>那么，为什么最终我们得到的句子语义不通呢？问题在于：有限的 <strong>上下文</strong>。</p>

<p>当我们试图生成下一个的单词时，只要我们是基于有限的固定大小的上下文单词，那么当生成长句子或者长段落时，最终的句子或者段落意思可能前后语义完全无关。这实际上不是模型本身的问题，而是关于模型假设的问题：即我们可以基于有限的几个单词生成文本。</p>

<h2 id="2-循环神经网络rnn">2. 循环神经网络（RNN）</h2>

<ul>
  <li>RNN 允许我们表示任意大小的输入。</li>
</ul>

<h2 id="4-总结">4. 总结</h2>
<ul>
  <li>神经网络
    <ul>
      <li>鲁棒性（例如：单词变体、拼写错误等）。</li>
      <li>优秀的泛化能力。</li>
      <li>灵活性 —— 基于不同任务定制不同的神经网络架构。</li>
    </ul>
  </li>
  <li>缺点
    <ul>
      <li>训练过程比传统机器学习方法要慢得多，但是可以通过 GPU 加速。</li>
      <li>参数数量很多，主要受词汇表大小、嵌入、网络深度等因素影响。</li>
      <li>对数据量需求很大（data hungry），在小型数据集上表现不是很好。</li>
      <li>在大型语料库上的预训练模型（例如：BERT）可以缓解数据饥饿问题。</li>
    </ul>
  </li>
</ul>

<h2 id="5-扩展阅读">5. 扩展阅读</h2>
<ul>
  <li>Feed-forward network: G15, section 4</li>
  <li>Convolutional network: G15, section 9</li>
</ul>

<p>下节内容：NLP 深度学习：循环网络</p>

:ET