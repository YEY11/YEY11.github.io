I"'<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-19-降维">Lecture 19 降维</h1>
<h2 id="主要内容">主要内容</h2>
<ul>
  <li><strong>主成成分分析（PCA）</strong>
    <ul>
      <li>线性降维方法</li>
      <li>对角化协方差矩阵</li>
    </ul>
  </li>
  <li><strong>核化 PCA</strong></li>
</ul>

<h2 id="1-关于降维">1. 关于降维</h2>
<h3 id="11-数据的真实维度">1.1 数据的真实维度？</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-27-WX20200227-151842%402x.png" width="80%" /></p>

<h3 id="12-降维">1.2 降维</h3>
<ul>
  <li>之前我们介绍了无监督学习中的一类常见任务：聚类</li>
  <li><strong>降维</strong> 是指使用 <span style="color:red">较少数量的变量</span>（维度）来表示数据，同时保留数据中我们 <span style="color:red">“感兴趣的”</span> 结构</li>
  <li>通过降维，我们可以达到以下目的：
    <ul>
      <li><span style="color:red">可视化</span>（例如，将高维数据映射到 2D）</li>
      <li>提高 pipeline 中的 <span style="color:red">计算效率</span></li>
      <li>提升 pipeline 中的数据压缩或者 <span style="color:red">统计效率</span></li>
    </ul>
  </li>
</ul>

<h3 id="13-探索数据结构">1.3 探索数据结构</h3>
<ul>
  <li>一般而言，降维会导致信息丢失</li>
  <li>诀窍是确保大部分我们 “感兴趣的” 信息被保留下来，而丢失的大部分都属于噪声</li>
  <li>这通常是可能的，因为相比那些记录的变量，真实数据具有的 <span style="color:red">内在维度可能要少得多</span></li>
  <li><strong>例子：</strong> GPS 坐标是 3D 的，而平坦道路上的汽车定位实际是在 2D 流形上的</li>
</ul>

<h2 id="2-主成成分分析pca">2. 主成成分分析（PCA）</h2>
<p><strong>寻找一种数据的旋转方式使得（新）变量之间的协方差最小化</strong></p>

<h3 id="21-主成成分分析">2.1 主成成分分析</h3>
<ul>
  <li>主成成分分析（PCA）是普遍用于降维和数据分析的一种常用方法</li>
  <li>给定一个数据集 $\boldsymbol x_1,…,\boldsymbol x_n$，其中 $\boldsymbol x_i\in \boldsymbol R^m$，PCA 的目标是找到一个新的坐标系，使得大部分方差都集中在第一个坐标轴上，然后剩下的大部分方差都集中在第二个坐标轴上，以此类推</li>
  <li>然后，降维是基于 <span style="color:red">丢弃坐标</span> 实现的，我们仅保留前 $l$ 个坐标，丢弃后面的其余坐标（$l&lt; m$）</li>
</ul>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-27-WX20200227-160440%402x.png" /></p>

<h3 id="22-朴素-pca-算法">2.2 朴素 PCA 算法</h3>
<p><strong><span style="color:steelblue">朴素 PCA 算法描述：</span></strong><br />
1.$\,$<span style="color:red">选择一个方向</span> 作为新的坐标轴，使得方差沿着该坐标轴是最大化的<br />
2.$\,$<span style="color:red">选择下一个方向</span> / 坐标轴垂直于当前所有的坐标轴，使得（余下的）方差着该坐标轴是最大化的<br />
3.$\,$重复步骤 2，直到你得到了和原始数据中同样数量的坐标轴（即，维度）<br />
4.$\,$将原始数据 <span style="color:red">投影</span> 到这些新的坐标轴上，从而得到新的坐标（“PCA 坐标”）<br />
5.$\,$对于每个数据点，仅保留 <span style="color:red">前 $l$ 个坐标</span><br />
<br />
如果可以直接实现，那么该算法是有效的，但是，我们还有其他更好的解决方案</p>

<h3 id="23-问题描述">2.3 问题描述</h3>
<ul>
  <li>PCA 的核心是寻找一个新的坐标系，使得大部分方差都被 “早先的” 几个坐标轴捕获到</li>
  <li>现在，让我们将这一目标的正式形式写出来，看一下如何实现</li>
</ul>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-27-WX20200227-163729%402x.png" width="25%" align="right" /></p>

<ul>
  <li>首先，回忆点积的几何定义 $\boldsymbol u\cdot \boldsymbol v=u_{\boldsymbol v}\|\boldsymbol v\|$</li>
  <li>假设 $\|\boldsymbol v\|=1$，那么 $\boldsymbol u\cdot \boldsymbol v=u_{\boldsymbol v}$</li>
  <li>向量 $\boldsymbol v$ 可以被视为一个候选的坐标 <span style="color:red">轴</span>，$u_{\boldsymbol v}$ 可以被视为点 $\boldsymbol u$ 的 <span style="color:red">坐标</span></li>
</ul>

<h3 id="24-数据变换">2.4 数据变换</h3>
<ul>
  <li>所以，新的 <span style="color:red">坐标系</span> 是一个向量的集合 $\boldsymbol p_1,…,\boldsymbol p_m$，其中每个 $\|\boldsymbol p_i\|=1$</li>
  <li>考虑一个原始的数据点 $\boldsymbol x_j\;(j=1,…,n)$ 和一个主轴 $\boldsymbol p_i\;(i=1,…,m)$</li>
  <li>第一个数据点在经过变换之后，其对应的第 $i$ 个坐标为 $(\boldsymbol p_i)’(\boldsymbol x_1)$
    <ul>
      <li>对于第二个数据点，则为 $(\boldsymbol p_i)’(\boldsymbol x_2)$，以此类推</li>
    </ul>
  </li>
  <li>
    <p>将所有这些数字整理为一个向量</p>

    <script type="math/tex; mode=display">\left[(\boldsymbol p_i)'(\boldsymbol x_1),...,(\boldsymbol p_i)'(\boldsymbol x_n)\right]'=\left((\boldsymbol p_i)'\boldsymbol X\right)'=\boldsymbol X'\boldsymbol p_i</script>

    <p>其中，$\boldsymbol X$ 的列中具有原始的数据点</p>
  </li>
</ul>

<h3 id="25-复习基础统计知识">2.5 复习：基础统计知识</h3>
<ul>
  <li>
    <p>考虑一个随机变量 $U$ 和相应的样本 $\boldsymbol u=[u_1,…,u_n]’$</p>
  </li>
  <li>
    <p>样本均值为：$\overline u \equiv \dfrac{1}{n}\sum_{i=1}^{n}u_i\qquad$ 样本方差为：$\dfrac{1}{n-1}\sum_{i=1}^{n}(u_i-\overline u)^2$</p>
  </li>
  <li>
    <p>假设事先已经减掉了均值（即样本是 <span style="color:red">居中的</span>），在这种情况下，方差是一个经过缩放的点积 $\dfrac{1}{n-1}\boldsymbol u’\boldsymbol u$</p>
  </li>
  <li>
    <p>类似地，如果我们有一个来自另一个随机变量的居中随机样本 $\boldsymbol v$，那么样本方差为 $\dfrac{1}{n-1}\boldsymbol u’\boldsymbol v$</p>
  </li>
  <li>
    <p>最后，如果我们的数据为 $\boldsymbol x_1=[u_1,v_1]’,…,\boldsymbol x_n=[u_n,v_n]’$，将其整合为一个矩阵 $\boldsymbol X$，其中列是数据，行是居中变量。我们可以得到 <span style="color:red">协方差矩阵</span> 为 $\mathbf \Sigma_{\boldsymbol X}\equiv \dfrac{1}{n-1}\boldsymbol X\boldsymbol X’$</p>
  </li>
</ul>

<h3 id="26-pca-的目标">2.6 PCA 的目标</h3>
<ul>
  <li>我们应当假设数据是居中的</li>
  <li>让我们从第一个主轴的目标开始，数据在这个主轴上的投影为 $\boldsymbol X’\boldsymbol p_1$</li>
  <li>
    <p>因此，沿该主轴的方差为</p>

    <script type="math/tex; mode=display">\dfrac{1}{n-1}\left(\boldsymbol X'\boldsymbol p_1\right)'\left(\boldsymbol X'\boldsymbol p_1\right)=\dfrac{1}{n-1}\boldsymbol p_1'\boldsymbol X\boldsymbol X'\boldsymbol p_1=\boldsymbol p_1'\mathbf \Sigma_{\boldsymbol X}\boldsymbol p_1</script>

    <ul>
      <li>这里，$\mathbf \Sigma_{\boldsymbol X}$ 是原始数据的协方差矩阵</li>
    </ul>
  </li>
  <li>因此，PCA 应该找到使得 $\boldsymbol p_1’\mathbf \Sigma_{\boldsymbol X}\boldsymbol p_1$ 最大化的 $\boldsymbol p_1$，其中 $\|\boldsymbol p_1\|=1$</li>
</ul>

<h3 id="27-解决这个优化问题">2.7 解决这个优化问题</h3>
<ul>
  <li>
    <p>PCA 的目标是找到 $\boldsymbol p_1$ 使得 $\boldsymbol p_1’\mathbf \Sigma_{\boldsymbol X}\boldsymbol p_1$ 最大化，其中 $\|\boldsymbol p_1\|=\boldsymbol p_1’\boldsymbol p_1=1$</p>
  </li>
  <li>
    <p>约束 $\to$ 拉格朗日乘子<br />
引入拉格朗日乘子 $\lambda_1$，令拉格朗日函数的一阶导数为零，求解</p>

    <script type="math/tex; mode=display">L=\boldsymbol p_1'\mathbf \Sigma_{\boldsymbol X}\boldsymbol p_1-\lambda_1(\boldsymbol p_1'\boldsymbol p_1-1)</script>

    <script type="math/tex; mode=display">\dfrac{\partial L}{\partial \boldsymbol p_1}=2\mathbf \Sigma_{\boldsymbol X}\boldsymbol p_1-2\lambda_1\boldsymbol p_1=0</script>

    <script type="math/tex; mode=display">\mathbf \Sigma_{\boldsymbol X}\boldsymbol p_1=\lambda_1\boldsymbol p_1</script>
  </li>
  <li>
    <p>定义 $\boldsymbol p_1$ 为 <span style="color:red">特征向量</span>，$\lambda_1$ 为对应的 <span style="color:red">特征值</span></p>
  </li>
</ul>

<h3 id="28-复习特征向量">2.8 复习：特征向量</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-27-WX20200227-174345%402x.png" width="25%" align="right" /></p>

<ul>
  <li>
    <p>给定一个方阵 $\boldsymbol A$，一个列向量 $\boldsymbol e$<br />
如果满足 $\boldsymbol {Ae}=\lambda \boldsymbol e$<br />
那么向量 $\boldsymbol e$ 被称为 方阵 $\boldsymbol A$ 的特征向量<br />
这里，$\lambda$ 是对应的特征值</p>
  </li>
  <li>
    <p>几何解释：将 $\boldsymbol {Ae}$ 与前面的 $\boldsymbol {px}_i$ 进行对比。这里，对于某个向量 $\boldsymbol e$ 而言，$\boldsymbol A$ 是一个变换矩阵（“新轴”），使得向量 $\boldsymbol e$ 在经过变换后仍然指向相同的方向。</p>
  </li>
</ul>

<h3 id="29-复习特征值">2.9 复习：特征值</h3>
<ul>
  <li>代数解释：</li>
</ul>

<h2 id="总结">总结</h2>
<ul>
  <li>无监督学习
    <ul>
      <li>问题的多样性</li>
    </ul>
  </li>
  <li>高斯混合模型（GMM）
    <ul>
      <li>一种聚类的概率方法</li>
      <li>GMM 模型</li>
      <li>GMM 聚类作为一个优化问题</li>
    </ul>
  </li>
  <li>期望最大化（EM）算法</li>
</ul>

<p>下节内容：降维</p>
:ET