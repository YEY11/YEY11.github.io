I"z3<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-07-多层感知器和反向传播">Lecture 07 多层感知器和反向传播</h1>
<h2 id="主要内容">主要内容</h2>
<ul>
  <li><strong>多层感知器</strong>
    <ul>
      <li>模型结构</li>
      <li>通用近似定理</li>
      <li>预训练</li>
    </ul>
  </li>
  <li><strong>反向传播</strong>
    <ul>
      <li>按步推导</li>
      <li>正则化注意事项</li>
    </ul>
  </li>
</ul>

<h2 id="1-多层感知器">1. 多层感知器</h2>
<p><strong>通过复合函数对非线性建模</strong></p>
<h3 id="动物园里的动物">动物园里的动物</h3>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jwez95q2j30vk0k4taf.jpg" width="60%" /></p>

<ul>
  <li>递归神经网络不在本节范围内</li>
  <li>自动编码器一种是经过特定方式训练的人工神经网络（ANN）。
    <ul>
      <li>例如，一个多层感知器或者递归神经网络可以被训练为一个自动编码器。</li>
    </ul>
  </li>
</ul>

<h3 id="11-线性模型的局限">1.1 线性模型的局限</h3>
<p>有些问题是线性可分的，但还有很多不是。</p>

<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jwumawd9j30uu0923z7.jpg" width="80%" /></p>

<p>可能的解：复合<br />
$x_1 \text{ XOR } x_2=(x_1 \text{ OR } x_2)\text{ AND not }(x_1 \text{ AND } x_2)$<br />
我们将组合一个感知器</p>

<h3 id="12-感知器是人工神经网络ann的基础">1.2 感知器是人工神经网络（ANN）的基础</h3>
<ul>
  <li>ANN 不仅限于二分类</li>
  <li>ANN 中的节点可以具有不同的激活函数
    <ul>
      <li>Step 函数：$f(s)=\begin{cases}1, \quad \text{if }s\ge 0\\0, \quad \text{if }s&lt;0\end{cases}$
<br /></li>
      <li>Sign 函数：$f(s)=\begin{cases}1, \quad \text{if }s\ge 0\\-1, \quad \text{if }s&lt;0\end{cases}$
<br /></li>
      <li>Logistic 函数：$f(s)=\dfrac{1}{1+e^{-s}}$
<br /></li>
    </ul>

    <p>还有很多其他的：tanh 函数、rectifier 函数 等</p>
  </li>
</ul>

<h3 id="13-前馈人工神经网络">1.3 前馈人工神经网络</h3>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8k0bc3qapj31320oah1r.jpg" width="80%" /></p>

<h3 id="14-ann-作为复合函数">1.4 ANN 作为复合函数</h3>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8k0suy6bcj317e0nkwut.jpg" width="85%" /></p>

<h3 id="15-监督学习中的-ann">1.5 监督学习中的 ANN</h3>
<ul>
  <li>ANN 可以自然地适应各种监督学习设置，例如单变量和多变量回归，以及二分类和多分类问题。</li>
  <li>单变量回归：$y=f(\boldsymbol x)$
    <ul>
      <li>例如：之前学过的线性回归</li>
    </ul>
  </li>
  <li>多变量回归：$\boldsymbol y=f(\boldsymbol x)$
    <ul>
      <li>预测多个连续结果的值</li>
    </ul>
  </li>
  <li>二分类
    <ul>
      <li>例如：预测一位病人是否患有 II 型糖尿病</li>
    </ul>
  </li>
  <li>多分类
    <ul>
      <li>例如：手写数字识别，标签为 $“0”,“1”,…,“9”$ 等</li>
    </ul>
  </li>
</ul>

<h3 id="16-ann-作为非线性模型的能力">1.6 ANN 作为非线性模型的能力</h3>
<ul>
  <li>ANN 具有近似各种非线性函数的能力，例如：$z(x)=x^2$ 和 $z(x)=\sin(x)$</li>
  <li>
    <p>例如，考虑如下网络。在这个例子中，隐藏单元激活函数是 tanh <br />
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8k39tzu5aj31c80hg4li.jpg" /></p>

    <p><br />
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8k3e5e1cej313s0jsdk7.jpg" width="80%" /></p>

    <ul>
      <li>蓝色点是在不同 $x$ 处评估的函数值</li>
      <li>红色线是来自 ANN 的预测</li>
      <li>虚线是隐藏单元的输出</li>
    </ul>
  </li>
  <li><strong>通用近似定理</strong>（Cybenko 1989）：一个具有隐藏层、有限数量单元、以及对激活函数适当假设的 ANN，可以很好地近似 $\boldsymbol R^n$ 的紧子集上的连续函数。<br /><br />
( 注意：该定理只是表明理论上存在一个可以拟合任何连续函数的神经网络，但并不代表我们的训练算法一定能够找到这样一个网络，它描述的仅仅是模型本身的特性，与训练过程无关。此外，这并非神经网络的独有特性，也存在其他可以近似任意连续函数的模型，例如，选择了适当的核函数的 SVM )</li>
</ul>

<h2 id="2-如何训练你的神经网络">2. 如何训练你的神经网络？</h2>
<ul>
  <li>你应该知道这个过程：定义损失函数并找到参数，使得在训练数据上的损失函数最小化。</li>
  <li>接下来，我们将使用<strong>批次大小</strong>为 1 的<strong>随机梯度下降</strong>。也就是说，我们将逐一处理训练样本。</li>
</ul>

<h3 id="21-训练设置单变量回归">2.1 训练设置：单变量回归</h3>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krc5x553j30hk0iqtas.jpg" width="30%" align="right" /></p>

<ul>
  <li>考虑回归</li>
  <li>此外，我们采用具有唯一输出的激活函数：<br />
$z=h(s)=s=\sum_{j=0}^{p}u_jw_j$</li>
  <li>这将简化反向传播的描述。在其他设置下，训练过程是类似的。</li>
</ul>

<p><strong>思考：</strong> 右图所示的 ANN 中一共有多少个参数？假设存在偏置节点 $x_0$ 和 $u_0$ 的情况下。</p>

<p>对于隐藏层，一共有 $p(m+1)$ 个输入参数，$p+1$ 个输出参数。<br />
所以，该网络一共有 $p(m+1)+(p+1)=(m+2)p+1$ 个参数。</p>

<h3 id="22-ann-训练中的损失函数">2.2 ANN 训练中的损失函数</h3>
<ul>
  <li>在训练样本 $\{\boldsymbol x,y\}$ 和预测 $\hat f(\boldsymbol x,\boldsymbol \theta)=z$ 之间需要 <strong>损失函数</strong>，其中，$\boldsymbol \theta$ 是由 $v_{ij}$ 和 $w_j$ 组成的参数向量。</li>
  <li>对于回归问题，可以采用 <strong>平方和误差</strong><br />
$L=\dfrac{1}{2}\left(\hat f(\boldsymbol x,\boldsymbol \theta)-y\right)^2=\dfrac{1}{2}(z-y)^2$<br />
( 前面的常数项是为了数学计算上的方便 )</li>
  <li><strong>决策理论</strong> 训练：最小化 $L \text{ w.r.t. }\boldsymbol \theta$
    <ul>
      <li>幸运的是，$L(\boldsymbol \theta)$ 是可微的</li>
      <li>不幸的是，通常情况下不存在解析解</li>
    </ul>
  </li>
</ul>

<h3 id="23-ann-中的随机梯度下降">2.3 ANN 中的随机梯度下降</h3>
<ul>
  <li>选择初始的 $\boldsymbol \theta^{(0)}$ 和 $k=0$<br />
( 这里，$\boldsymbol \theta$ 是一个所有层的所有权重的集合 )</li>
  <li>从 $i=1$ 到 $T$（轮）：
    <ul>
      <li>从 $j=1$ 到 $N$（训练样本）：
        <ul>
          <li>考虑样本 $\{\boldsymbol x_j,y_j\}$</li>
          <li>更新：$\boldsymbol \theta^{(i+1)}=\boldsymbol \theta^{(i)}-\eta\nabla L(\boldsymbol \theta^{(i)})$<br />
其中，<br />
$L=\dfrac{1}{2}(z_j-y_j)^2$<br />
需要计算偏导数 $\dfrac{\partial L}{\partial v_{ij}}$ 和 $\dfrac{\partial L}{\partial w_j}$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="3-反向传播">3. 反向传播</h2>
<p><strong>= “误差的反向传播”</strong>
<strong>计算一个复合函数的损失函数的梯度</strong></p>
<h3 id="31-反向传播从链式法则出发">3.1 反向传播：从链式法则出发</h3>
<ul>
  <li>回忆一个 ANN 的输出 $z$ 是一个复合函数，因此，$L(z)$ 也是一个复合函数<br /><br />
<script type="math/tex">% <![CDATA[
\begin{eqnarray}
L &=& 0.5(z-y)^2=0.5(h(s)-y)^2=0.5(s-y)^2\\
&=& 0.5\left( \sum_{j=0}^{p}u_jw_j-y \right)^2 = 0.5\left(\sum_{j=0}^{p}g(r_j)w_j-y\right)^2=...
\end{eqnarray} %]]></script></li>
</ul>

<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krc5x553j30hk0iqtas.jpg" width="30%" align="right" /></p>

<ul>
  <li>反向传播利用了链式求导法则
<br />
    <ul>
      <li>$\dfrac{\partial L}{\partial w_j}=\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}\dfrac{\partial s}{\partial w_j}$
<br /></li>
      <li>$\dfrac{\partial L}{\partial v_{ij}}=\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}\dfrac{\partial s}{\partial u_j}\dfrac{\partial u_j}{\partial r_j}\dfrac{\partial r_j}{\partial v_{ij}}$</li>
    </ul>
  </li>
</ul>

<h3 id="32-反向传播中间步骤">3.2 反向传播：中间步骤</h3>
<ul>
  <li>应用链式法则：
<br />
    <ul>
      <li>$\dfrac{\partial L}{\partial w_j}=\begingroup\color{red}\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}\endgroup\dfrac{\partial s}{\partial w_j}$
<br /></li>
      <li>$\dfrac{\partial L}{\partial v_{ij}}=\begingroup\color{red}\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}\dfrac{\partial s}{\partial u_j}\dfrac{\partial u_j}{\partial r_j}\endgroup\dfrac{\partial r_j}{\partial v_{ij}}$
<br /></li>
    </ul>
  </li>
  <li>现在，我们定义：
<br />
    <ul>
      <li>$\delta\equiv\dfrac{\partial L}{\partial s}=\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}$
<br /></li>
      <li>$\varepsilon_j\equiv\dfrac{\partial L}{\partial r_j}=\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}\dfrac{\partial s}{\partial u_j}\dfrac{\partial u_j}{\partial r_j}$
<br /></li>
    </ul>
  </li>
  <li>这里，$L=0.5(z-y)^2$ 并且 $z=s$<br /><br />
因此，$\color{red}\delta=(z-y)$
<br /></li>
  <li>这里，$s=\sum_{j=0}^{p}u_jw_j$ 并且 $u_j=g(r_j)$<br />
因此，$\color{red}\varepsilon_j=\delta w_jg’(r_j)$</li>
</ul>

<h3 id="33-反向传播等式">3.3 反向传播等式</h3>
<ul>
  <li>我们有
<br />
    <ul>
      <li>$\dfrac{\partial L}{\partial w_j}=\delta\begingroup\color{red}\dfrac{\partial s}{\partial w_j}\endgroup$ ， 其中，$\delta=\dfrac{\partial L}{\partial s}=(z-y)$
<br /></li>
      <li>$\dfrac{\partial L}{\partial v_{ij}}=\varepsilon_j\begingroup\color{red}\dfrac{\partial r_j}{\partial v_{ij}}\endgroup$ ， 其中，$\varepsilon_j=\dfrac{\partial L}{\partial r_j}=\delta w_jg’(r_j)$
<br /></li>
    </ul>
  </li>
  <li>回忆，$s=\sum_{j=0}^{p}u_jw_j$ ， $r_j=\sum_{i=0}^{m}x_iv_{ij}$
<br /></li>
  <li>因此，$\dfrac{\partial s}{\partial w_j}=u_j$ ， $\dfrac{\partial r_j}{\partial v_{ij}}=x_i$
<br /></li>
  <li>我们有
<br />
    <ul>
      <li>$\dfrac{\partial L}{\partial w_j}=\delta u_j=(z-y)u_j$
<br /></li>
      <li>$\dfrac{\partial L}{\partial v_{ij}}=\varepsilon_j x_i=\delta w_jg’(r_j)x_i$</li>
    </ul>
  </li>
</ul>

<h3 id="34-向前传播">3.4 向前传播</h3>
<ul>
  <li>用当前估计的 $v_{ij}$ 和 $w_j$ $\quad\Longrightarrow\quad$ 计算 $r_j,u_j,s$ 和 $z$</li>
</ul>

<h3 id="35-误差的反向传播">3.5 误差的反向传播</h3>
<ul>
  <li>$\dfrac{\partial L}{\partial v_{ij}}=\varepsilon_jx_i\quad\Longleftarrow\quad\varepsilon_j=\delta w_jg’(r_j)\quad\Longleftarrow\quad\dfrac{\partial L}{\partial w_j}=\delta u_j\quad\Longleftarrow\quad\delta=(z-y)$</li>
</ul>

<h3 id="36-ann-训练中的其他注意事项">3.6 ANN 训练中的其他注意事项</h3>
<ul>
  <li>ANN 非常灵活（回忆通用近似定理），但另一方面导致了过度参数化，因此倾向于 <strong>过拟合</strong></li>
  <li>起始权重通常随机分布在零附近</li>
  <li>隐式正则化：<strong>提前停止</strong>
    <ul>
      <li>对于有些激活函数，将会使 ANN 退化为线性模型（为什么？）
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8l205se3uj30io0d0jtd.jpg" width="50%" />
如上图所示，假如我们采用 tanh 函数作为激活函数，如果权重接近 0，那么 tanh 函数起作用的部分近似线性，因此，神经网络退化成近似线性模型。</li>
    </ul>
  </li>
</ul>

<h3 id="37-显式正则化">3.7 显式正则化</h3>
<ul>
  <li>或者，我们也可以采用一种显式正则化的方法，这与岭回归中的正则化非常类似</li>
  <li>相比最小化损失函数 $L$，我们选择最小化正则化后的损失函数：<br /><br />
$L+\lambda(\sum_{i=0}^{m}\sum_{j=1}^{p}v_{ij}^2+\sum_{j=0}^{p}w_j^2)$
<br /></li>
  <li>这将在计算偏导数时，简单地加上 $2\lambda v_{ij}$ 和 $2\lambda w_j$ 项</li>
  <li>对于某些激活函数，这种方法同样会存在 ANN 退化成线性模型的问题</li>
</ul>

<h2 id="总结">总结</h2>
<ul>
  <li>多层感知器
    <ul>
      <li>模型结构</li>
      <li>通用近似定理</li>
      <li>预训练</li>
    </ul>
  </li>
  <li>反向传播
    <ul>
      <li>按步推导</li>
      <li>正则化注意事项</li>
    </ul>
  </li>
</ul>

<p>下节内容：DNN（深度神经网络）、CNN（卷积神经网络）、自动编码器</p>

<p>$\dfrac{\partial L}{\partial w_j}=\color{red}{\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}}\dfrac{\partial s}{\partial w_j}$</p>

<p>$\begingroup{\color{red}{\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}}}\endgroup$</p>
:ET