I"<!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<h1 id="lecture-00-02-数值优化基础">Lecture 00-02 数值优化基础</h1>

<blockquote>
  <p>参考资料：Natural Language Processing: Appendix B (p.485-488) —— by Eisenstein, Jacob</p>
</blockquote>

<h2 id="引言">引言</h2>
<p>无约束数值优化涉及到求解具有以下形式的问题：</p>

<script type="math/tex; mode=display">\min \limits_{\boldsymbol x\in \mathbb R^D}f(\boldsymbol x) \tag{1}</script>

<p>其中，$x\in \mathbb R^D$ 是一个由 $D$ 个实数构成的向量。</p>

<p>微分是数值优化的基础。假设在某个点 $\boldsymbol x^*$ 处，$f$ 的每个偏导数都等于 $0$：形式上，$\left. \frac{\partial f}{\partial x_i}\right| _{\boldsymbol x^*}=0$。那么 $\boldsymbol x^*$ 被称为 $f$ 的一个 <strong>临界点</strong>。 如果 $f$ 是一个 <strong>凸函数</strong>，那么 $f(\boldsymbol x^*)$ 的值等于 $f$ 的全局最小值，当且仅当 $\boldsymbol x^*$ 是 $f$ 的临界点时。</p>

<p>例如，考虑一个凸函数 $f(x)=(x-2)^2+3$，如 <a href="#fig1">图 1(a)</a> 所示。其导数为 $\frac{\partial f}{\partial x}=2x-4$。通过将导数设置为零并求解 $x$，可以得到一个唯一的最小值 $x^* = 2$。现在考虑一个多元凸函数 $f(\boldsymbol x)=\frac{1}{2}\|\boldsymbol x-[2,1]^{\top}\|^2$ ，其中 $\|\boldsymbol x \|^2$ 是平方欧几里得范数。偏导数为：</p>

<script type="math/tex; mode=display">\dfrac{\partial d}{\partial x_1}=x_1-2  \tag{2}</script>

<script type="math/tex; mode=display">\dfrac{\partial d}{\partial x_2}=x_2-1  \tag{3}</script>

<p>唯一最小值为 $\boldsymbol x^*=[2,1]^{\top}$。</p>

<p><a name="fig1"><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-02-28-WX20200228-231327%402x.png" /></a></p>

<p><strong><center><span style="font-size:10pt">图 1：两个具有唯一全局最小值的函数</span></center></strong></p>

<p>对于非凸函数，临界点不一定是全局最小值。一个局部最小值 $\boldsymbol x^*$ 意味着函数在该点处的取值小于在其所有近邻点的取值：形式上，$\boldsymbol x^*$ 是一个局部最小值，如果存在某个正值 $\epsilon$ 使得 $f(\boldsymbol x^*)\le f(\boldsymbol x)$ 对于所有和 $\boldsymbol x^*$ 距离在 $\epsilon$ 以内的 $\boldsymbol x$ 都成立。<a href="#fig1">图 1(b)</a> 描绘了函数 $f(x)=|x|-2\cos(x)$，它具有许多局部最小值，并且在 $x = 0$ 处还具有一个唯一的全局最小值。临界点也可以是函数的局部最大值或者全局最大值；它也可以是一个 <strong>鞍点</strong>，至少在一个坐标方向上是最小值，而在至少一个其余的坐标方向上是最大值；它还可以是一个 <strong>拐点</strong>，既不是最小值，也不是最大值。如果可以的话，$f$ 的二阶导数可以帮助区分这些情况。</p>

<h2 id="1-梯度下降">1. 梯度下降</h2>
<p>对于许多凸函数，我们不可能求得 $\boldsymbol x^*$ 的闭合解。在梯度下降中，我们计算一系列的解，$\boldsymbol x^{(0)},\boldsymbol x^{(1)},…$ 方法是通过沿着局部梯度 $\nabla_{\boldsymbol x^{(t)}}f$ 的方向一步步前进，这里的梯度是在点 $\boldsymbol x^{(t)}$ 处求得的函数 $f$ 的偏导数构成的向量。每个解 $\boldsymbol x^{(t+1)}$ 可以根据下式计算得到：</p>

<script type="math/tex; mode=display">\boldsymbol x^{(t+1)} \leftarrow \boldsymbol x^{(t)} - \eta^{(t)}\nabla_{\boldsymbol x^{(t)}}f  \tag{4}</script>

<p>其中，$\eta^{(t)}&gt;0$ 是一个 <strong>步长</strong>。如果适当选择步长，那么此过程将会找到一个可微凸函数的全局最小值。对于非凸函数，梯度下降将找到一个局部最小值。</p>

<h2 id="2-约束优化">2. 约束优化</h2>
<p>很多时候，优化必须在约束条件下进行：例如，当对一个概率分布的参数进行优化时，所有事件的概率之和必须等于 $1$。约束优化问题可以写成如下形式：</p>

<script type="math/tex; mode=display">\min \limits_{\boldsymbol x}f(\boldsymbol x) \tag{5}</script>

<script type="math/tex; mode=display">\text{s.t.} \quad g_c(\boldsymbol x)\le 0,\qquad \forall c=1,2,...,C  \tag{6}</script>

<p>其中，每个 $g_c(\boldsymbol x)$ 都是 $\boldsymbol x$ 的一个标量函数。例如，假设 $\boldsymbol x$ 必须为非负，并且其总和不能超过预设的 $b$。然后，存在 $D+1$ 个不等式约束：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
g_i(\boldsymbol x) &= -x_i,\qquad \forall i=1,2,...,D  \tag{7}\\
g_{D+1}(\boldsymbol x) &= -b+\sum_{i=1}^{D}x_i \tag{8}
\end{align} %]]></script>

<p>不等式约束可以与原始目标函数 $f$ 结合起来，这可以通过构造一个 <strong>拉格朗日方程</strong> 实现：</p>

<script type="math/tex; mode=display">L(\boldsymbol x,\boldsymbol \lambda)=f(\boldsymbol x)+\sum_{c=1}^{C}\lambda_c g_c(\boldsymbol x)  \tag{9}</script>

<p>其中，$\lambda_c$ 是一个 <strong>拉格朗日乘子</strong>。对于任何拉格朗日函数，都存在一个对应的对偶形式，它是一个关于 $\boldsymbol \lambda$ 的函数：</p>

<script type="math/tex; mode=display">D(\boldsymbol \lambda)=\min \limits_{\boldsymbol x} L(\boldsymbol x,\boldsymbol \lambda)  \tag{10}</script>

<p>我们可以将拉格朗日函数 $L$ 称为问题的原始形式。</p>

<h2 id="3-例子被动攻击在线学习">3. 例子：被动攻击在线学习</h2>
<p>有时可以通过改造拉格朗日方程来解决约束优化问题。一个例子是朴素贝叶斯概率模型的最大似然估计。在这种情况下，不必显式计算拉格朗日乘子。另一个例子是用于在线学习的 <strong>被动攻击算法（passive-aggressive algorithm）</strong>（Crammer 等，2006）。该算法与感知器类似，但其在每一步的目标是进行最保守的更新，使得当前样本上的边缘损失为零。（<em><span style="font-size:10pt">这是该算法名称的基础：当损失为零时它是被动的，但在必要时会主动出击使损失为零。</span></em>）每次更新都可以表述为一个权重 $\boldsymbol \theta$ 上的约束优化：</p>

<p>下节内容：数值优化基础</p>
:ET